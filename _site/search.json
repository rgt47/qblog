[
  {
    "objectID": "white-papers/index.html",
    "href": "white-papers/index.html",
    "title": "White Papers",
    "section": "",
    "text": "These white papers represent in-depth technical analyses, methodological frameworks, and implementation guides developed for research and statistical computing applications. Each document provides detailed specifications, best practices, and reproducible workflows.\nUse the search box above to find specific topics, or click on category tags to filter papers by research area."
  },
  {
    "objectID": "white-papers/index.html#technical-reports-methodological-frameworks",
    "href": "white-papers/index.html#technical-reports-methodological-frameworks",
    "title": "White Papers",
    "section": "",
    "text": "These white papers represent in-depth technical analyses, methodological frameworks, and implementation guides developed for research and statistical computing applications. Each document provides detailed specifications, best practices, and reproducible workflows.\nUse the search box above to find specific topics, or click on category tags to filter papers by research area."
  },
  {
    "objectID": "tutorials/r-package-development-basics.html",
    "href": "tutorials/r-package-development-basics.html",
    "title": "R Package Development: From Idea to CRAN",
    "section": "",
    "text": "By the end of this tutorial, you will: - Set up a proper R package development environment - Create package structure and documentation - Write and test package functions - Prepare for CRAN submission"
  },
  {
    "objectID": "tutorials/r-package-development-basics.html#learning-objectives",
    "href": "tutorials/r-package-development-basics.html#learning-objectives",
    "title": "R Package Development: From Idea to CRAN",
    "section": "",
    "text": "By the end of this tutorial, you will: - Set up a proper R package development environment - Create package structure and documentation - Write and test package functions - Prepare for CRAN submission"
  },
  {
    "objectID": "tutorials/r-package-development-basics.html#prerequisites",
    "href": "tutorials/r-package-development-basics.html#prerequisites",
    "title": "R Package Development: From Idea to CRAN",
    "section": "2 Prerequisites",
    "text": "2 Prerequisites\n\nBasic R programming knowledge\nRStudio installed\nGit familiarity (helpful but not required)"
  },
  {
    "objectID": "tutorials/r-package-development-basics.html#step-1-development-environment-setup",
    "href": "tutorials/r-package-development-basics.html#step-1-development-environment-setup",
    "title": "R Package Development: From Idea to CRAN",
    "section": "3 Step 1: Development Environment Setup",
    "text": "3 Step 1: Development Environment Setup\nFirst, install the essential packages for R development:\ninstall.packages(c(\"devtools\", \"usethis\", \"roxygen2\", \"testthat\"))\nConfigure your development environment:\nlibrary(usethis)\nuse_git_config(user.name = \"Your Name\", user.email = \"your.email@example.com\")"
  },
  {
    "objectID": "tutorials/r-package-development-basics.html#step-2-create-package-structure",
    "href": "tutorials/r-package-development-basics.html#step-2-create-package-structure",
    "title": "R Package Development: From Idea to CRAN",
    "section": "4 Step 2: Create Package Structure",
    "text": "4 Step 2: Create Package Structure\nCreate a new package:\ncreate_package(\"~/path/to/mypackage\")\nThis creates the standard package directory structure: - R/ - Your R functions - man/ - Documentation files (auto-generated) - DESCRIPTION - Package metadata - NAMESPACE - Exported functions (auto-generated)"
  },
  {
    "objectID": "tutorials/r-package-development-basics.html#step-3-write-your-first-function",
    "href": "tutorials/r-package-development-basics.html#step-3-write-your-first-function",
    "title": "R Package Development: From Idea to CRAN",
    "section": "5 Step 3: Write Your First Function",
    "text": "5 Step 3: Write Your First Function\nCreate a new R file in the R/ directory:\n#' Add two numbers together\n#'\n#' This function takes two numeric inputs and returns their sum.\n#'\n#' @param x A numeric value\n#' @param y A numeric value\n#' @return The sum of x and y\n#' @export\n#' @examples\n#' add_numbers(2, 3)\n#' add_numbers(10, -5)\nadd_numbers &lt;- function(x, y) {\n  if (!is.numeric(x) || !is.numeric(y)) {\n    stop(\"Both inputs must be numeric\")\n  }\n  x + y\n}"
  },
  {
    "objectID": "tutorials/r-package-development-basics.html#step-4-generate-documentation",
    "href": "tutorials/r-package-development-basics.html#step-4-generate-documentation",
    "title": "R Package Development: From Idea to CRAN",
    "section": "6 Step 4: Generate Documentation",
    "text": "6 Step 4: Generate Documentation\nUse roxygen2 to generate documentation:\ndevtools::document()\nThis creates help files in the man/ directory and updates your NAMESPACE."
  },
  {
    "objectID": "tutorials/r-package-development-basics.html#step-5-testing",
    "href": "tutorials/r-package-development-basics.html#step-5-testing",
    "title": "R Package Development: From Idea to CRAN",
    "section": "7 Step 5: Testing",
    "text": "7 Step 5: Testing\nCreate unit tests to ensure your functions work correctly:\nusethis::use_testthat()\nusethis::use_test(\"add_numbers\")\nWrite tests in tests/testthat/test-add_numbers.R:\ntest_that(\"add_numbers works correctly\", {\n  expect_equal(add_numbers(2, 3), 5)\n  expect_equal(add_numbers(-1, 1), 0)\n  expect_error(add_numbers(\"a\", 1))\n})\nRun tests:\ndevtools::test()"
  },
  {
    "objectID": "tutorials/r-package-development-basics.html#step-6-package-checks",
    "href": "tutorials/r-package-development-basics.html#step-6-package-checks",
    "title": "R Package Development: From Idea to CRAN",
    "section": "8 Step 6: Package Checks",
    "text": "8 Step 6: Package Checks\nBefore submitting to CRAN, run comprehensive checks:\ndevtools::check()\nThis runs R CMD check and identifies potential issues."
  },
  {
    "objectID": "tutorials/r-package-development-basics.html#step-7-preparing-for-cran",
    "href": "tutorials/r-package-development-basics.html#step-7-preparing-for-cran",
    "title": "R Package Development: From Idea to CRAN",
    "section": "9 Step 7: Preparing for CRAN",
    "text": "9 Step 7: Preparing for CRAN\nUpdate your DESCRIPTION file with proper metadata:\nPackage: mypackage\nTitle: What the Package Does (One Line, Title Case)\nVersion: 0.1.0\nAuthors@R: \n    person(\"First\", \"Last\", , \"first.last@example.com\", role = c(\"aut\", \"cre\"))\nDescription: What the package does (one paragraph).\nLicense: MIT + file LICENSE\nEncoding: UTF-8\nRoxygen: list(markdown = TRUE)\nRoxygenNote: 7.2.3\nSuggests: \n    testthat (&gt;= 3.0.0)\nConfig/testthat/edition: 3"
  },
  {
    "objectID": "tutorials/r-package-development-basics.html#next-steps",
    "href": "tutorials/r-package-development-basics.html#next-steps",
    "title": "R Package Development: From Idea to CRAN",
    "section": "10 Next Steps",
    "text": "10 Next Steps\n\nAdd more functions and documentation\nCreate vignettes for complex workflows\nSet up continuous integration\nSubmit to CRAN when ready"
  },
  {
    "objectID": "tutorials/r-package-development-basics.html#resources",
    "href": "tutorials/r-package-development-basics.html#resources",
    "title": "R Package Development: From Idea to CRAN",
    "section": "11 Resources",
    "text": "11 Resources\n\nR Packages book by Hadley Wickham\nWriting R Extensions manual\nCRAN Policy"
  },
  {
    "objectID": "tutorials/git-setup-guide/index.html",
    "href": "tutorials/git-setup-guide/index.html",
    "title": "Setting up git for (solo) data science workflow",
    "section": "",
    "text": "purrr"
  },
  {
    "objectID": "tutorials/git-setup-guide/index.html#prerequisites",
    "href": "tutorials/git-setup-guide/index.html#prerequisites",
    "title": "Setting up git for (solo) data science workflow",
    "section": "4.1 Prerequisites",
    "text": "4.1 Prerequisites\nIn development"
  },
  {
    "objectID": "tutorials/git-setup-guide/index.html#step-by-step-implementation",
    "href": "tutorials/git-setup-guide/index.html#step-by-step-implementation",
    "title": "Setting up git for (solo) data science workflow",
    "section": "4.2 Step-by-Step Implementation",
    "text": "4.2 Step-by-Step Implementation\nIn development"
  },
  {
    "objectID": "tutorials/git-setup-guide/index.html#key-takeaways",
    "href": "tutorials/git-setup-guide/index.html#key-takeaways",
    "title": "Setting up git for (solo) data science workflow",
    "section": "4.3 Key Takeaways",
    "text": "4.3 Key Takeaways\nIn development"
  },
  {
    "objectID": "tutorials/git-setup-guide/index.html#further-reading",
    "href": "tutorials/git-setup-guide/index.html#further-reading",
    "title": "Setting up git for (solo) data science workflow",
    "section": "4.4 Further Reading",
    "text": "4.4 Further Reading\nIn development"
  },
  {
    "objectID": "teaching/r-commands-cheatsheet.html",
    "href": "teaching/r-commands-cheatsheet.html",
    "title": "R Commands Quick Reference",
    "section": "",
    "text": "Task\nCommand\nExample\n\n\n\n\nRead CSV\nread.csv()\nread.csv(\"data.csv\")\n\n\nRead Excel\nreadxl::read_excel()\nread_excel(\"data.xlsx\")\n\n\nWrite CSV\nwrite.csv()\nwrite.csv(df, \"output.csv\")\n\n\nSave RDS\nsaveRDS()\nsaveRDS(data, \"data.rds\")\n\n\nLoad RDS\nreadRDS()\nreadRDS(\"data.rds\")"
  },
  {
    "objectID": "teaching/r-commands-cheatsheet.html#data-importexport",
    "href": "teaching/r-commands-cheatsheet.html#data-importexport",
    "title": "R Commands Quick Reference",
    "section": "",
    "text": "Task\nCommand\nExample\n\n\n\n\nRead CSV\nread.csv()\nread.csv(\"data.csv\")\n\n\nRead Excel\nreadxl::read_excel()\nread_excel(\"data.xlsx\")\n\n\nWrite CSV\nwrite.csv()\nwrite.csv(df, \"output.csv\")\n\n\nSave RDS\nsaveRDS()\nsaveRDS(data, \"data.rds\")\n\n\nLoad RDS\nreadRDS()\nreadRDS(\"data.rds\")"
  },
  {
    "objectID": "teaching/r-commands-cheatsheet.html#data-manipulation-dplyr",
    "href": "teaching/r-commands-cheatsheet.html#data-manipulation-dplyr",
    "title": "R Commands Quick Reference",
    "section": "2 Data Manipulation (dplyr)",
    "text": "2 Data Manipulation (dplyr)\n\n\n\nTask\nCommand\nExample\n\n\n\n\nFilter rows\nfilter()\nfilter(df, age &gt; 18)\n\n\nSelect columns\nselect()\nselect(df, name, age)\n\n\nCreate columns\nmutate()\nmutate(df, age_months = age * 12)\n\n\nGroup data\ngroup_by()\ngroup_by(df, category)\n\n\nSummarize\nsummarise()\nsummarise(df, mean_age = mean(age))\n\n\nSort\narrange()\narrange(df, desc(age))"
  },
  {
    "objectID": "teaching/r-commands-cheatsheet.html#data-visualization-ggplot2",
    "href": "teaching/r-commands-cheatsheet.html#data-visualization-ggplot2",
    "title": "R Commands Quick Reference",
    "section": "3 Data Visualization (ggplot2)",
    "text": "3 Data Visualization (ggplot2)\n\n\n\n\n\n\n\n\nTask\nCommand\nExample\n\n\n\n\nScatter plot\ngeom_point()\nggplot(df, aes(x, y)) + geom_point()\n\n\nLine plot\ngeom_line()\nggplot(df, aes(x, y)) + geom_line()\n\n\nBar plot\ngeom_bar()\nggplot(df, aes(x)) + geom_bar()\n\n\nHistogram\ngeom_histogram()\nggplot(df, aes(x)) + geom_histogram()\n\n\nBox plot\ngeom_boxplot()\nggplot(df, aes(x, y)) + geom_boxplot()"
  },
  {
    "objectID": "teaching/r-commands-cheatsheet.html#statistical-functions",
    "href": "teaching/r-commands-cheatsheet.html#statistical-functions",
    "title": "R Commands Quick Reference",
    "section": "4 Statistical Functions",
    "text": "4 Statistical Functions\n\n\n\nTask\nCommand\nExample\n\n\n\n\nMean\nmean()\nmean(x, na.rm = TRUE)\n\n\nMedian\nmedian()\nmedian(x, na.rm = TRUE)\n\n\nStandard deviation\nsd()\nsd(x, na.rm = TRUE)\n\n\nCorrelation\ncor()\ncor(x, y, use = \"complete.obs\")\n\n\nLinear model\nlm()\nlm(y ~ x, data = df)\n\n\nANOVA\naov()\naov(y ~ group, data = df)"
  },
  {
    "objectID": "teaching/r-commands-cheatsheet.html#string-operations",
    "href": "teaching/r-commands-cheatsheet.html#string-operations",
    "title": "R Commands Quick Reference",
    "section": "5 String Operations",
    "text": "5 String Operations\n\n\n\nTask\nCommand\nExample\n\n\n\n\nConcatenate\npaste()\npaste(\"Hello\", \"World\")\n\n\nSplit string\nstrsplit()\nstrsplit(\"a,b,c\", \",\")\n\n\nFind pattern\ngrep()\ngrep(\"pattern\", x)\n\n\nReplace pattern\ngsub()\ngsub(\"old\", \"new\", x)\n\n\nString length\nnchar()\nnchar(\"hello\")"
  },
  {
    "objectID": "teaching/r-commands-cheatsheet.html#package-management",
    "href": "teaching/r-commands-cheatsheet.html#package-management",
    "title": "R Commands Quick Reference",
    "section": "6 Package Management",
    "text": "6 Package Management\n\n\n\nTask\nCommand\nExample\n\n\n\n\nInstall package\ninstall.packages()\ninstall.packages(\"dplyr\")\n\n\nLoad package\nlibrary()\nlibrary(dplyr)\n\n\nUpdate packages\nupdate.packages()\nupdate.packages()\n\n\nList packages\ninstalled.packages()\ninstalled.packages()"
  },
  {
    "objectID": "teaching/r-commands-cheatsheet.html#workspace-management",
    "href": "teaching/r-commands-cheatsheet.html#workspace-management",
    "title": "R Commands Quick Reference",
    "section": "7 Workspace Management",
    "text": "7 Workspace Management\n\n\n\nTask\nCommand\nExample\n\n\n\n\nList objects\nls()\nls()\n\n\nRemove objects\nrm()\nrm(x, y)\n\n\nClear workspace\nrm(list = ls())\nrm(list = ls())\n\n\nWorking directory\ngetwd()\ngetwd()\n\n\nSet directory\nsetwd()\nsetwd(\"/path/to/dir\")"
  },
  {
    "objectID": "teaching/r-commands-cheatsheet.html#data-types-structure",
    "href": "teaching/r-commands-cheatsheet.html#data-types-structure",
    "title": "R Commands Quick Reference",
    "section": "8 Data Types & Structure",
    "text": "8 Data Types & Structure\n\n\n\nTask\nCommand\nExample\n\n\n\n\nData type\nclass()\nclass(x)\n\n\nStructure\nstr()\nstr(df)\n\n\nDimensions\ndim()\ndim(df)\n\n\nColumn names\nnames()\nnames(df)\n\n\nSummary\nsummary()\nsummary(df)\n\n\nFirst rows\nhead()\nhead(df, 10)\n\n\nLast rows\ntail()\ntail(df, 10)"
  },
  {
    "objectID": "teaching/r-commands-cheatsheet.html#missing-values",
    "href": "teaching/r-commands-cheatsheet.html#missing-values",
    "title": "R Commands Quick Reference",
    "section": "9 Missing Values",
    "text": "9 Missing Values\n\n\n\nTask\nCommand\nExample\n\n\n\n\nCheck for NA\nis.na()\nis.na(x)\n\n\nRemove NA\nna.omit()\nna.omit(df)\n\n\nComplete cases\ncomplete.cases()\ncomplete.cases(df)"
  },
  {
    "objectID": "teaching/r-commands-cheatsheet.html#quick-tips",
    "href": "teaching/r-commands-cheatsheet.html#quick-tips",
    "title": "R Commands Quick Reference",
    "section": "10 Quick Tips",
    "text": "10 Quick Tips\n\nUse ?function_name to get help\nUse Tab for auto-completion in RStudio\nUse Ctrl+Shift+M for pipe operator %&gt;%\nUse Ctrl+Shift+C to comment/uncomment code"
  },
  {
    "objectID": "research/r-package-development-basics.html",
    "href": "research/r-package-development-basics.html",
    "title": "R Package Development: From Idea to CRAN",
    "section": "",
    "text": "By the end of this tutorial, you will: - Set up a proper R package development environment - Create package structure and documentation - Write and test package functions - Prepare for CRAN submission"
  },
  {
    "objectID": "research/r-package-development-basics.html#learning-objectives",
    "href": "research/r-package-development-basics.html#learning-objectives",
    "title": "R Package Development: From Idea to CRAN",
    "section": "",
    "text": "By the end of this tutorial, you will: - Set up a proper R package development environment - Create package structure and documentation - Write and test package functions - Prepare for CRAN submission"
  },
  {
    "objectID": "research/r-package-development-basics.html#prerequisites",
    "href": "research/r-package-development-basics.html#prerequisites",
    "title": "R Package Development: From Idea to CRAN",
    "section": "2 Prerequisites",
    "text": "2 Prerequisites\n\nBasic R programming knowledge\nRStudio installed\nGit familiarity (helpful but not required)"
  },
  {
    "objectID": "research/r-package-development-basics.html#step-1-development-environment-setup",
    "href": "research/r-package-development-basics.html#step-1-development-environment-setup",
    "title": "R Package Development: From Idea to CRAN",
    "section": "3 Step 1: Development Environment Setup",
    "text": "3 Step 1: Development Environment Setup\nFirst, install the essential packages for R development:\ninstall.packages(c(\"devtools\", \"usethis\", \"roxygen2\", \"testthat\"))\nConfigure your development environment:\nlibrary(usethis)\nuse_git_config(user.name = \"Your Name\", user.email = \"your.email@example.com\")"
  },
  {
    "objectID": "research/r-package-development-basics.html#step-2-create-package-structure",
    "href": "research/r-package-development-basics.html#step-2-create-package-structure",
    "title": "R Package Development: From Idea to CRAN",
    "section": "4 Step 2: Create Package Structure",
    "text": "4 Step 2: Create Package Structure\nCreate a new package:\ncreate_package(\"~/path/to/mypackage\")\nThis creates the standard package directory structure: - R/ - Your R functions - man/ - Documentation files (auto-generated) - DESCRIPTION - Package metadata - NAMESPACE - Exported functions (auto-generated)"
  },
  {
    "objectID": "research/r-package-development-basics.html#step-3-write-your-first-function",
    "href": "research/r-package-development-basics.html#step-3-write-your-first-function",
    "title": "R Package Development: From Idea to CRAN",
    "section": "5 Step 3: Write Your First Function",
    "text": "5 Step 3: Write Your First Function\nCreate a new R file in the R/ directory:\n#' Add two numbers together\n#'\n#' This function takes two numeric inputs and returns their sum.\n#'\n#' @param x A numeric value\n#' @param y A numeric value\n#' @return The sum of x and y\n#' @export\n#' @examples\n#' add_numbers(2, 3)\n#' add_numbers(10, -5)\nadd_numbers &lt;- function(x, y) {\n  if (!is.numeric(x) || !is.numeric(y)) {\n    stop(\"Both inputs must be numeric\")\n  }\n  x + y\n}"
  },
  {
    "objectID": "research/r-package-development-basics.html#step-4-generate-documentation",
    "href": "research/r-package-development-basics.html#step-4-generate-documentation",
    "title": "R Package Development: From Idea to CRAN",
    "section": "6 Step 4: Generate Documentation",
    "text": "6 Step 4: Generate Documentation\nUse roxygen2 to generate documentation:\ndevtools::document()\nThis creates help files in the man/ directory and updates your NAMESPACE."
  },
  {
    "objectID": "research/r-package-development-basics.html#step-5-testing",
    "href": "research/r-package-development-basics.html#step-5-testing",
    "title": "R Package Development: From Idea to CRAN",
    "section": "7 Step 5: Testing",
    "text": "7 Step 5: Testing\nCreate unit tests to ensure your functions work correctly:\nusethis::use_testthat()\nusethis::use_test(\"add_numbers\")\nWrite tests in tests/testthat/test-add_numbers.R:\ntest_that(\"add_numbers works correctly\", {\n  expect_equal(add_numbers(2, 3), 5)\n  expect_equal(add_numbers(-1, 1), 0)\n  expect_error(add_numbers(\"a\", 1))\n})\nRun tests:\ndevtools::test()"
  },
  {
    "objectID": "research/r-package-development-basics.html#step-6-package-checks",
    "href": "research/r-package-development-basics.html#step-6-package-checks",
    "title": "R Package Development: From Idea to CRAN",
    "section": "8 Step 6: Package Checks",
    "text": "8 Step 6: Package Checks\nBefore submitting to CRAN, run comprehensive checks:\ndevtools::check()\nThis runs R CMD check and identifies potential issues."
  },
  {
    "objectID": "research/r-package-development-basics.html#step-7-preparing-for-cran",
    "href": "research/r-package-development-basics.html#step-7-preparing-for-cran",
    "title": "R Package Development: From Idea to CRAN",
    "section": "9 Step 7: Preparing for CRAN",
    "text": "9 Step 7: Preparing for CRAN\nUpdate your DESCRIPTION file with proper metadata:\nPackage: mypackage\nTitle: What the Package Does (One Line, Title Case)\nVersion: 0.1.0\nAuthors@R: \n    person(\"First\", \"Last\", , \"first.last@example.com\", role = c(\"aut\", \"cre\"))\nDescription: What the package does (one paragraph).\nLicense: MIT + file LICENSE\nEncoding: UTF-8\nRoxygen: list(markdown = TRUE)\nRoxygenNote: 7.2.3\nSuggests: \n    testthat (&gt;= 3.0.0)\nConfig/testthat/edition: 3"
  },
  {
    "objectID": "research/r-package-development-basics.html#next-steps",
    "href": "research/r-package-development-basics.html#next-steps",
    "title": "R Package Development: From Idea to CRAN",
    "section": "10 Next Steps",
    "text": "10 Next Steps\n\nAdd more functions and documentation\nCreate vignettes for complex workflows\nSet up continuous integration\nSubmit to CRAN when ready"
  },
  {
    "objectID": "research/r-package-development-basics.html#resources",
    "href": "research/r-package-development-basics.html#resources",
    "title": "R Package Development: From Idea to CRAN",
    "section": "11 Resources",
    "text": "11 Resources\n\nR Packages book by Hadley Wickham\nWriting R Extensions manual\nCRAN Policy"
  },
  {
    "objectID": "references/r-commands-cheatsheet.html",
    "href": "references/r-commands-cheatsheet.html",
    "title": "R Commands Quick Reference",
    "section": "",
    "text": "Task\nCommand\nExample\n\n\n\n\nRead CSV\nread.csv()\nread.csv(\"data.csv\")\n\n\nRead Excel\nreadxl::read_excel()\nread_excel(\"data.xlsx\")\n\n\nWrite CSV\nwrite.csv()\nwrite.csv(df, \"output.csv\")\n\n\nSave RDS\nsaveRDS()\nsaveRDS(data, \"data.rds\")\n\n\nLoad RDS\nreadRDS()\nreadRDS(\"data.rds\")"
  },
  {
    "objectID": "references/r-commands-cheatsheet.html#data-importexport",
    "href": "references/r-commands-cheatsheet.html#data-importexport",
    "title": "R Commands Quick Reference",
    "section": "",
    "text": "Task\nCommand\nExample\n\n\n\n\nRead CSV\nread.csv()\nread.csv(\"data.csv\")\n\n\nRead Excel\nreadxl::read_excel()\nread_excel(\"data.xlsx\")\n\n\nWrite CSV\nwrite.csv()\nwrite.csv(df, \"output.csv\")\n\n\nSave RDS\nsaveRDS()\nsaveRDS(data, \"data.rds\")\n\n\nLoad RDS\nreadRDS()\nreadRDS(\"data.rds\")"
  },
  {
    "objectID": "references/r-commands-cheatsheet.html#data-manipulation-dplyr",
    "href": "references/r-commands-cheatsheet.html#data-manipulation-dplyr",
    "title": "R Commands Quick Reference",
    "section": "2 Data Manipulation (dplyr)",
    "text": "2 Data Manipulation (dplyr)\n\n\n\nTask\nCommand\nExample\n\n\n\n\nFilter rows\nfilter()\nfilter(df, age &gt; 18)\n\n\nSelect columns\nselect()\nselect(df, name, age)\n\n\nCreate columns\nmutate()\nmutate(df, age_months = age * 12)\n\n\nGroup data\ngroup_by()\ngroup_by(df, category)\n\n\nSummarize\nsummarise()\nsummarise(df, mean_age = mean(age))\n\n\nSort\narrange()\narrange(df, desc(age))"
  },
  {
    "objectID": "references/r-commands-cheatsheet.html#data-visualization-ggplot2",
    "href": "references/r-commands-cheatsheet.html#data-visualization-ggplot2",
    "title": "R Commands Quick Reference",
    "section": "3 Data Visualization (ggplot2)",
    "text": "3 Data Visualization (ggplot2)\n\n\n\n\n\n\n\n\nTask\nCommand\nExample\n\n\n\n\nScatter plot\ngeom_point()\nggplot(df, aes(x, y)) + geom_point()\n\n\nLine plot\ngeom_line()\nggplot(df, aes(x, y)) + geom_line()\n\n\nBar plot\ngeom_bar()\nggplot(df, aes(x)) + geom_bar()\n\n\nHistogram\ngeom_histogram()\nggplot(df, aes(x)) + geom_histogram()\n\n\nBox plot\ngeom_boxplot()\nggplot(df, aes(x, y)) + geom_boxplot()"
  },
  {
    "objectID": "references/r-commands-cheatsheet.html#statistical-functions",
    "href": "references/r-commands-cheatsheet.html#statistical-functions",
    "title": "R Commands Quick Reference",
    "section": "4 Statistical Functions",
    "text": "4 Statistical Functions\n\n\n\nTask\nCommand\nExample\n\n\n\n\nMean\nmean()\nmean(x, na.rm = TRUE)\n\n\nMedian\nmedian()\nmedian(x, na.rm = TRUE)\n\n\nStandard deviation\nsd()\nsd(x, na.rm = TRUE)\n\n\nCorrelation\ncor()\ncor(x, y, use = \"complete.obs\")\n\n\nLinear model\nlm()\nlm(y ~ x, data = df)\n\n\nANOVA\naov()\naov(y ~ group, data = df)"
  },
  {
    "objectID": "references/r-commands-cheatsheet.html#string-operations",
    "href": "references/r-commands-cheatsheet.html#string-operations",
    "title": "R Commands Quick Reference",
    "section": "5 String Operations",
    "text": "5 String Operations\n\n\n\nTask\nCommand\nExample\n\n\n\n\nConcatenate\npaste()\npaste(\"Hello\", \"World\")\n\n\nSplit string\nstrsplit()\nstrsplit(\"a,b,c\", \",\")\n\n\nFind pattern\ngrep()\ngrep(\"pattern\", x)\n\n\nReplace pattern\ngsub()\ngsub(\"old\", \"new\", x)\n\n\nString length\nnchar()\nnchar(\"hello\")"
  },
  {
    "objectID": "references/r-commands-cheatsheet.html#package-management",
    "href": "references/r-commands-cheatsheet.html#package-management",
    "title": "R Commands Quick Reference",
    "section": "6 Package Management",
    "text": "6 Package Management\n\n\n\nTask\nCommand\nExample\n\n\n\n\nInstall package\ninstall.packages()\ninstall.packages(\"dplyr\")\n\n\nLoad package\nlibrary()\nlibrary(dplyr)\n\n\nUpdate packages\nupdate.packages()\nupdate.packages()\n\n\nList packages\ninstalled.packages()\ninstalled.packages()"
  },
  {
    "objectID": "references/r-commands-cheatsheet.html#workspace-management",
    "href": "references/r-commands-cheatsheet.html#workspace-management",
    "title": "R Commands Quick Reference",
    "section": "7 Workspace Management",
    "text": "7 Workspace Management\n\n\n\nTask\nCommand\nExample\n\n\n\n\nList objects\nls()\nls()\n\n\nRemove objects\nrm()\nrm(x, y)\n\n\nClear workspace\nrm(list = ls())\nrm(list = ls())\n\n\nWorking directory\ngetwd()\ngetwd()\n\n\nSet directory\nsetwd()\nsetwd(\"/path/to/dir\")"
  },
  {
    "objectID": "references/r-commands-cheatsheet.html#data-types-structure",
    "href": "references/r-commands-cheatsheet.html#data-types-structure",
    "title": "R Commands Quick Reference",
    "section": "8 Data Types & Structure",
    "text": "8 Data Types & Structure\n\n\n\nTask\nCommand\nExample\n\n\n\n\nData type\nclass()\nclass(x)\n\n\nStructure\nstr()\nstr(df)\n\n\nDimensions\ndim()\ndim(df)\n\n\nColumn names\nnames()\nnames(df)\n\n\nSummary\nsummary()\nsummary(df)\n\n\nFirst rows\nhead()\nhead(df, 10)\n\n\nLast rows\ntail()\ntail(df, 10)"
  },
  {
    "objectID": "references/r-commands-cheatsheet.html#missing-values",
    "href": "references/r-commands-cheatsheet.html#missing-values",
    "title": "R Commands Quick Reference",
    "section": "9 Missing Values",
    "text": "9 Missing Values\n\n\n\nTask\nCommand\nExample\n\n\n\n\nCheck for NA\nis.na()\nis.na(x)\n\n\nRemove NA\nna.omit()\nna.omit(df)\n\n\nComplete cases\ncomplete.cases()\ncomplete.cases(df)"
  },
  {
    "objectID": "references/r-commands-cheatsheet.html#quick-tips",
    "href": "references/r-commands-cheatsheet.html#quick-tips",
    "title": "R Commands Quick Reference",
    "section": "10 Quick Tips",
    "text": "10 Quick Tips\n\nUse ?function_name to get help\nUse Tab for auto-completion in RStudio\nUse Ctrl+Shift+M for pipe operator %&gt;%\nUse Ctrl+Shift+C to comment/uncomment code"
  },
  {
    "objectID": "posts/zzedcindependence/index.html",
    "href": "posts/zzedcindependence/index.html",
    "title": "Taking Control of Your Clinical Trial: Running ZZedc Independently",
    "section": "",
    "text": "The landscape of clinical research data management is changing. Investigators increasingly want full control over their data, systems, and operational decisions. Yet many commonly used electronic data capture (EDC) systems create vendor dependency: expensive licenses, proprietary data formats, and the uncomfortable reality that your system operates at the pleasure of your vendor.\nThis post documents ZZedc, an open-source, investigator-owned EDC platform designed with a different philosophy. ZZedc runs on infrastructure you control, uses standard databases and formats, costs a fraction of commercial EDC systems, and—most importantly—can be deployed and managed by your research team without ongoing dependency on any biostatistics lab or commercial vendor.\n\n\n\nThe independence problem: Why control matters in clinical research\nZZedc overview: What it is and how it differs from commercial EDC systems\nDeployment paths: From a single investigator’s laptop to multi-site trials on AWS\nTechnical architecture: How ZZedc achieves simplicity and security\nGetting started: Step-by-step guides for different deployment scenarios\nLong-term sustainability: Ongoing maintenance, migration, and data ownership"
  },
  {
    "objectID": "posts/zzedcindependence/index.html#what-this-post-covers",
    "href": "posts/zzedcindependence/index.html#what-this-post-covers",
    "title": "Taking Control of Your Clinical Trial: Running ZZedc Independently",
    "section": "",
    "text": "The independence problem: Why control matters in clinical research\nZZedc overview: What it is and how it differs from commercial EDC systems\nDeployment paths: From a single investigator’s laptop to multi-site trials on AWS\nTechnical architecture: How ZZedc achieves simplicity and security\nGetting started: Step-by-step guides for different deployment scenarios\nLong-term sustainability: Ongoing maintenance, migration, and data ownership"
  },
  {
    "objectID": "posts/zzedcindependence/index.html#why-investigator-control-matters",
    "href": "posts/zzedcindependence/index.html#why-investigator-control-matters",
    "title": "Taking Control of Your Clinical Trial: Running ZZedc Independently",
    "section": "2.1 Why Investigator Control Matters",
    "text": "2.1 Why Investigator Control Matters\nClinical researchers routinely face uncomfortable situations with EDC systems:\nVendor dependency: Your study relies on continued vendor support. If the company changes pricing, goes out of business, or decides to discontinue support, you’re stuck.\nData ownership ambiguity: Proprietary data formats mean your data isn’t truly “yours.” Exporting data can be expensive, slow, or impossible without vendor cooperation.\nOperational constraints: You can’t customize validation rules, reports, or workflows without vendor professional services (and associated costs).\nSecurity concerns: Your patient data sits on vendor infrastructure. You don’t control security, backup locations, or data residency.\nCost escalation: EDC licensing grows with patient volume. A system that costs $20K/year at baseline often costs $50K+ by study completion."
  },
  {
    "objectID": "posts/zzedcindependence/index.html#common-scenarios-where-investigators-break-free",
    "href": "posts/zzedcindependence/index.html#common-scenarios-where-investigators-break-free",
    "title": "Taking Control of Your Clinical Trial: Running ZZedc Independently",
    "section": "2.2 Common Scenarios Where Investigators Break Free",
    "text": "2.2 Common Scenarios Where Investigators Break Free\nIn practice, these situations drive investigators to seek alternatives:\nCollaboration breakup: A productive collaboration with a biostatistics lab ends due to cost disagreements or service level mismatches. The investigator is left with data trapped in a vendor system.\nStudy expansion: A pilot study is successful and scales to multi-site. Existing EDC licensing becomes prohibitively expensive. The investigator needs a more scalable approach.\nRegulatory concerns: The investigator’s institution requires data to be stored in a specific location or under specific security controls that the vendor can’t accommodate.\nLong-term stewardship: The study becomes long-term follow-up. Vendor relationship doesn’t last 5+ years, but the investigator must maintain the system and data indefinitely.\nMethodological evolution: As analysis evolves, the investigator needs flexible validation rules, custom reports, and integration with analysis tools. The vendor system feels restrictive."
  },
  {
    "objectID": "posts/zzedcindependence/index.html#existing-solutions-have-limitations",
    "href": "posts/zzedcindependence/index.html#existing-solutions-have-limitations",
    "title": "Taking Control of Your Clinical Trial: Running ZZedc Independently",
    "section": "2.3 Existing Solutions Have Limitations",
    "text": "2.3 Existing Solutions Have Limitations\n\n\n\n\n\n\n\n\nSolution\nAdvantage\nLimitation\n\n\n\n\nCommercial EDC (REDCap, Medidata)\nPolished interface, vendor support\nHigh cost, vendor lock-in, data ownership questions\n\n\nSpreadsheets (Excel, Google Sheets)\nFamiliar, free\nNo validation, poor audit trail, compliance issues\n\n\nHomebrew databases (Access, FileMaker)\nCustomizable\nNo security, poor scalability, compliance nightmare\n\n\nCustom R/Shiny apps\nCompletely flexible\nRequires skilled programmer, no pre-built features\n\n\nZZedc\nOpen source, low cost, independent\nRequires basic technical setup"
  },
  {
    "objectID": "posts/zzedcindependence/index.html#core-design-philosophy",
    "href": "posts/zzedcindependence/index.html#core-design-philosophy",
    "title": "Taking Control of Your Clinical Trial: Running ZZedc Independently",
    "section": "3.1 Core Design Philosophy",
    "text": "3.1 Core Design Philosophy\nInvestigator-centered: Every design decision prioritizes your control. You own the data, the system, and the infrastructure.\nCloud-native but independent: Deploy on AWS, Azure, Google Cloud, or local servers. You choose the infrastructure provider based on cost and compliance needs.\nSecurity and compliance by default: GDPR and 21 CFR Part 11 compliance frameworks built in. Data encryption, audit trails, and electronic signatures are standard features.\nOpen source, not proprietary: Source code is available on GitHub. If you need customization, you can do it yourself or hire any consultant—you’re not locked into the vendor.\nStandards-based: SQLite databases, YAML configuration, standard web technologies. If you need to migrate away, your data is in standard formats."
  },
  {
    "objectID": "posts/zzedcindependence/index.html#what-zzedc-includes",
    "href": "posts/zzedcindependence/index.html#what-zzedc-includes",
    "title": "Taking Control of Your Clinical Trial: Running ZZedc Independently",
    "section": "3.2 What ZZedc Includes",
    "text": "3.2 What ZZedc Includes\nElectronic data capture with real-time validation: Enter study data with immediate field-level validation. No waiting for data manager review.\nRole-based access control: Five built-in roles (Admin, PI, Coordinator, Data Manager, Monitor) with configurable permissions.\nComprehensive reporting: Basic enrollment reports, quality control summaries, and statistical overviews—all built-in, no custom programming.\nData quality framework: Automated checks for missing data, outliers, and consistency across visits. Nightly QC runs identify issues early.\nAudit trail and compliance: Every action is logged with user, timestamp, and change history. Electronic signatures supported for regulatory studies.\nData export and analysis: Export to CSV, Excel, SPSS, or R. Integrate with your preferred analysis tool directly.\nUser-friendly admin interface: Create users, manage backups, view audit logs—all from the web interface. No command-line expertise required."
  },
  {
    "objectID": "posts/zzedcindependence/index.html#architecture-simplicity-and-transparency",
    "href": "posts/zzedcindependence/index.html#architecture-simplicity-and-transparency",
    "title": "Taking Control of Your Clinical Trial: Running ZZedc Independently",
    "section": "4.1 Architecture: Simplicity and Transparency",
    "text": "4.1 Architecture: Simplicity and Transparency\n┌─────────────────────────────────────────────────┐\n│  Your Infrastructure (AWS, Local, Hybrid)      │\n│                                                  │\n│  ┌──────────────────────────────────────────┐  │\n│  │  Web Browser (Any Location)              │  │\n│  │  https://trial.example.org               │  │\n│  └──────────────────────────────────────────┘  │\n│            ↓ HTTPS (Automatic)                  │\n│  ┌──────────────────────────────────────────┐  │\n│  │  Caddy Reverse Proxy                     │  │\n│  │  (Automatic HTTPS, Let's Encrypt)        │  │\n│  └──────────────────────────────────────────┘  │\n│            ↓ Reverse Proxy                      │\n│  ┌──────────────────────────────────────────┐  │\n│  │  ZZedc R/Shiny Application               │  │\n│  │  (Authentication, Forms, Reporting)      │  │\n│  └──────────────────────────────────────────┘  │\n│            ↓ Database                           │\n│  ┌──────────────────────────────────────────┐  │\n│  │  SQLite Database (Standard Format)       │  │\n│  │  Your Data, Your Control                 │  │\n│  └──────────────────────────────────────────┘  │\n│                                                  │\n└─────────────────────────────────────────────────┘\nKey design points:\n\nAll infrastructure is standard: Docker containers, standard web server (Caddy), open-source database (SQLite)\nNo proprietary components: You’re not dependent on any vendor-specific technology\nTransparent processes: You can see, audit, and modify every part of the system\nPortable data: Your data is in SQLite—you can access it with any SQL tool, analyze it with any tool, migrate it anywhere"
  },
  {
    "objectID": "posts/zzedcindependence/index.html#deployment-paths-flexibility-for-different-needs",
    "href": "posts/zzedcindependence/index.html#deployment-paths-flexibility-for-different-needs",
    "title": "Taking Control of Your Clinical Trial: Running ZZedc Independently",
    "section": "4.2 Deployment Paths: Flexibility for Different Needs",
    "text": "4.2 Deployment Paths: Flexibility for Different Needs\nZZedc supports multiple deployment approaches, depending on your scale and technical resources:\n\n4.2.1 Path 1: Solo Researcher (Local Laptop)\nBest for: Individual investigators, small pilot studies, initial prototyping\nInstall on your personal laptop in 10 minutes:\n# 1. Install ZZedc package from CRAN\ninstall.packages(\"zzedc\")\n\n# 2. Initialize project with interactive setup\nlibrary(zzedc)\nzzedc::init()  # Answers 15 simple questions\n\n# 3. Launch\nlaunch_zzedc()\n# Application opens in browser at http://localhost:3838\nData lives in a local SQLite file (data/zzedc.db). You’re the only user. Backup by copying the file to cloud storage.\nCost: Free (only your laptop electricity) Maintenance: Minimal (you run it when needed)\n\n\n4.2.2 Path 2: Team Research (Single AWS Server)\nBest for: Research team at single institution, collaborative multi-site trial\nDeploy on AWS EC2 in ~15 minutes:\n# 1. Install AWS CLI, configure credentials\n\n# 2. Run deployment script\n./aws_setup.sh \\\n  --region us-west-2 \\\n  --study-name \"Depression Treatment Trial\" \\\n  --study-id \"DEPR-2025-001\" \\\n  --admin-password \"SecurePass123!\" \\\n  --domain trial.example.org \\\n  --instance-type t3.medium\nApplication runs on AWS infrastructure. Multiple team members access via HTTPS. Database backed up automatically.\nCost: ~$30-50/month for EC2 instance (or less, depending on size) Maintenance: Basic (Docker handles updates, Caddy handles HTTPS renewal)\n\n\n4.2.3 Path 3: Enterprise/Multi-Site\nBest for: Large NIH-funded studies, pharmaceutical trials, production deployments\nDeploy across multiple AWS availability zones with load balancing, RDS database, S3 backup, CloudWatch monitoring.\nCost: $200-500/month depending on data volume Maintenance: Automated (infrastructure-as-code, CI/CD pipeline)"
  },
  {
    "objectID": "posts/zzedcindependence/index.html#security-and-compliance",
    "href": "posts/zzedcindependence/index.html#security-and-compliance",
    "title": "Taking Control of Your Clinical Trial: Running ZZedc Independently",
    "section": "4.3 Security and Compliance",
    "text": "4.3 Security and Compliance\nZZedc includes security and compliance frameworks that commercial EDC systems charge extra for:\nGDPR Compliance: - Data subject rights portal (access your data, request deletion) - Purpose limitation (users only see data they need) - Audit trail of all access\n21 CFR Part 11 (FDA): - Electronic signatures with role-based authorization - Immutable audit trail with hash chaining - System validation framework\nSecurity Baseline: - Password encryption with configurable salt - HTTPS with automatic Let’s Encrypt certificates - Role-based access control - Session timeout and concurrent login limits"
  },
  {
    "objectID": "posts/zzedcindependence/index.html#scenario-1-solo-researcher-prototype",
    "href": "posts/zzedcindependence/index.html#scenario-1-solo-researcher-prototype",
    "title": "Taking Control of Your Clinical Trial: Running ZZedc Independently",
    "section": "5.1 Scenario 1: Solo Researcher Prototype",
    "text": "5.1 Scenario 1: Solo Researcher Prototype\nDr. Jane is a clinical psychologist planning a small depression treatment study. She wants to test intervention feasibility before seeking NIH funding.\nHer approach:\n# Install on her laptop\ninstall.packages(\"zzedc\")\nlibrary(zzedc)\n\n# Quick setup (5 minutes of questions)\nzzedc::init()\n# - Study name: \"Depression CBT Pilot\"\n# - Target enrollment: 20\n# - Admin username: jane_smith\n# - Password: (secure password)\n\n# Launch\nlaunch_zzedc()\n# App opens at http://localhost:3838\nResult: Jane has a secure, validated EDC system running locally. She can: - Create forms for baseline, weekly, and endpoint visits - Enroll patients and enter data - Generate enrollment reports - Export data to Excel for analysis - Back up by copying a single file to Dropbox\nCost: $0 Timeline: 15 minutes from zero to collecting data"
  },
  {
    "objectID": "posts/zzedcindependence/index.html#scenario-2-multi-site-trial-migration",
    "href": "posts/zzedcindependence/index.html#scenario-2-multi-site-trial-migration",
    "title": "Taking Control of Your Clinical Trial: Running ZZedc Independently",
    "section": "5.2 Scenario 2: Multi-Site Trial Migration",
    "text": "5.2 Scenario 2: Multi-Site Trial Migration\nThe ADHD research consortium at 5 universities currently uses an expensive commercial EDC that costs $40K/year and is inflexible. They want to migrate to something more affordable and customizable.\nTheir approach:\n# IT staff deploys to AWS\n./aws_setup.sh \\\n  --region us-west-2 \\\n  --study-name \"Multisite ADHD Trial\" \\\n  --study-id \"ADHD-MULTI-2025\" \\\n  --admin-password \"SecurePassword123!\" \\\n  --domain adhd-trial.org \\\n  --instance-type t3.large  # Larger instance for multi-site\nResult: - Single centralized instance accessible from all 5 sites - HTTPS with automatic security certificates - Role-based access: 2 administrators, 5 principal investigators, 20 coordinators, 5 data managers - Data shared securely across institutions - Monthly cost: $40 (95% cheaper than commercial EDC)\nCost: $40/month infrastructure + staff time for administration Timeline: 1 week from decision to enrollment opened"
  },
  {
    "objectID": "posts/zzedcindependence/index.html#scenario-3-individual-investigator-independence",
    "href": "posts/zzedcindependence/index.html#scenario-3-individual-investigator-independence",
    "title": "Taking Control of Your Clinical Trial: Running ZZedc Independently",
    "section": "5.3 Scenario 3: Individual Investigator Independence",
    "text": "5.3 Scenario 3: Individual Investigator Independence\nDr. Robert was running a study with a local Biostatistics Lab that managed his EDC. The lab relationship deteriorated due to cost escalation and support issues. He wants to take control.\nThe situation: - His data is in a commercial EDC (vendor owns the data format) - Migration would cost $20K to export and reformat - He needs a system that’s independent from any vendor - He has basic IT skills but doesn’t want to manage Linux servers\nHis solution: 1. Deploy ZZedc independently on AWS (not through the Biostatistics Lab) bash    ./aws_setup.sh --region us-west-1 --study-name \"Robert's Study\" \\      --study-id \"ROBERT-2025\" --domain robert-study.org\n\nMigrate his data from the old system to ZZedc\n\nOld system exports to CSV\nCSV imported into ZZedc\nData is now in standard SQLite format\n\nComplete his study independently\n\nData entry continues in ZZedc\nAnalysis done with R/Python (direct database access)\nFinal data archived as standard database file\n\nEnd of study\n\nData archived in standard SQLite format to institutional repository\nSystem shut down (delete EC2 instance)\nOngoing data access requires only free SQLite tools\n\n\nResult: Dr. Robert owns his data and system. If he wants to work with another Biostatistics Lab in the future, he can—without vendor lock-in."
  },
  {
    "objectID": "posts/zzedcindependence/index.html#for-solo-researchers",
    "href": "posts/zzedcindependence/index.html#for-solo-researchers",
    "title": "Taking Control of Your Clinical Trial: Running ZZedc Independently",
    "section": "6.1 For Solo Researchers",
    "text": "6.1 For Solo Researchers\nRequirements: R installed, ~5 minutes\n# Step 1: Install\ninstall.packages(\"zzedc\")\n\n# Step 2: Load and initialize\nlibrary(zzedc)\nzzedc::init()\n\n# Step 3: Answer interactive questions\n# The system guides you through setup\n\n# Step 4: Launch\nSys.setenv(ZZEDC_SALT = \"...\")  # (from setup output)\nlaunch_zzedc()\n\n# Step 5: Open browser to http://localhost:3838\n# Step 6: Login with admin credentials you chose\nDetailed instructions: See vignettes/quick-start-solo-researcher.Rmd"
  },
  {
    "objectID": "posts/zzedcindependence/index.html#for-aws-deployment",
    "href": "posts/zzedcindependence/index.html#for-aws-deployment",
    "title": "Taking Control of Your Clinical Trial: Running ZZedc Independently",
    "section": "6.2 For AWS Deployment",
    "text": "6.2 For AWS Deployment\nRequirements: AWS account, AWS CLI, ~15 minutes plus setup time\nStep 1: Prepare\n# Ensure AWS credentials configured\naws sts get-caller-identity\n\n# Gather information\n# - Study name\n# - Study protocol ID\n# - Domain name (e.g., trial.example.org)\n# - Admin password (8+ characters)\nStep 2: Deploy\ncd deployment/\nchmod +x aws_setup.sh\n\n./aws_setup.sh \\\n  --region us-west-2 \\\n  --study-name \"Your Study\" \\\n  --study-id \"YOUR-STUDY-ID\" \\\n  --admin-password \"SecurePassword123!\" \\\n  --domain trial.example.org \\\n  --instance-type t3.medium\nStep 3: Point domain to instance - Wait for DNS to propagate (can take up to 24 hours) - Access application at https://trial.example.org\nDetailed instructions: See deployment/AWS_DEPLOYMENT_GUIDE.md or vignettes/quick-start-aws-devops.Rmd"
  },
  {
    "objectID": "posts/zzedcindependence/index.html#for-migration-from-other-systems",
    "href": "posts/zzedcindependence/index.html#for-migration-from-other-systems",
    "title": "Taking Control of Your Clinical Trial: Running ZZedc Independently",
    "section": "6.3 For Migration from Other Systems",
    "text": "6.3 For Migration from Other Systems\nMigrating from commercial EDC, Excel, or other sources?\n\nExport from source system (typically CSV or Excel)\nImport into ZZedc using the data loader tools\nVerify data quality in ZZedc’s validation interface\nComplete study in ZZedc\nArchive final data as standard SQLite database\n\nDetailed migration guides available in documentation."
  },
  {
    "objectID": "posts/zzedcindependence/index.html#ownership-and-control",
    "href": "posts/zzedcindependence/index.html#ownership-and-control",
    "title": "Taking Control of Your Clinical Trial: Running ZZedc Independently",
    "section": "7.1 Ownership and Control",
    "text": "7.1 Ownership and Control\nWhen you deploy ZZedc, you own: - The infrastructure: Your EC2 instance, your VPC, your data storage - The data: Standard SQLite format, completely portable - The system configuration: You control every setting - The codebase: Open source on GitHub; you can fork and modify if needed"
  },
  {
    "objectID": "posts/zzedcindependence/index.html#maintenance-and-updates",
    "href": "posts/zzedcindependence/index.html#maintenance-and-updates",
    "title": "Taking Control of Your Clinical Trial: Running ZZedc Independently",
    "section": "7.2 Maintenance and Updates",
    "text": "7.2 Maintenance and Updates\nMonthly: - Monitor disk usage and instance health - Check logs for errors\nQuarterly: - Update OS packages on EC2 - Update ZZedc package if new version available - Test backup/restore procedure\nAnnually: - Security audit - Capacity planning (do you need a larger instance?) - Archive completed studies\nSee IT_STAFF_DEPLOYMENT_CHECKLIST.md and IT_STAFF_TROUBLESHOOTING.md for detailed guidance."
  },
  {
    "objectID": "posts/zzedcindependence/index.html#migration-path",
    "href": "posts/zzedcindependence/index.html#migration-path",
    "title": "Taking Control of Your Clinical Trial: Running ZZedc Independently",
    "section": "7.3 Migration Path",
    "text": "7.3 Migration Path\nIf you want to migrate away from ZZedc in the future:\n\nExport database:\n# SQLite is a standard database format\nsqlite3 zzedc.db \".dump\" &gt; database_export.sql\nAccess with any tool: Your data can be accessed by any SQL tool, Python, R, Stata, SAS, etc.\nComplete ownership: No vendor locks, no proprietary formats"
  },
  {
    "objectID": "posts/zzedcindependence/index.html#next-steps",
    "href": "posts/zzedcindependence/index.html#next-steps",
    "title": "Taking Control of Your Clinical Trial: Running ZZedc Independently",
    "section": "9.1 Next Steps",
    "text": "9.1 Next Steps\nStart exploring: - Solo researcher? See vignettes/quick-start-solo-researcher.Rmd - Team with AWS? See vignettes/quick-start-aws-devops.Rmd - IT staff deploying? See deployment/IT_STAFF_DEPLOYMENT_CHECKLIST.md - Need help? Visit https://github.com/rgt47/zzedc for documentation and issues\nThe goal of ZZedc is simple: Put clinical research data management back in the hands of investigators."
  },
  {
    "objectID": "posts/zzedcindependence/index.html#resources",
    "href": "posts/zzedcindependence/index.html#resources",
    "title": "Taking Control of Your Clinical Trial: Running ZZedc Independently",
    "section": "9.2 Resources",
    "text": "9.2 Resources\n\nZZedc GitHub: https://github.com/rgt47/zzedc\nDocumentation: See vignettes/ directory for comprehensive guides\nDeployment guides: See deployment/ directory for AWS, Docker, and operational guides\nSupport: Open an issue on GitHub or contact zzedc@ucsd.edu\n\n\nPublished: December 2025 Author: Clinical Research Technology Team License: Open source (see GitHub repository for license details) Status: Production ready"
  },
  {
    "objectID": "posts/templatepost/index.html",
    "href": "posts/templatepost/index.html",
    "title": "Your Engaging Title Here: A Learning Journey",
    "section": "",
    "text": "Engaging hero image that introduces your topic visually\nPhoto caption with attribution if needed. This image sets the visual tone for your entire post."
  },
  {
    "objectID": "posts/templatepost/index.html#motivations",
    "href": "posts/templatepost/index.html#motivations",
    "title": "Your Engaging Title Here: A Learning Journey",
    "section": "1.1 Motivations",
    "text": "1.1 Motivations\n\nWhy explore [topic]? - [Personal reason 1: specific problem you faced] - [Practical need 2: gap in your workflow] - [Learning goal 3: skill you wanted to develop] - [Curiosity 4: interesting question you had]"
  },
  {
    "objectID": "posts/templatepost/index.html#objectives",
    "href": "posts/templatepost/index.html#objectives",
    "title": "Your Engaging Title Here: A Learning Journey",
    "section": "1.2 Objectives",
    "text": "1.2 Objectives\n\nWhat I wanted to accomplish: 1. [Specific, measurable objective 1] 2. [Specific, measurable objective 2] 3. [Specific, measurable objective 3] 4. [Stretch goal or advanced concept]\nDisclaimer: I’m documenting my learning process here. If you spot errors or have better approaches, please let me know! 💙\n\n\n\n\nAtmospheric image to maintain visual engagement - replace with relevant scene"
  },
  {
    "objectID": "posts/templatepost/index.html#looking-for-relationships",
    "href": "posts/templatepost/index.html#looking-for-relationships",
    "title": "Your Engaging Title Here: A Learning Journey",
    "section": "5.1 Looking for Relationships",
    "text": "5.1 Looking for Relationships\n\n# Find strongest correlations with MPG\ncorrelations &lt;- cor(mtcars_clean %&gt;% select(where(is.numeric))) %&gt;%\n  as.data.frame() %&gt;%\n  rownames_to_column(\"var1\") %&gt;%\n  pivot_longer(-var1, names_to = \"var2\", values_to = \"correlation\") %&gt;%\n  filter(var1 == \"mpg\", var2 != \"mpg\") %&gt;%\n  arrange(desc(abs(correlation)))\n\n# Display top 5\nkable(correlations %&gt;% head(5),\n      caption = \"Top 5 Correlations with MPG (fuel efficiency)\")\n\n🔍 Weight has the strongest correlation with MPG (r = -0.87). Let’s visualize that relationship:\n\nknitr::include_graphics(\"figures/correlation-plot.png\")\n\n\n\n\nStrong negative relationship between vehicle weight and fuel efficiency. Heavier cars consistently get worse mileage, regardless of cylinder count. The fitted regression line (dashed) shows the overall trend.\n\n\n\n\nInteresting! Heavier cars consistently get worse mileage. Makes sense when you think about it 🚗"
  },
  {
    "objectID": "posts/templatepost/index.html#making-predictions",
    "href": "posts/templatepost/index.html#making-predictions",
    "title": "Your Engaging Title Here: A Learning Journey",
    "section": "6.1 Making Predictions",
    "text": "6.1 Making Predictions\n\nLet me make some predictions to see how this works in practice:\n\n# Predict MPG for different weights\nnew_data &lt;- tibble(wt = c(2, 3, 4))\nmodel &lt;- readRDS(\"data/derived_data/simple_model.rds\")\npredictions &lt;- predict(model, newdata = new_data, interval = \"confidence\")\n\ncbind(new_data, predictions) %&gt;%\n  kable(digits = 2,\n        caption = \"Predicted MPG for Vehicles of Different Weights\")\n\n📝 So a 2,000 lb car gets ~30 MPG, while a 4,000 lb car only gets ~15 MPG. That’s quite a difference!"
  },
  {
    "objectID": "posts/templatepost/index.html#things-to-watch-out-for",
    "href": "posts/templatepost/index.html#things-to-watch-out-for",
    "title": "Your Engaging Title Here: A Learning Journey",
    "section": "7.1 Things to Watch Out For",
    "text": "7.1 Things to Watch Out For\n\nA few gotchas I encountered while working on this:\n\nDon’t extrapolate too far - This model works for weights between 1.5-5.5 thousand lbs. Predicting outside that range? Risky!\nCorrelation ≠ Causation - Weight correlates with MPG, but there are confounding variables (engine size, aerodynamics, etc.)\nCheck your assumptions - Always plot residuals! A good R² doesn’t guarantee your model is appropriate.\nSmall sample size - We only have 32 cars. Take the confidence intervals seriously!\n\n\n\n\n\nConcluding visual - tie back to topic theme"
  },
  {
    "objectID": "posts/templatepost/index.html#lessons-learnt",
    "href": "posts/templatepost/index.html#lessons-learnt",
    "title": "Your Engaging Title Here: A Learning Journey",
    "section": "8.1 Lessons Learnt",
    "text": "8.1 Lessons Learnt\nHere’s what I took away from this exploration:\nConceptual Understanding: - Vehicle weight is a strong predictor of fuel efficiency (R² = 0.75) - Each 1,000 lbs reduces MPG by ~5.3 miles (95% CI: [-6.5, -4.1]) - Cylinder count effects are partially mediated through weight - Simple models can be surprisingly effective with the right predictor\nTechnical Skills: - Using broom::tidy() for clean model output formatting ✅ - Calculating and interpreting confidence intervals for predictions - Creating diagnostic plots to validate regression assumptions - Combining multiple ggplot visualizations with patchwork\nGotchas and Pitfalls: - Always check residual plots - R² alone isn’t enough! - Extrapolation beyond data range is dangerous - Small sample sizes (n=32) require cautious interpretation - Correlation doesn’t prove causation (confounding variables matter)"
  },
  {
    "objectID": "posts/templatepost/index.html#limitations",
    "href": "posts/templatepost/index.html#limitations",
    "title": "Your Engaging Title Here: A Learning Journey",
    "section": "8.2 Limitations",
    "text": "8.2 Limitations\n\nThis analysis has several limitations to keep in mind:\n\nOld data: mtcars is from 1974 - modern vehicles (hybrids, EVs) behave differently\nSmall sample: Only 32 observations limits statistical power\nMissing variables: Doesn’t account for aerodynamics, transmission type, engine tech\nSimple model: Single predictor ignores important confounders\nLimited scope: Only passenger cars; may not generalize to trucks/SUVs"
  },
  {
    "objectID": "posts/templatepost/index.html#opportunities-for-improvement",
    "href": "posts/templatepost/index.html#opportunities-for-improvement",
    "title": "Your Engaging Title Here: A Learning Journey",
    "section": "8.3 Opportunities for Improvement",
    "text": "8.3 Opportunities for Improvement\n\nIf I had more time, here’s what I’d explore next:\n\nMultiple regression - Add cylinder count, horsepower, transmission type\nInteraction effects - Does weight impact differ by number of cylinders?\nModern data - Replicate with 2020+ vehicle data to see how relationships changed\nNon-linear models - Try polynomial regression or splines for better fit\nMachine learning comparison - How does linear regression compare to random forest?\nCausal inference - Use techniques to establish causality, not just correlation"
  },
  {
    "objectID": "posts/tableplacementrmarkdown/index.html",
    "href": "posts/tableplacementrmarkdown/index.html",
    "title": "Converting R data.frames to pdf for better placement control in latex draft: true pdf report",
    "section": "",
    "text": "purrr"
  },
  {
    "objectID": "posts/tableplacementrmarkdown/index.html#prerequisites",
    "href": "posts/tableplacementrmarkdown/index.html#prerequisites",
    "title": "Converting R data.frames to pdf for better placement control in latex draft: true pdf report",
    "section": "1.1 Prerequisites",
    "text": "1.1 Prerequisites\nIn development"
  },
  {
    "objectID": "posts/tableplacementrmarkdown/index.html#step-by-step-implementation",
    "href": "posts/tableplacementrmarkdown/index.html#step-by-step-implementation",
    "title": "Converting R data.frames to pdf for better placement control in latex draft: true pdf report",
    "section": "1.2 Step-by-Step Implementation",
    "text": "1.2 Step-by-Step Implementation\nIn development"
  },
  {
    "objectID": "posts/tableplacementrmarkdown/index.html#key-takeaways",
    "href": "posts/tableplacementrmarkdown/index.html#key-takeaways",
    "title": "Converting R data.frames to pdf for better placement control in latex draft: true pdf report",
    "section": "1.3 Key Takeaways",
    "text": "1.3 Key Takeaways\nIn development"
  },
  {
    "objectID": "posts/tableplacementrmarkdown/index.html#further-reading",
    "href": "posts/tableplacementrmarkdown/index.html#further-reading",
    "title": "Converting R data.frames to pdf for better placement control in latex draft: true pdf report",
    "section": "1.4 Further Reading",
    "text": "1.4 Further Reading\nIn development"
  },
  {
    "objectID": "posts/shinyvsobservable/index.html",
    "href": "posts/shinyvsobservable/index.html",
    "title": "Combining Observable JS and Shiny in One Quarto Document",
    "section": "",
    "text": "Penguins make great companions for learning new technologies\nCombining two reactive frameworks in one document: harder than it looks!"
  },
  {
    "objectID": "posts/shinyvsobservable/index.html#motivations",
    "href": "posts/shinyvsobservable/index.html#motivations",
    "title": "Combining Observable JS and Shiny in One Quarto Document",
    "section": "1.1 Motivations",
    "text": "1.1 Motivations\nWhy explore this combination?\n\nWanted to compare client-side (OJS) vs server-side (Shiny) reactive frameworks\nNeeded to demonstrate both technologies to colleagues in a single document\nCurious whether Quarto’s multi-language support extends to mixing reactive systems\nCouldn’t find good documentation on combining these two approaches"
  },
  {
    "objectID": "posts/shinyvsobservable/index.html#objectives",
    "href": "posts/shinyvsobservable/index.html#objectives",
    "title": "Combining Observable JS and Shiny in One Quarto Document",
    "section": "1.2 Objectives",
    "text": "1.2 Objectives\nWhat I wanted to accomplish:\n\nCreate identical interactive visualizations using both Observable JS and Shiny\nDisplay them side-by-side in a single Quarto document\nDocument the gotchas and workarounds for others\nUnderstand the fundamental differences between client-side and server-side reactivity\n\nDisclaimer: I’m documenting my learning process here. If you spot errors or have better approaches, please let me know!"
  },
  {
    "objectID": "posts/shinyvsobservable/index.html#challenge-1-fileattachment-doesnt-work",
    "href": "posts/shinyvsobservable/index.html#challenge-1-fileattachment-doesnt-work",
    "title": "Combining Observable JS and Shiny in One Quarto Document",
    "section": "4.1 Challenge 1: FileAttachment Doesn’t Work",
    "text": "4.1 Challenge 1: FileAttachment Doesn’t Work\nOur first attempt used Observable’s standard data loading:\n// THIS DOESN'T WORK IN SHINY DOCUMENTS!\ndata = FileAttachment(\"palmer-penguins.csv\").csv({typed: true})\nResult: OJS Error: Unable to load file\nWhy: When you add server: shiny, the document runs as a Shiny app. The Shiny server doesn’t know about Observable’s file attachment system.\n\n\n\n\n\n\nWarningGotcha #1\n\n\n\nFileAttachment() does NOT work in Shiny documents. This is the biggest “gotcha” when combining OJS and Shiny."
  },
  {
    "objectID": "posts/shinyvsobservable/index.html#challenge-2-ojs_define-fails-too",
    "href": "posts/shinyvsobservable/index.html#challenge-2-ojs_define-fails-too",
    "title": "Combining Observable JS and Shiny in One Quarto Document",
    "section": "4.2 Challenge 2: ojs_define() Fails Too",
    "text": "4.2 Challenge 2: ojs_define() Fails Too\nThe Quarto docs mention ojs_define() for passing data from R to OJS:\n#| context: server\ndata &lt;- read.csv(\"penguins.csv\")\nojs_define(my_data = data)  # THIS FAILS!\nResult: Error: Unexpected data.frame object... Did you forget to use a render function?\nWhy: In Shiny context, ojs_define() expects reactive expressions, not static data frames.\n\n\n\n\n\n\nWarningGotcha #2\n\n\n\nojs_define() behaves differently in Shiny documents. Don’t expect it to work like the docs show for regular Quarto."
  },
  {
    "objectID": "posts/shinyvsobservable/index.html#challenge-3-local-d3.csv-returns-404",
    "href": "posts/shinyvsobservable/index.html#challenge-3-local-d3.csv-returns-404",
    "title": "Combining Observable JS and Shiny in One Quarto Document",
    "section": "4.3 Challenge 3: Local d3.csv() Returns 404",
    "text": "4.3 Challenge 3: Local d3.csv() Returns 404\nWe tried D3’s CSV loader:\n// THIS ALSO DOESN'T WORK!\ndata = d3.csv(\"palmer-penguins.csv\", d3.autoType)\nResult: OJS Error: 404 Not Found\nWhy: The Shiny server doesn’t serve local static files the way Observable expects."
  },
  {
    "objectID": "posts/shinyvsobservable/index.html#the-solution-fetch-from-public-url",
    "href": "posts/shinyvsobservable/index.html#the-solution-fetch-from-public-url",
    "title": "Combining Observable JS and Shiny in One Quarto Document",
    "section": "4.4 The Solution: Fetch from Public URL",
    "text": "4.4 The Solution: Fetch from Public URL\nThe only reliable approach is fetching data from a public URL:\n// THIS WORKS!\npenguins_raw = {\n  const response = await fetch(\n    \"https://raw.githubusercontent.com/.../penguins.csv\"\n  );\n  const text = await response.text();\n  return d3.csvParse(text, d3.autoType);\n}\nThis bypasses the Shiny server entirely by making an HTTP request to an external source."
  },
  {
    "objectID": "posts/shinyvsobservable/index.html#do",
    "href": "posts/shinyvsobservable/index.html#do",
    "title": "Combining Observable JS and Shiny in One Quarto Document",
    "section": "5.1 DO",
    "text": "5.1 DO\n\nUse fetch() with public URLs for OJS data in Shiny documents\nKeep OJS and Shiny data loading separate—don’t try to share data\nTest incrementally—get Shiny working first, then add OJS\nHard refresh your browser (Cmd+Shift+R) when debugging\nRestart the preview server after major changes"
  },
  {
    "objectID": "posts/shinyvsobservable/index.html#dont",
    "href": "posts/shinyvsobservable/index.html#dont",
    "title": "Combining Observable JS and Shiny in One Quarto Document",
    "section": "5.2 DON’T",
    "text": "5.2 DON’T\n\nDon’t use FileAttachment() in Shiny documents\nDon’t use ojs_define() with static data in Shiny context\nDon’t assume d3.csv(\"local.csv\") will work\nDon’t trust displayed code—browser caching causes confusion\nDon’t mix contexts—keep OJS and Shiny logic separate"
  },
  {
    "objectID": "posts/shinyvsobservable/index.html#browser-caching",
    "href": "posts/shinyvsobservable/index.html#browser-caching",
    "title": "Combining Observable JS and Shiny in One Quarto Document",
    "section": "6.1 Browser Caching",
    "text": "6.1 Browser Caching\nWhen debugging, the browser often shows old code. The displayed code might show FileAttachment(...) even though your file now uses fetch().\nSolution: Kill server, delete generated files, restart, hard-refresh:\nrm -rf yourfile_files yourfile.html\nquarto preview yourfile.qmd\n# Then Cmd+Shift+R in browser"
  },
  {
    "objectID": "posts/shinyvsobservable/index.html#port-changes",
    "href": "posts/shinyvsobservable/index.html#port-changes",
    "title": "Combining Observable JS and Shiny in One Quarto Document",
    "section": "6.2 Port Changes",
    "text": "6.2 Port Changes\nEvery restart may give a different port (5155, 6532, 7665…). Check the terminal output for the correct URL."
  },
  {
    "objectID": "posts/shinyvsobservable/index.html#misleading-error-messages",
    "href": "posts/shinyvsobservable/index.html#misleading-error-messages",
    "title": "Combining Observable JS and Shiny in One Quarto Document",
    "section": "6.3 Misleading Error Messages",
    "text": "6.3 Misleading Error Messages\nThe error “Did you forget to use a render function?” doesn’t mean you need renderSomething(). It means ojs_define() doesn’t work with static data in Shiny context."
  },
  {
    "objectID": "posts/shinyvsobservable/index.html#limitations",
    "href": "posts/shinyvsobservable/index.html#limitations",
    "title": "Combining Observable JS and Shiny in One Quarto Document",
    "section": "8.1 Limitations",
    "text": "8.1 Limitations\n\nThis approach requires external data hosting for OJS\nNo data sharing between OJS and Shiny contexts\nDebugging is significantly harder than single-framework documents\nDocumentation doesn’t cover this edge case well"
  },
  {
    "objectID": "posts/shinyvsobservable/index.html#opportunities-for-improvement",
    "href": "posts/shinyvsobservable/index.html#opportunities-for-improvement",
    "title": "Combining Observable JS and Shiny in One Quarto Document",
    "section": "8.2 Opportunities for Improvement",
    "text": "8.2 Opportunities for Improvement\nIf I had more time, here’s what I’d explore next:\n\nTest ojs_define() with reactive expressions - might work with reactive() wrapper\nCreate a helper function to bridge OJS and Shiny data\nExplore Shiny’s resource serving - maybe there’s a way to make local files available to OJS\nDocument a clean workflow for teams who need both frameworks"
  },
  {
    "objectID": "posts/shinyvsobservable/analysis/paper/penguins_observable.html",
    "href": "posts/shinyvsobservable/analysis/paper/penguins_observable.html",
    "title": "Penguins",
    "section": "",
    "text": "A simple example based on Allison Horst’s Palmer Penguins dataset. Here we look at how penguin body mass varies across both sex and species (use the provided inputs to filter the dataset by bill length and island):\n\n\n\nCode\nviewof bill_length_min = Inputs.range(\n  [32, 50],\n  {value: 35, step: 1, label: \"Bill length (min):\"}\n)\n\n// -----------------------------------------------------------------------------\n// CHECKBOX INPUT\n// -----------------------------------------------------------------------------\n// Inputs.checkbox() creates a group of checkboxes\n//\n// Syntax: Inputs.checkbox(options_array, options)\n//\n// Arguments:\n//   [\"Torgersen\", \"Biscoe\", \"Dream\"] → The available choices to display\n//\n// Options object:\n//   value: [\"Torgersen\", \"Biscoe\"] → Initially selected values (array)\n//   label: \"Islands:\"              → Text label for the checkbox group\n//\n// The \"islands\" variable will contain an array of currently checked values\n// e.g., [\"Torgersen\", \"Biscoe\"] or [\"Dream\"] or [] (empty if none selected)\n\nviewof islands = Inputs.checkbox(\n  [\"Torgersen\", \"Biscoe\", \"Dream\"],\n  { value: [\"Torgersen\", \"Biscoe\"],\n    label: \"Islands:\"\n  }\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlotData\n\n\n\n\n\nCode\n// -----------------------------------------------------------------------------\n// OBSERVABLE PLOT - FACETED HISTOGRAM\n// -----------------------------------------------------------------------------\n// This creates a histogram of penguin body mass, faceted by sex and species.\n//\n// PLOT STRUCTURE (read from inside out):\n//\n// 1. Plot.binX({y: \"count\"}, {...})\n//    - binX groups continuous x-values into bins (like a histogram)\n//    - {y: \"count\"} means the y-axis shows the count of items in each bin\n//    - x: \"body_mass_g\" → bin by body mass\n//    - fill: \"species\"  → color bars by species\n//    - thresholds: 20   → create approximately 20 bins\n//\n// 2. Plot.rectY(filtered, ...)\n//    - rectY draws vertical rectangles (bars)\n//    - \"filtered\" is our reactive data (defined below)\n//    - The bars represent the binned counts\n//\n// 3. .plot({...})\n//    - Configures the overall plot layout\n//    - facet: splits the plot into a grid of small multiples\n//      - data: the dataset to facet\n//      - x: \"sex\"     → columns are male/female\n//      - y: \"species\" → rows are Adelie/Chinstrap/Gentoo\n//      - marginRight: 80 → space for labels on the right\n//    - marks: additional visual elements\n//      - Plot.frame() draws a border around each facet\n\nPlot.rectY(filtered,\n  Plot.binX(\n    {y: \"count\"},\n    {x: \"body_mass_g\", fill: \"species\", thresholds: 20}\n  ))\n  .plot({\n    facet: {\n      data: filtered,\n      x: \"sex\",\n      y: \"species\",\n      marginRight: 80\n    },\n    marks: [\n      Plot.frame(),\n    ]\n  }\n)\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n// -----------------------------------------------------------------------------\n// INTERACTIVE DATA TABLE\n// -----------------------------------------------------------------------------\n// Inputs.table() displays data in a sortable, scrollable table\n//\n// Features:\n//   - Click column headers to sort\n//   - Scroll through rows\n//   - Automatically updates when \"filtered\" data changes\n//\n// This is useful for:\n//   - Debugging (see what data the plot is using)\n//   - Allowing users to explore individual records\n\nInputs.table(filtered)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n// -----------------------------------------------------------------------------\n// LOAD DATA FROM CSV FILE\n// -----------------------------------------------------------------------------\n// FileAttachment() loads files that are in the same directory as this document\n//\n// .csv({ typed: true }) parses the CSV with automatic type inference:\n//   - Numbers become JavaScript numbers (not strings)\n//   - This is important for plotting and filtering numeric values\n//\n// The \"data\" variable will contain an array of objects, where each object\n// represents one row (one penguin), like:\n// {\n//   species: \"Adelie\",\n//   island: \"Torgersen\",\n//   bill_length_mm: 39.1,\n//   bill_depth_mm: 18.7,\n//   flipper_length_mm: 181,\n//   body_mass_g: 3750,\n//   sex: \"male\",\n//   year: 2007\n// }\n\ndata = FileAttachment(\"palmer-penguins.csv\").csv({ typed: true })\n\n\n\n\n\n\n\n\n\n\nCode\n// -----------------------------------------------------------------------------\n// FILTER DATA BASED ON USER INPUTS\n// -----------------------------------------------------------------------------\n// This variable \"filtered\" depends on:\n//   1. data (the full dataset)\n//   2. bill_length_min (from the slider)\n//   3. islands (from the checkboxes)\n//\n// REACTIVITY: Whenever ANY of these dependencies change, Observable\n// automatically re-runs this code block and updates \"filtered\".\n// Then, anything that depends on \"filtered\" (the plot, the table)\n// also updates automatically!\n//\n// The filter function:\n//   - data.filter() returns a new array with only matching items\n//   - For each penguin, we check two conditions:\n//     1. bill_length_min &lt; penguin.bill_length_mm\n//        → Keep penguins with bill length GREATER than the slider value\n//     2. islands.includes(penguin.island)\n//        → Keep penguins from islands that are currently checked\n//   - Both conditions must be true (&&) for a penguin to be included\n\nfiltered = data.filter(function(penguin) {\n  return bill_length_min &lt; penguin.bill_length_mm &&\n         islands.includes(penguin.island);\n})\n\n// ALTERNATIVE: Using arrow function syntax (more modern JavaScript):\n// filtered = data.filter(penguin =&gt;\n//   bill_length_min &lt; penguin.bill_length_mm &&\n//   islands.includes(penguin.island)\n// )\n\n\n\n\n\n\n\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{(ryy)_glenn_thomas,\n  author = {(Ryy) Glenn Thomas, Ronald},\n  title = {Penguins},\n  url = {https://focusonr.org/posts/shinyvsobservable/analysis/paper/penguins_observable.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\n(Ryy) Glenn Thomas, Ronald. n.d. “Penguins.” https://focusonr.org/posts/shinyvsobservable/analysis/paper/penguins_observable.html."
  },
  {
    "objectID": "posts/shinyvsobservable/analysis/paper/comparison_whitepaper.html",
    "href": "posts/shinyvsobservable/analysis/paper/comparison_whitepaper.html",
    "title": "Observable JS vs Shiny for Interactive Statistical Reports",
    "section": "",
    "text": "Interactive data visualization has become essential for modern statistical reporting. Two leading technologies for creating interactive reports within the Quarto ecosystem are Observable JavaScript (OJS) and R Shiny. This white paper provides a comprehensive comparison to help statisticians and data analysts choose the appropriate technology for their reporting needs.\n\n\n\n\n\n\nNoteKey Takeaway\n\n\n\nObservable JS excels for self-contained, client-side reports that need broad distribution. Shiny excels when you need R’s statistical ecosystem or server-side computation for sensitive data."
  },
  {
    "objectID": "posts/shinyvsobservable/analysis/paper/comparison_whitepaper.html#executive-summary",
    "href": "posts/shinyvsobservable/analysis/paper/comparison_whitepaper.html#executive-summary",
    "title": "Observable JS vs Shiny for Interactive Statistical Reports",
    "section": "",
    "text": "Interactive data visualization has become essential for modern statistical reporting. Two leading technologies for creating interactive reports within the Quarto ecosystem are Observable JavaScript (OJS) and R Shiny. This white paper provides a comprehensive comparison to help statisticians and data analysts choose the appropriate technology for their reporting needs.\n\n\n\n\n\n\nNoteKey Takeaway\n\n\n\nObservable JS excels for self-contained, client-side reports that need broad distribution. Shiny excels when you need R’s statistical ecosystem or server-side computation for sensitive data."
  },
  {
    "objectID": "posts/shinyvsobservable/analysis/paper/comparison_whitepaper.html#introduction",
    "href": "posts/shinyvsobservable/analysis/paper/comparison_whitepaper.html#introduction",
    "title": "Observable JS vs Shiny for Interactive Statistical Reports",
    "section": "2 Introduction",
    "text": "2 Introduction\n\n2.1 The Evolution of Statistical Reporting\nTraditional statistical reports were static documents—PDF or Word files with fixed tables and figures. Modern stakeholders increasingly expect interactive elements that allow them to:\n\nFilter data by subgroups\nExplore different parameter values\nDrill down into specific observations\nExport customized views\n\nQuarto, the next-generation publishing system from Posit, supports both Observable JS and Shiny as interactivity backends, giving statisticians flexibility in their choice of technology.\n\n\n2.2 Scope of This Comparison\nThis paper compares Observable JS and Shiny across dimensions critical to statistical reporting:\n\nTechnical architecture\nStatistical computing capabilities\nDeployment and distribution\nPerformance characteristics\nLearning curve and development experience\nSecurity and compliance considerations"
  },
  {
    "objectID": "posts/shinyvsobservable/analysis/paper/comparison_whitepaper.html#technical-architecture",
    "href": "posts/shinyvsobservable/analysis/paper/comparison_whitepaper.html#technical-architecture",
    "title": "Observable JS vs Shiny for Interactive Statistical Reports",
    "section": "3 Technical Architecture",
    "text": "3 Technical Architecture\n\n3.1 Observable JS: Client-Side Execution\n\n\nCode\nflowchart LR\n    A[Quarto Doc] --&gt; B[Static HTML/JS]\n    B --&gt; C[Browser]\n    C --&gt; D[Computation]\n\n\n\n\n\nflowchart LR\n    A[Quarto Doc] --&gt; B[Static HTML/JS]\n    B --&gt; C[Browser]\n    C --&gt; D[Computation]\n\n\n Observable JS Architecture \n\n\n\nObservable JS compiles to static HTML and JavaScript files. When a user opens the report:\n\nAll code executes in the browser\nData is embedded in the document or fetched via HTTP\nNo server infrastructure required after initial hosting\nReactivity is handled by Observable’s runtime\n\n\n\n3.2 Shiny: Server-Side Execution\n\n\nCode\nflowchart LR\n    A[Browser] &lt;--&gt; B[Shiny Server]\n    B &lt;--&gt; C[R Session]\n    C &lt;--&gt; D[DB/APIs]\n\n\n\n\n\nflowchart LR\n    A[Browser] &lt;--&gt; B[Shiny Server]\n    B &lt;--&gt; C[R Session]\n    C &lt;--&gt; D[DB/APIs]\n\n\n Shiny Architecture \n\n\n\nShiny maintains a persistent R session on a server:\n\nCode executes on the server\nOnly UI updates are sent to the browser\nRequires running server infrastructure\nFull access to R’s computing environment\n\n\n\n3.3 Architectural Comparison\n\n\n\nAspect\nObservable JS\nShiny\n\n\n\n\nExecution\nClient (browser)\nServer (R)\n\n\nState\nBrowser memory\nServer memory\n\n\nConcurrency\nUnlimited\nLimited by server\n\n\nNetwork\nInitial load only\nContinuous\n\n\nOffline\nYes (once loaded)\nNo"
  },
  {
    "objectID": "posts/shinyvsobservable/analysis/paper/comparison_whitepaper.html#statistical-computing-capabilities",
    "href": "posts/shinyvsobservable/analysis/paper/comparison_whitepaper.html#statistical-computing-capabilities",
    "title": "Observable JS vs Shiny for Interactive Statistical Reports",
    "section": "4 Statistical Computing Capabilities",
    "text": "4 Statistical Computing Capabilities\n\n4.1 R/Shiny: The Statistical Powerhouse\nShiny provides direct access to R’s comprehensive statistical ecosystem:\nAdvantages:\n\n20,000+ CRAN packages including specialized methods for:\n\nSurvival analysis (survival, survminer)\nMixed-effects models (lme4, nlme)\nBayesian inference (brms, rstanarm)\nCausal inference (MatchIt, WeightIt)\nMeta-analysis (metafor, meta)\n\nEstablished validation in regulated industries (pharma, finance)\nFamiliar syntax for statisticians already using R\nHeavy computation can run on powerful servers, not limited by browser\n\n\n\nCode\n# Example: Complex statistical model in Shiny\n# This runs on the server with full R capabilities\noutput$model_summary &lt;- renderPrint({\n  model &lt;- lme4::lmer(\n    outcome ~ treatment * time + (1 | subject),\n    data = filtered_data()\n  )\n  summary(model)\n})\n\n\n\n\n4.2 Observable JS: Modern Visualization Focus\nObservable provides excellent visualization with growing statistical capabilities:\nAdvantages:\n\nObservable Plot: Elegant, grammar-of-graphics style plotting\nD3.js integration: Unlimited customization potential\nArquero: dplyr-like data transformation in JavaScript\nStatistical libraries: simple-statistics, jstat for basic analyses\n\nLimitations:\n\nFewer specialized statistical methods\nLess validation history in regulated environments\nComplex models require custom implementation or API calls\n\n\n\nCode\nimport { mean, standardDeviation, linearRegression } from \"simple-statistics\"\n\nstats = ({\n  mean: mean(data.map(d =&gt; d.value)),\n  sd: standardDeviation(data.map(d =&gt; d.value)),\n  regression: linearRegression(data.map(d =&gt; [d.x, d.y]))\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.3 Statistical Capability Matrix\n\n\n\nCapability\nObservable JS\nShiny\n\n\n\n\nDescriptive statistics\n✓\n✓\n\n\nLinear regression\n✓\n✓\n\n\nGeneralized linear models\nLimited\n✓\n\n\nMixed-effects models\n✗\n✓\n\n\nSurvival analysis\n✗\n✓\n\n\nBayesian inference\n✗\n✓\n\n\nTime series (ARIMA, etc.)\nLimited\n✓\n\n\nMachine learning\nLimited\n✓\n\n\nCustom visualizations\n✓✓\n✓"
  },
  {
    "objectID": "posts/shinyvsobservable/analysis/paper/comparison_whitepaper.html#deployment-and-distribution",
    "href": "posts/shinyvsobservable/analysis/paper/comparison_whitepaper.html#deployment-and-distribution",
    "title": "Observable JS vs Shiny for Interactive Statistical Reports",
    "section": "5 Deployment and Distribution",
    "text": "5 Deployment and Distribution\n\n5.1 Observable JS: Simplicity and Portability\nDeployment options:\n\nStatic file hosting: Any web server, S3, GitHub Pages\nEmail attachment: Self-contained HTML file\nShared drives: No server needed\nQuarto Pub: Free hosting from Posit\n\nAdvantages for report distribution:\n\nRecipients need only a web browser\nNo IT infrastructure required\nWorks behind firewalls (no external connections)\nCan be archived as single files\nNo ongoing server costs\n\n# Generate self-contained HTML\nquarto render report.qmd --to html\n\n# Result: Single HTML file with embedded data and code\n# File size: Typically 1-10 MB depending on data\n\n\n5.2 Shiny: Server Infrastructure Required\nDeployment options:\n\nshinyapps.io: Managed hosting (free tier available)\nPosit Connect: Enterprise server\nShiny Server: Self-hosted (open source or pro)\nDocker/Kubernetes: Container deployment\n\nConsiderations:\n\nRequires ongoing server maintenance\nPer-user resource consumption\nSession management for concurrent users\nAuthentication integration needed for sensitive reports\n\n# Deploy to shinyapps.io\nrsconnect::deployDoc(\"report.qmd\")\n\n# Or render for Posit Connect\nquarto render report.qmd\n# Then publish via Connect interface\n\n\n5.3 Deployment Decision Matrix\n\n\n\nScenario\nChoice\nRationale\n\n\n\n\nEmail to stakeholders\nOJS\nSelf-contained HTML\n\n\nDashboard, &lt;10 users\nEither\nShiny may be overkill\n\n\n100+ concurrent users\nOJS\nNo server scaling\n\n\nDatabase queries\nShiny\nServer-side access\n\n\nRegulated (21 CFR 11)\nShiny\nAudit trail support\n\n\nConference presentation\nOJS\nNo network needed\n\n\nClient deliverable\nOJS\nNo infrastructure"
  },
  {
    "objectID": "posts/shinyvsobservable/analysis/paper/comparison_whitepaper.html#performance-characteristics",
    "href": "posts/shinyvsobservable/analysis/paper/comparison_whitepaper.html#performance-characteristics",
    "title": "Observable JS vs Shiny for Interactive Statistical Reports",
    "section": "6 Performance Characteristics",
    "text": "6 Performance Characteristics\n\n6.1 Data Size Considerations\n\n\n\nPerformance by Data Size\n\n\nData Size\nObservable JS\nShiny\n\n\n\n\n&lt; 10 MB\nExcellent\nExcellent\n\n\n10-100 MB\nGood\nExcellent\n\n\n100 MB-1 GB\nPoor\nGood\n\n\n&gt; 1 GB\nNot feasible\nFeasible\n\n\n\n\n\n\n\n6.2 Observable JS Performance Profile\nStrengths:\n\nInstant reactivity (no network round-trip)\nSmooth animations and transitions\nResponsive UI updates\nCDN caching for repeat visits\n\nLimitations:\n\nBrowser memory constraints (~1-4 GB practical limit)\nInitial load time proportional to data size\nNo background computation\nSingle-threaded (UI can freeze during computation)\n\n\n\n6.3 Shiny Performance Profile\nStrengths:\n\nHandle arbitrarily large datasets\nServer-side caching across users\nDatabase connections (query what you need)\nParallel computation via R packages\n\nLimitations:\n\nNetwork latency on each interaction\nServer resources scale with users\nCold start time for new sessions\nWebSocket connection required\n\n\n\n6.4 Optimization Strategies\n\nObservable JSShiny\n\n\n// Pre-aggregate data to reduce browser load\naggregated = data\n  .groupby(\"category\")\n  .rollup({\n    count: d =&gt; op.count(),\n    mean_value: d =&gt; op.mean(d.value)\n  })\n\n// Use efficient data formats\ndata = FileAttachment(\"data.parquet\").parquet()\n\n\n# Use server-side filtering before sending to UI\noutput$plot &lt;- renderPlot({\n  # Only process visible subset\n  data %&gt;%\n    filter(date &gt;= input$date_range[1],\n           date &lt;= input$date_range[2]) %&gt;%\n    ggplot(...)\n})\n\n# Cache expensive computations\nmodel_result &lt;- reactive({\n  expensive_model(data())\n}) %&gt;% bindCache(input$params)"
  },
  {
    "objectID": "posts/shinyvsobservable/analysis/paper/comparison_whitepaper.html#learning-curve-and-development-experience",
    "href": "posts/shinyvsobservable/analysis/paper/comparison_whitepaper.html#learning-curve-and-development-experience",
    "title": "Observable JS vs Shiny for Interactive Statistical Reports",
    "section": "7 Learning Curve and Development Experience",
    "text": "7 Learning Curve and Development Experience\n\n7.1 For R Statisticians\n\n\n\nAspect\nObservable JS\nShiny\n\n\n\n\nLanguage familiarity\nNew (JavaScript)\nFamiliar (R)\n\n\nData manipulation\nLearn Arquero\nUse dplyr\n\n\nPlotting\nLearn Plot/D3\nUse ggplot2\n\n\nReactivity model\nAutomatic\nExplicit\n\n\nDebugging\nBrowser DevTools\nRStudio\n\n\nTime to productivity\n2-4 weeks\n1-2 weeks\n\n\n\n\n\n7.2 Observable JS Learning Path\n\nWeek 1: JavaScript basics, Observable notebook syntax\nWeek 2: Observable Inputs and reactivity\nWeek 3: Observable Plot for visualization\nWeek 4: Arquero for data transformation, integration patterns\n\nResources:\n\nObservable tutorials\nQuarto OJS documentation\n\n\n\n7.3 Shiny Learning Path\n\nWeek 1: Shiny UI components and layout\nWeek 2: Reactive expressions and observers\n\nResources:\n\nMastering Shiny by Hadley Wickham\nQuarto Shiny documentation\n\n\n\n7.4 Code Comparison: Same Feature\n\nObservable JSShiny\n\n\n\n\nCode\nviewof species = Inputs.select(\n  [\"All\", \"Adelie\", \"Chinstrap\", \"Gentoo\"],\n  {label: \"Species\"}\n)\n\nfiltered = species === \"All\"\n  ? data\n  : data.filter(d =&gt; d.species === species)\n\nPlot.plot({\n  marks: [\n    Plot.dot(filtered, {x: \"bill_length\", y: \"bill_depth\", fill: \"species\"})\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# UI\nselectInput(\"species\", \"Species\",\n            choices = c(\"All\", \"Adelie\", \"Chinstrap\", \"Gentoo\"))\nplotOutput(\"scatter\")\n\n# Server\nfiltered &lt;- reactive({\n  if (input$species == \"All\") data\n  else data %&gt;% filter(species == input$species)\n})\n\noutput$scatter &lt;- renderPlot({\n  ggplot(filtered(), aes(bill_length, bill_depth, color = species)) +\n    geom_point()\n})"
  },
  {
    "objectID": "posts/shinyvsobservable/analysis/paper/comparison_whitepaper.html#security-and-compliance",
    "href": "posts/shinyvsobservable/analysis/paper/comparison_whitepaper.html#security-and-compliance",
    "title": "Observable JS vs Shiny for Interactive Statistical Reports",
    "section": "8 Security and Compliance",
    "text": "8 Security and Compliance\n\n8.1 Data Privacy Considerations\n\n\n\nConcern\nObservable JS\nShiny\n\n\n\n\nData exposure\nIn browser (DevTools)\nStays on server\n\n\nPHI/PII\nNeeds de-identification\nServer-side processing\n\n\nHIPAA\nChallenging\nFeasible\n\n\nAudit logging\nManual\nBuilt-in (Connect)\n\n\n\n\n\n8.2 Observable JS Security Model\nRisks:\n\nAll data sent to the browser is accessible to users\nCannot truly hide business logic\nAPI keys should never be embedded\n\nMitigations:\n\nPre-aggregate sensitive data\nUse server APIs for sensitive queries\nEmbed only de-identified data\n\n\n\n8.3 Shiny Security Model\nAdvantages:\n\nData never leaves the server\nCan integrate with enterprise authentication (LDAP, SAML)\nAudit trails available on Posit Connect\nRow-level security through user context\n\nRequirements:\n\nSecure server configuration\nHTTPS enforcement\nSession timeout policies\nRegular security updates\n\n\n\n8.4 Regulatory Compliance\nFor FDA-regulated environments (21 CFR Part 11) or similar:\n\n\n\nRequirement\nObservable JS\nShiny\n\n\n\n\nE-signatures\nN/A\nConnect supports\n\n\nAudit trail\nLimited\nFull support\n\n\nAccess controls\nBasic\nRole-based\n\n\nValidation\nChallenging\nEstablished\n\n\n\n\n\n\n\n\n\nWarningRegulatory Note\n\n\n\nFor GxP environments, Shiny with Posit Connect provides established validation pathways. Observable JS may require additional validation effort."
  },
  {
    "objectID": "posts/shinyvsobservable/analysis/paper/comparison_whitepaper.html#use-case-recommendations",
    "href": "posts/shinyvsobservable/analysis/paper/comparison_whitepaper.html#use-case-recommendations",
    "title": "Observable JS vs Shiny for Interactive Statistical Reports",
    "section": "9 Use Case Recommendations",
    "text": "9 Use Case Recommendations\n\n9.1 When to Choose Observable JS\n\n\n\n\n\n\nTipBest Fit Scenarios\n\n\n\n\nSelf-contained deliverables: Client reports, manuscripts, presentations\nWide distribution: Reports going to many recipients\nLimited IT support: No server infrastructure available\nExploratory visualization: Focus on data exploration over complex statistics\nOffline access needed: Field work, conferences, travel\nSimple interactivity: Filters, tooltips, basic calculations\n\n\n\n\n\n9.2 When to Choose Shiny\n\n\n\n\n\n\nTipBest Fit Scenarios\n\n\n\n\nComplex statistical methods: Mixed models, Bayesian analysis, survival curves\nLarge datasets: More than ~50 MB of data\nDatabase integration: Live queries, real-time data\nSensitive data: PHI, PII, proprietary information\nRegulated environments: Pharma, finance, healthcare\nEnterprise deployment: Integration with existing R infrastructure\n\n\n\n\n\n9.3 Hybrid Approaches\nFor some projects, combining both technologies may be optimal:\n\n\nCode\nflowchart LR\n    subgraph Backend\n        A[R] --&gt; B[JSON]\n    end\n    B --&gt; C\n    subgraph Report\n        C[OJS] --&gt; D[HTML]\n    end\n\n\n\n\n\nflowchart LR\n    subgraph Backend\n        A[R] --&gt; B[JSON]\n    end\n    B --&gt; C\n    subgraph Report\n        C[OJS] --&gt; D[HTML]\n    end\n\n\n Hybrid Architecture \n\n\n\nHybrid workflow:\n\nUse R for complex statistical analysis\nExport results to JSON or Parquet\nUse Observable JS for interactive visualization\nDistribute as static HTML"
  },
  {
    "objectID": "posts/shinyvsobservable/analysis/paper/comparison_whitepaper.html#conclusion",
    "href": "posts/shinyvsobservable/analysis/paper/comparison_whitepaper.html#conclusion",
    "title": "Observable JS vs Shiny for Interactive Statistical Reports",
    "section": "10 Conclusion",
    "text": "10 Conclusion\n\n10.1 Summary Comparison\n\n\n\nDimension\nObservable JS\nShiny\n\n\n\n\nBest for\nDistribution, visualization\nStatistics, security\n\n\nDeployment\nStatic hosting\nServer infrastructure\n\n\nData size limit\n~50 MB practical\nLimited by server\n\n\nStatistical depth\nBasic\nComprehensive\n\n\nLearning curve (R users)\nModerate\nLow\n\n\nOffline capability\nYes\nNo\n\n\nEnterprise features\nLimited\nExtensive\n\n\n\n\n\n10.2 Decision Framework\n\n\nCode\nflowchart LR\n    A[New Report] --&gt; B{Complex?}\n    B --&gt;|Yes| C[Shiny]\n    B --&gt;|No| D{Big data?}\n    D --&gt;|Yes| C\n    D --&gt;|No| E{Sensitive?}\n    E --&gt;|Yes| C\n    E --&gt;|No| F[Observable]\n\n\n\n\n\nflowchart LR\n    A[New Report] --&gt; B{Complex?}\n    B --&gt;|Yes| C[Shiny]\n    B --&gt;|No| D{Big data?}\n    D --&gt;|Yes| C\n    D --&gt;|No| E{Sensitive?}\n    E --&gt;|Yes| C\n    E --&gt;|No| F[Observable]\n\n\n Technology Selection Flowchart \n\n\n\n\n\n10.3 Final Recommendations\n\nStart with your constraints: Server availability, data sensitivity, and statistical requirements often dictate the choice.\nConsider your audience: Technical users may appreciate Shiny’s depth; broader audiences benefit from Observable’s portability.\nInvest in learning both: The skills complement each other, and hybrid approaches are increasingly common.\nPrototype quickly: Both technologies allow rapid prototyping within Quarto—try both before committing."
  },
  {
    "objectID": "posts/shinyvsobservable/analysis/paper/comparison_whitepaper.html#appendix-quick-reference",
    "href": "posts/shinyvsobservable/analysis/paper/comparison_whitepaper.html#appendix-quick-reference",
    "title": "Observable JS vs Shiny for Interactive Statistical Reports",
    "section": "11 Appendix: Quick Reference",
    "text": "11 Appendix: Quick Reference\n\n11.1 Observable JS Cheat Sheet\n// Inputs\nviewof x = Inputs.range([0, 100], {label: \"Value\"})\nviewof y = Inputs.select([\"A\", \"B\", \"C\"], {label: \"Category\"})\nviewof z = Inputs.checkbox([\"X\", \"Y\", \"Z\"], {label: \"Options\"})\n\n// Data\ndata = FileAttachment(\"file.csv\").csv({typed: true})\n\n// Filtering\nfiltered = data.filter(d =&gt; d.value &gt; x && y.includes(d.category))\n\n// Plotting\nPlot.plot({\n  marks: [\n    Plot.dot(filtered, {x: \"xvar\", y: \"yvar\", fill: \"group\"})\n  ]\n})\n\n\n11.2 Shiny Cheat Sheet\n# UI Inputs\nsliderInput(\"x\", \"Value\", 0, 100, 50)\nselectInput(\"y\", \"Category\", c(\"A\", \"B\", \"C\"))\ncheckboxGroupInput(\"z\", \"Options\", c(\"X\", \"Y\", \"Z\"))\n\n# Server\nfiltered &lt;- reactive({\n  data %&gt;% filter(value &gt; input$x, category %in% input$y)\n})\n\noutput$plot &lt;- renderPlot({\n  ggplot(filtered(), aes(xvar, yvar, color = group)) + geom_point()\n})"
  },
  {
    "objectID": "posts/shinyvsobservable/analysis/paper/comparison_whitepaper.html#references",
    "href": "posts/shinyvsobservable/analysis/paper/comparison_whitepaper.html#references",
    "title": "Observable JS vs Shiny for Interactive Statistical Reports",
    "section": "12 References",
    "text": "12 References\n\nWickham, H. (2021). Mastering Shiny. O’Reilly Media. https://mastering-shiny.org/\nQuarto Team. (2024). Quarto Documentation. Posit. https://quarto.org/\nObservable, Inc. (2024). Observable Plot Documentation. https://observablehq.com/plot/\nPosit. (2024). Shiny Documentation. https://shiny.posit.co/"
  },
  {
    "objectID": "posts/shareRcodeviadockerp25/analysis/docker_renv.html",
    "href": "posts/shareRcodeviadockerp25/analysis/docker_renv.html",
    "title": "Docker and renv Strategy for Reproducible R Development: A Practical Example",
    "section": "",
    "text": "Here’s an updated version of main1 that includes a listing of peng1.Rmd as initially written by Developer 1 and as later modified by Developer 2.\n⸻\nDocker and renv Strategy for Reproducible R Development: A Practical Example\n\nIntroduction\n\nEnsuring reproducibility in R Markdown workflows can be challenging due to package version mismatches and differences in computing environments. By combining renv (for package management) and Docker (for OS and system-level reproducibility), we can create fully reproducible workflows that work identically across different machines.\nIn this example, two developers collaborate on an R Markdown analysis using: • renv for package dependency management • Docker for containerizing the computing environment • GitHub for version control and collaboration • DockerHub for sharing a pre-configured R environment\n⸻\n\nStep-by-Step Example: Collaborative Reproducible Development with Docker and renv\n\n2.1 Developer 1: Setting Up the Project Locally\nStep 1: Create a GitHub Repository and Clone It Locally\nDeveloper 1 creates a new GitHub repository (penguins-analysis) and clones it:\n\ngit clone https://github.com/username/penguins-analysis.git\ncd penguins-analysis\nStep 2: Initialize renv for Dependency Management\nInside the project directory, Developer 1 initializes renv:\n\ninstall.packages(\"renv\")  # Install renv if not already installed\nrenv::init()              # Initialize renv for the project\nThis creates an renv.lock file to track exact package versions.\nStep 3: Install Required R Packages\nSince the analysis will use the palmerpenguins dataset, Developer 1 installs the required packages:\n\ninstall.packages(\"ggplot2\")\ninstall.packages(\"palmerpenguins\")\nrenv::snapshot()  # Save exact package versions to renv.lock\n⸻\n2.2 Developer 1: Writing the Initial R Markdown File (peng1.Rmd)\nListing: peng1.Rmd Created by Developer 1\nDeveloper 1 creates peng1.Rmd with an initial plot of flipper length vs. bill length:\n---\ntitle: \"Palmer Penguins Analysis\"\nauthor: \"Developer 1\"\ndate: \"2025-12-08\"\noutput: html_document\n---\n\n\n\n\n0.1 2.3 Developer 1: Creating a Docker Image\n\n0.1.1 Step 4: Write a Minimal Dockerfile (Without peng1.Rmd)\nDeveloper 1 creates a Dockerfile that does not include peng1.Rmd, ensuring that Developer 2’s local files are used when running the container.\n# Use R 4.1.0 as base image\nFROM rocker/r-ver:4.1.0\n\n# Set the working directory inside the container\nWORKDIR /workspace\n\n# Install renv and restore dependencies\nRUN R -e \"install.packages('renv', repos='https://cloud.r-project.org')\"\n\n# Copy only the renv.lock and renv infrastructure\nCOPY renv.lock renv/activate.R /workspace/\n\n# Restore the R package environment\nRUN R -e \"renv::restore()\"\n\nCMD [\"/bin/bash\"]\nStep 5: Build and Push the Docker Image\nDeveloper 1 builds the Docker image:\ndocker build -t username/penguins-analysis:v1 .\nTo push the image to DockerHub:\ndocker login docker push username/penguins-analysis:v1\n⸻\n2.4 Developer 1: Push to GitHub and Communicate to Developer 2\nStep 6: Commit and Push to GitHub\nDeveloper 1 commits the project without peng1.Rmd in the Docker image:\ngit add . git commit -m “Initial renv setup and Docker environment (without Rmd)” git push origin main\nDeveloper 1 then shares these instructions with Developer 2: 1. Clone the GitHub repository. 2. Pull the prebuilt Docker image from DockerHub. 3. Run the container interactively, mounting the local repository. 4. Write the peng1.Rmd file and generate the report. 5. Push changes back to GitHub.\n⸻\n2.5 Developer 2: Running the Analysis\nStep 7: Clone the Repository and Pull the Docker Image\nDeveloper 2 clones the repository:\ngit clone https://github.com/username/penguins-analysis.git cd penguins-analysis\nPull the Docker image:\ndocker pull username/penguins-analysis:v1\nStep 8: Run Docker Interactively with Local Repository\nSince peng1.Rmd is not included in the Docker image, Developer 2 mounts the local repo inside the container:\ndocker run –rm -it -v “$(pwd):/workspace” -w /workspace username/penguins-analysis:v1 /bin/bash\nThis allows Developer 2 to: • Use the renv-restored environment from the container. • Access and modify peng1.Rmd directly from their local machine.\n⸻\n2.6 Developer 2: Extending the Analysis\nStep 9: Modify peng1.Rmd to Include a Second Plot\nDeveloper 2 adds a new plot for body mass vs. bill length:\nListing: peng1.Rmd as Modified by Developer 2\n\n---\ntitle: \"Palmer Penguins Analysis\"\nauthor: \"Developer 2\"\ndate: \"2025-12-08\"\noutput: html_document\n---\n\n\n\n\n0.1.2 Step 10: Commit and Push Changes Back to GitHub\nDeveloper 2 commits and pushes the changes:\ngit add peng1.Rmd\ngit commit -m \"Added second plot: Body Mass vs. Bill Length\"\ngit push origin main\n⸻\n\nConclusion: Achieving Full Reproducibility\n\nBy following these steps: • renv ensures that package versions are identical across environments. • Docker guarantees that the R version and OS-level dependencies are consistent. • GitHub enables collaborative development. • DockerHub provides a shared, pre-configured execution environment.\nKey Updates in This Version\n✅ Includes a listing of peng1.Rmd as written by Developer 1. ✅ Includes a listing of peng1.Rmd as modified by Developer 2. ✅ Ensures peng1.Rmd is not inside the Docker image—instead, Developer 2 uses a bind mount.\nWould you like any further refinements or additions? 🚀\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{(ryy)_glenn_thomas2025,\n  author = {(Ryy) Glenn Thomas, Ronald},\n  title = {Docker and Renv {Strategy} for {Reproducible} {R}\n    {Development:} {A} {Practical} {Example}},\n  date = {2025-12-08},\n  url = {https://focusonr.org/posts/shareRcodeviadockerp25/analysis/docker_renv.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\n(Ryy) Glenn Thomas, Ronald. 2025. “Docker and Renv Strategy for\nReproducible R Development: A Practical Example.” December 8,\n2025. https://focusonr.org/posts/shareRcodeviadockerp25/analysis/docker_renv.html."
  },
  {
    "objectID": "posts/setupquartop01/archive/quarto-blog-template.html",
    "href": "posts/setupquartop01/archive/quarto-blog-template.html",
    "title": "Your Technical Blog Post Title",
    "section": "",
    "text": "Brief introduction that:\n\nHooks the reader with an interesting problem or observation\nStates the purpose of your analysis/tutorial\nOutlines what readers will learn or gain"
  },
  {
    "objectID": "posts/setupquartop01/archive/quarto-blog-template.html#introduction",
    "href": "posts/setupquartop01/archive/quarto-blog-template.html#introduction",
    "title": "Your Technical Blog Post Title",
    "section": "",
    "text": "Brief introduction that:\n\nHooks the reader with an interesting problem or observation\nStates the purpose of your analysis/tutorial\nOutlines what readers will learn or gain"
  },
  {
    "objectID": "posts/setupquartop01/archive/quarto-blog-template.html#required-packages-and-setup",
    "href": "posts/setupquartop01/archive/quarto-blog-template.html#required-packages-and-setup",
    "title": "Your Technical Blog Post Title",
    "section": "2 Required Packages and Setup",
    "text": "2 Required Packages and Setup\n\n# List the packages readers will need\nlibrary(tidyverse)\n# Add other packages\n\nBrief explanation of why these packages were chosen and any setup requirements."
  },
  {
    "objectID": "posts/setupquartop01/archive/quarto-blog-template.html#the-problemdata",
    "href": "posts/setupquartop01/archive/quarto-blog-template.html#the-problemdata",
    "title": "Your Technical Blog Post Title",
    "section": "3 The Problem/Data",
    "text": "3 The Problem/Data\n\n# Data loading and initial preparation\n# Load sample dataset\ndata &lt;- mtcars\nglimpse(data)\n\nRows: 32\nColumns: 11\n$ mpg  &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,…\n$ cyl  &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,…\n$ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16…\n$ hp   &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180…\n$ drat &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,…\n$ wt   &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.…\n$ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18…\n$ vs   &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,…\n$ am   &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,…\n$ gear &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3,…\n$ carb &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2,…\n\n\n\nDescribe your data source\nExplain the problem you’re addressing\nShare any initial data preparation steps"
  },
  {
    "objectID": "posts/setupquartop01/archive/quarto-blog-template.html#analysistutorial-steps",
    "href": "posts/setupquartop01/archive/quarto-blog-template.html#analysistutorial-steps",
    "title": "Your Technical Blog Post Title",
    "section": "4 Analysis/Tutorial Steps",
    "text": "4 Analysis/Tutorial Steps\n\n4.1 Step 1: Initial Data Exploration\n\n# Your analysis code here\nglimpse(mtcars)\n\nRows: 32\nColumns: 11\n$ mpg  &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,…\n$ cyl  &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,…\n$ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16…\n$ hp   &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180…\n$ drat &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,…\n$ wt   &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.…\n$ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18…\n$ vs   &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,…\n$ am   &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,…\n$ gear &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3,…\n$ carb &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2,…\n\nggplot(mtcars, aes(x=cyl, y=mpg)) +\n  geom_point()\n\n\n\n\nDescription of your visualization\n\n\n\n  # Your visualization\n\nExplain what you found and why it’s interesting.\n\n\n4.2 Step 2: Main Analysis\n\n# Core analysis code\n\nWalk through your analysis, explaining: - Why you chose this approach - What the code does - What the results mean\n\n\n4.3 Step 3: Results and Visualization\n\n# Create compelling visualizations\n\nInterpret your results and explain their significance."
  },
  {
    "objectID": "posts/setupquartop01/archive/quarto-blog-template.html#key-takeaways",
    "href": "posts/setupquartop01/archive/quarto-blog-template.html#key-takeaways",
    "title": "Your Technical Blog Post Title",
    "section": "5 Key Takeaways",
    "text": "5 Key Takeaways\n\nBullet point summary of main findings\nPractical applications\nImportant insights"
  },
  {
    "objectID": "posts/setupquartop01/archive/quarto-blog-template.html#reproducibility",
    "href": "posts/setupquartop01/archive/quarto-blog-template.html#reproducibility",
    "title": "Your Technical Blog Post Title",
    "section": "6 Reproducibility",
    "text": "6 Reproducibility\n\n# Print session info for reproducibility\nsessionInfo()\n\nR version 4.5.2 (2025-10-31)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sequoia 15.6.1\n\nMatrix products: default\nBLAS:   /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/Los_Angeles\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] here_1.0.2      lubridate_1.9.4 forcats_1.0.0   stringr_1.6.0  \n [5] dplyr_1.1.4     purrr_1.2.0     readr_2.1.5     tidyr_1.3.1    \n [9] tibble_3.3.0    ggplot2_4.0.1   tidyverse_2.0.0\n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.6       jsonlite_2.0.0     compiler_4.5.2     tidyselect_1.2.1  \n [5] parallel_4.5.2     scales_1.4.0       yaml_2.3.11        fastmap_1.2.0     \n [9] R6_2.6.1           labeling_0.4.3     generics_0.1.4     knitr_1.50        \n[13] htmlwidgets_1.6.4  rprojroot_2.1.1    pillar_1.11.1      RColorBrewer_1.1-3\n[17] tzdb_0.5.0         rlang_1.1.6        stringi_1.8.7      xfun_0.54         \n[21] S7_0.2.1           timechange_0.3.0   cli_3.6.5          withr_3.0.2       \n[25] magrittr_2.0.4     digest_0.6.39      grid_4.5.2         hms_1.1.3         \n[29] lifecycle_1.0.4    vctrs_0.6.5        evaluate_1.0.5     glue_1.8.0        \n[33] farver_2.1.2       rmarkdown_2.30     tools_4.5.2        pkgconfig_2.0.3   \n[37] htmltools_0.5.9"
  },
  {
    "objectID": "posts/setupquartop01/archive/quarto-blog-template.html#next-steps",
    "href": "posts/setupquartop01/archive/quarto-blog-template.html#next-steps",
    "title": "Your Technical Blog Post Title",
    "section": "7 Next Steps",
    "text": "7 Next Steps\n\nSuggest areas for further exploration\nMention potential improvements\nInvite reader engagement"
  },
  {
    "objectID": "posts/setupquartop01/archive/quarto-blog-template.html#references",
    "href": "posts/setupquartop01/archive/quarto-blog-template.html#references",
    "title": "Your Technical Blog Post Title",
    "section": "8 References",
    "text": "8 References\n\nCite your sources\nLink to relevant documentation\nCredit other contributors"
  },
  {
    "objectID": "posts/penguins1zzcollab/index.html",
    "href": "posts/penguins1zzcollab/index.html",
    "title": "Palmer Penguins Data Analysis Series (Part 1): Exploratory Data Analysis and Simple Regression",
    "section": "",
    "text": "Curious Adelie penguins beginning their data science journey - because every great analysis starts with getting to know your data!\nPhoto: African penguins at Boulders Beach, South Africa. Licensed under CC BY 2.0 via Wikimedia Commons"
  },
  {
    "objectID": "posts/penguins1zzcollab/index.html#species-and-morphometric-overview",
    "href": "posts/penguins1zzcollab/index.html#species-and-morphometric-overview",
    "title": "Palmer Penguins Data Analysis Series (Part 1): Exploratory Data Analysis and Simple Regression",
    "section": "4.1 Species and Morphometric Overview",
    "text": "4.1 Species and Morphometric Overview\nLet’s understand our penguin community composition and key measurements:\n\n# Species summary with key statistics\nspecies_summary &lt;- penguins_clean %&gt;%\n  group_by(species) %&gt;%\n  summarise(\n    n = n(),\n    body_mass_mean = round(mean(body_mass_g), 0),\n    flipper_length_mean = round(mean(flipper_length_mm), 1),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(percentage = round(n / sum(n) * 100, 1))\n\nkable(species_summary, \n      caption = \"Species Distribution and Key Morphometrics\",\n      col.names = c(\"Species\", \"N\", \"Body Mass (g)\", \"Flipper Length (mm)\", \"% of Dataset\"))\n\n\nSpecies Distribution and Key Morphometrics\n\n\nSpecies\nN\nBody Mass (g)\nFlipper Length (mm)\n% of Dataset\n\n\n\n\nAdelie\n146\n3706\n190.1\n43.8\n\n\nChinstrap\n68\n3733\n195.8\n20.4\n\n\nGentoo\n119\n5092\n217.2\n35.7\n\n\n\n\n# Combined visualization: species distribution and key relationships\np_species &lt;- ggplot(species_summary, aes(x = species, y = n, fill = species)) +\n  geom_col(alpha = 0.8) +\n  geom_text(aes(label = paste0(n, \"\\n(\", percentage, \"%)\")), \n            vjust = -0.5, size = 3.5) +\n  scale_fill_manual(values = penguin_colors) +\n  labs(title = \"Species Distribution\", x = \"Species\", y = \"Count\") +\n  theme_minimal() + theme(legend.position = \"none\")\n\n# Flipper length vs body mass by species\np_relationship &lt;- ggplot(penguins_clean, aes(x = flipper_length_mm, y = body_mass_g, color = species)) +\n  geom_point(alpha = 0.7, size = 1.5) +\n  geom_smooth(method = \"lm\", se = FALSE, size = 0.8) +\n  scale_color_manual(values = penguin_colors) +\n  labs(title = \"Flipper Length vs Body Mass\", \n       x = \"Flipper Length (mm)\", y = \"Body Mass (g)\", color = \"Species\") +\n  theme_minimal()\n\n# Combine plots\neda_overview &lt;- p_species + p_relationship\nprint(eda_overview)\n\n\n\n\n\n\n\n# Save the plot\nggsave(\"eda-overview.png\", plot = eda_overview, width = 10, height = 5, dpi = 300)\n\n\n\n\nSpecies distribution and morphometric relationship overview showing sample sizes and the key flipper-body mass relationship across species"
  },
  {
    "objectID": "posts/penguins1zzcollab/index.html#building-and-interpreting-the-model",
    "href": "posts/penguins1zzcollab/index.html#building-and-interpreting-the-model",
    "title": "Palmer Penguins Data Analysis Series (Part 1): Exploratory Data Analysis and Simple Regression",
    "section": "7.1 Building and Interpreting the Model",
    "text": "7.1 Building and Interpreting the Model\n\n# Fit simple linear regression model\nsimple_model &lt;- lm(body_mass_g ~ flipper_length_mm, data = penguins_clean)\n\n# Extract coefficients with confidence intervals\nmodel_coefficients &lt;- tidy(simple_model, conf.int = TRUE)\nmodel_metrics &lt;- glance(simple_model)\n\n# Display key results\ncat(\"📊 Simple Linear Model Results:\\n\")\n\n📊 Simple Linear Model Results:\n\ncat(\"===============================\\n\")\n\n===============================\n\ncat(sprintf(\"R-squared: %.3f (%.1f%% of variance explained)\\n\", \n            model_metrics$r.squared, model_metrics$r.squared * 100))\n\nR-squared: 0.762 (76.2% of variance explained)\n\ncat(sprintf(\"RMSE: %.1f grams\\n\", sigma(simple_model)))\n\nRMSE: 393.3 grams\n\ncat(sprintf(\"F-statistic: %.1f (p &lt; 0.001)\\n\", model_metrics$statistic))\n\nF-statistic: 1060.3 (p &lt; 0.001)\n\n# Model equation with confidence intervals\nintercept &lt;- model_coefficients$estimate[1]\nslope &lt;- model_coefficients$estimate[2]\nslope_ci_lower &lt;- model_coefficients$conf.low[2]\nslope_ci_upper &lt;- model_coefficients$conf.high[2]\n\ncat(\"\\n🧮 Model Equation:\\n\")\n\n\n🧮 Model Equation:\n\ncat(sprintf(\"Body Mass = %.1f + %.1f × Flipper Length\\n\", intercept, slope))\n\nBody Mass = -5872.1 + 50.2 × Flipper Length\n\ncat(sprintf(\"Slope 95%% CI: [%.1f, %.1f] grams/mm\\n\", slope_ci_lower, slope_ci_upper))\n\nSlope 95% CI: [47.1, 53.2] grams/mm\n\n# Generate predictions with confidence intervals\nnew_data &lt;- tibble(flipper_length_mm = c(180, 200, 220))\npredictions &lt;- predict(simple_model, newdata = new_data, interval = \"confidence\")\n\ncat(\"\\n📝 Example Predictions (95% CI):\\n\")\n\n\n📝 Example Predictions (95% CI):\n\nfor(i in 1:nrow(new_data)) {\n  cat(sprintf(\"• %dmm flippers: %.0f g [%.0f, %.0f]\\n\", \n              new_data$flipper_length_mm[i], \n              predictions[i, \"fit\"], \n              predictions[i, \"lwr\"], \n              predictions[i, \"upr\"]))\n}\n\n• 180mm flippers: 3155 g [3079, 3232]\n• 200mm flippers: 4159 g [4116, 4201]\n• 220mm flippers: 5162 g [5090, 5233]\n\n# Visualize model with confidence bands\nmodel_plot &lt;- ggplot(penguins_clean, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(aes(color = species), alpha = 0.6) +\n  geom_smooth(method = \"lm\", color = \"black\", fill = \"gray80\") +\n  scale_color_manual(values = penguin_colors) +\n  labs(title = \"Simple Linear Regression: Body Mass ~ Flipper Length\",\n       subtitle = \"Gray band shows 95% confidence interval\",\n       x = \"Flipper Length (mm)\", y = \"Body Mass (g)\", color = \"Species\") +\n  theme_minimal()\n\nprint(model_plot)\n\n\n\n\n\n\n\nggsave(\"simple-regression-model.png\", plot = model_plot, width = 8, height = 5, dpi = 300)\n\n\n\n\nSimple linear regression model showing body mass predicted by flipper length with 95% confidence interval"
  },
  {
    "objectID": "posts/penguins1zzcollab/index.html#statistical-limitations",
    "href": "posts/penguins1zzcollab/index.html#statistical-limitations",
    "title": "Palmer Penguins Data Analysis Series (Part 1): Exploratory Data Analysis and Simple Regression",
    "section": "8.1 Statistical Limitations",
    "text": "8.1 Statistical Limitations\n\n# Model diagnostic checks\npenguins_with_predictions &lt;- penguins_clean %&gt;%\n  mutate(\n    predicted = predict(simple_model),\n    residuals = residuals(simple_model),\n    standardized_residuals = rstandard(simple_model)\n  )\n\n# Check for outliers and influential points\noutliers &lt;- which(abs(penguins_with_predictions$standardized_residuals) &gt; 2.5)\ncat(\"⚠️  Model Assumption Checks:\\n\")\n\n⚠️  Model Assumption Checks:\n\ncat(sprintf(\"• Potential outliers: %d observations (&gt;2.5 SD from mean)\\n\", length(outliers)))\n\n• Potential outliers: 5 observations (&gt;2.5 SD from mean)\n\ncat(sprintf(\"• Residual standard error: %.1f grams\\n\", sigma(simple_model)))\n\n• Residual standard error: 393.3 grams\n\n# Residuals diagnostic plot\ndiagnostic_plot &lt;- ggplot(penguins_with_predictions, aes(x = predicted, y = standardized_residuals)) +\n  geom_point(aes(color = species), alpha = 0.6) +\n  geom_hline(yintercept = c(-2, 0, 2), linetype = c(\"dashed\", \"solid\", \"dashed\"), \n             color = c(\"red\", \"black\", \"red\")) +\n  scale_color_manual(values = penguin_colors) +\n  labs(title = \"Model Residuals Diagnostic\",\n       subtitle = \"Species clustering suggests missing predictors\",\n       x = \"Predicted Body Mass (g)\", y = \"Standardized Residuals\", color = \"Species\") +\n  theme_minimal()\n\nprint(diagnostic_plot)\n\n\n\n\n\n\n\nggsave(\"model-diagnostics.png\", plot = diagnostic_plot, width = 8, height = 5, dpi = 300)\n\n\n\n\nModel diagnostic plot showing residuals clustered by species, indicating model limitations"
  },
  {
    "objectID": "posts/penguins1zzcollab/index.html#key-limitations",
    "href": "posts/penguins1zzcollab/index.html#key-limitations",
    "title": "Palmer Penguins Data Analysis Series (Part 1): Exploratory Data Analysis and Simple Regression",
    "section": "8.2 Key Limitations",
    "text": "8.2 Key Limitations\n\nSimpson’s Paradox Risk: The model ignores species differences, potentially masking important biological relationships\nModel Assumptions:\n\nLinear relationship assumption appears reasonable\nResidual clustering by species indicates missing predictors\nHomoscedasticity assumption may be violated across species\n\nTemporal Generalizability: Data spans 2007-2009; climate change may affect current relationships\nGeographic Scope: Limited to Palmer Station region; may not generalize to other penguin populations\nMeasurement Precision: Morphometric measurements have inherent measurement error not captured in model\nBiological Constraints: Model predictions outside observed flipper length range (172-231mm) should be interpreted cautiously"
  },
  {
    "objectID": "posts/penguins1zzcollab/index.html#real-world-applications",
    "href": "posts/penguins1zzcollab/index.html#real-world-applications",
    "title": "Palmer Penguins Data Analysis Series (Part 1): Exploratory Data Analysis and Simple Regression",
    "section": "9.1 Real-World Applications",
    "text": "9.1 Real-World Applications\nOur simple regression model has several practical applications in Antarctic research:\n\n# Calculate effect sizes and practical significance\neffect_size &lt;- slope / sd(penguins_clean$body_mass_g)\ncat(\"🌍 Practical Applications:\\n\")\n\n🌍 Practical Applications:\n\ncat(sprintf(\"• Field Assessment: Flipper measurements can estimate body condition (effect size: %.2f)\\n\", effect_size))\n\n• Field Assessment: Flipper measurements can estimate body condition (effect size: 0.06)\n\ncat(sprintf(\"• Population Monitoring: Track penguin health trends using flipper-mass relationships\\n\"))\n\n• Population Monitoring: Track penguin health trends using flipper-mass relationships\n\ncat(sprintf(\"• Climate Research: Changes in morphometric relationships may indicate environmental stress\\n\"))\n\n• Climate Research: Changes in morphometric relationships may indicate environmental stress\n\ncat(sprintf(\"• Conservation Planning: Identify underweight individuals for targeted intervention\\n\"))\n\n• Conservation Planning: Identify underweight individuals for targeted intervention\n\n# Practical thresholds based on model\nlow_threshold &lt;- quantile(penguins_clean$body_mass_g, 0.25)\nhigh_threshold &lt;- quantile(penguins_clean$body_mass_g, 0.75)\n\ncat(\"\\n📊 Clinical Thresholds:\\n\")\n\n\n📊 Clinical Thresholds:\n\ncat(sprintf(\"• Low body condition: &lt;%.0f g (based on 25th percentile)\\n\", low_threshold))\n\n• Low body condition: &lt;3550 g (based on 25th percentile)\n\ncat(sprintf(\"• Normal range: %.0f-%.0f g\\n\", low_threshold, high_threshold))\n\n• Normal range: 3550-4775 g\n\ncat(sprintf(\"• High body condition: &gt;%.0f g (based on 75th percentile)\\n\", high_threshold))\n\n• High body condition: &gt;4775 g (based on 75th percentile)"
  },
  {
    "objectID": "posts/penguins1zzcollab/index.html#what-weve-learned-in-part-1",
    "href": "posts/penguins1zzcollab/index.html#what-weve-learned-in-part-1",
    "title": "Palmer Penguins Data Analysis Series (Part 1): Exploratory Data Analysis and Simple Regression",
    "section": "10.1 What We’ve Learned in Part 1",
    "text": "10.1 What We’ve Learned in Part 1\n\nStrong Predictive Relationship: Flipper length explains 76% of body mass variance (R² = 0.762), providing a reliable field assessment tool\nSpecies-Specific Patterns: Residual clustering by species suggests important biological differences not captured by flipper length alone\nModel Performance: RMSE of 393g indicates reasonable prediction accuracy for most applications\nResearch Implications: Simple morphometric relationships can support field research and conservation efforts"
  },
  {
    "objectID": "posts/penguins1zzcollab/index.html#looking-ahead-to-part-2",
    "href": "posts/penguins1zzcollab/index.html#looking-ahead-to-part-2",
    "title": "Palmer Penguins Data Analysis Series (Part 1): Exploratory Data Analysis and Simple Regression",
    "section": "10.2 Looking Ahead to Part 2",
    "text": "10.2 Looking Ahead to Part 2\nOur residual analysis reveals clear opportunities for improvement through:\n\nSpecies Integration: Accounting for biological differences between penguin species\nMultiple Predictors: Incorporating bill measurements for enhanced accuracy\n\nInteraction Effects: Exploring how predictors work together\nModel Validation: Comparing simple vs. complex model performance\n\n\n\n\n\n\n\nTip🎯 Preview: Dramatic Model Improvement\n\n\n\nIn Part 2, adding species information will improve our model’s R² from 0.762 to over 0.860 - demonstrating why biological context matters in ecological modeling!"
  },
  {
    "objectID": "posts/palmerpenguinspart4/index.html",
    "href": "posts/palmerpenguinspart4/index.html",
    "title": "Palmer Penguins Data Analysis Series (Part 4): Model Diagnostics and Interpretation",
    "section": "",
    "text": "A penguin scientist with a magnifying glass, carefully examining model diagnostics and residual plots!\nPhoto: African penguins at Boulders Beach, South Africa. Licensed under CC BY 2.0 via Wikimedia Commons"
  },
  {
    "objectID": "posts/palmerpenguinspart4/index.html#linearity-assessment",
    "href": "posts/palmerpenguinspart4/index.html#linearity-assessment",
    "title": "Palmer Penguins Data Analysis Series (Part 4): Model Diagnostics and Interpretation",
    "section": "3.1 1. Linearity Assessment",
    "text": "3.1 1. Linearity Assessment\n\n# Simplified linearity check using residuals vs predictors\nlinearity_data &lt;- penguins_diagnostics %&gt;%\n  select(residuals, bill_length_mm, bill_depth_mm, flipper_length_mm, species) %&gt;%\n  pivot_longer(cols = c(bill_length_mm, bill_depth_mm, flipper_length_mm), \n               names_to = \"predictor\", values_to = \"value\")\n\nggplot(linearity_data, aes(x = value, y = residuals)) +\n  geom_point(aes(color = species), alpha = 0.6) +\n  geom_smooth(method = \"loess\", se = TRUE, color = \"red\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  facet_wrap(~predictor, scales = \"free_x\") +\n  scale_color_manual(values = penguin_colors) +\n  labs(title = \"Linearity Check: Residuals vs Predictors\", x = \"Predictor Value\", y = \"Residuals (g)\") +\n  theme_minimal()\n\n\n\n\n\n\n\ncat(\"Linearity Assessment: Smooth curves should be approximately horizontal around zero.\\n\")\n\nLinearity Assessment: Smooth curves should be approximately horizontal around zero."
  },
  {
    "objectID": "posts/palmerpenguinspart4/index.html#independence-check",
    "href": "posts/palmerpenguinspart4/index.html#independence-check",
    "title": "Palmer Penguins Data Analysis Series (Part 4): Model Diagnostics and Interpretation",
    "section": "3.2 2. Independence Check",
    "text": "3.2 2. Independence Check\n\n# Quick independence assessment via Durbin-Watson test\nif (exists(\"durbinWatsonTest\")) {\n  dw_test &lt;- durbinWatsonTest(best_model)\n  cat(sprintf(\"Durbin-Watson Test: DW = %.3f, p = %.3f\\n\", dw_test$dw, dw_test$p))\n  cat(\"Independence assumption:\", ifelse(dw_test$p &gt; 0.05, \"✓ Satisfied\", \"⚠ Questionable\"), \"\\n\")\n} else {\n  cat(\"Independence assumption: ✓ Likely satisfied (cross-sectional biological data)\\n\")\n}\n\nDurbin-Watson Test: DW = 2.248, p = 0.028\nIndependence assumption: ⚠ Questionable"
  },
  {
    "objectID": "posts/palmerpenguinspart4/index.html#homoscedasticity-constant-variance",
    "href": "posts/palmerpenguinspart4/index.html#homoscedasticity-constant-variance",
    "title": "Palmer Penguins Data Analysis Series (Part 4): Model Diagnostics and Interpretation",
    "section": "3.3 3. Homoscedasticity (Constant Variance)",
    "text": "3.3 3. Homoscedasticity (Constant Variance)\n\n# Essential diagnostic: residuals vs fitted values\nggplot(penguins_diagnostics, aes(x = fitted_values, y = residuals)) +\n  geom_point(aes(color = species), alpha = 0.7) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_smooth(method = \"loess\", se = TRUE) +\n  scale_color_manual(values = penguin_colors) +\n  labs(title = \"Residuals vs Fitted Values\", x = \"Fitted Values (g)\", y = \"Residuals (g)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Statistical test for homoscedasticity\nif (exists(\"bptest\")) {\n  bp_test &lt;- bptest(best_model)\n  cat(sprintf(\"Breusch-Pagan Test: p = %.3f\\n\", bp_test$p.value))\n  cat(\"Homoscedasticity:\", ifelse(bp_test$p.value &gt; 0.05, \"✓ Satisfied\", \"⚠ Questionable\"), \"\\n\")\n} else {\n  cat(\"Homoscedasticity: Visual inspection of residuals vs fitted values plot\\n\")\n}\n\nBreusch-Pagan Test: p = 0.764\nHomoscedasticity: ✓ Satisfied"
  },
  {
    "objectID": "posts/palmerpenguinspart4/index.html#normality-of-residuals",
    "href": "posts/palmerpenguinspart4/index.html#normality-of-residuals",
    "title": "Palmer Penguins Data Analysis Series (Part 4): Model Diagnostics and Interpretation",
    "section": "3.4 4. Normality of Residuals",
    "text": "3.4 4. Normality of Residuals\n\n# Q-Q plot for normality assessment\nggplot(penguins_diagnostics, aes(sample = residuals)) +\n  stat_qq(aes(color = species), alpha = 0.7) +\n  stat_qq_line() +\n  scale_color_manual(values = penguin_colors) +\n  labs(title = \"Q-Q Plot of Residuals\", x = \"Theoretical Quantiles\", y = \"Sample Quantiles\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Normality test\nshapiro_test &lt;- shapiro.test(residuals(best_model))\ncat(sprintf(\"Shapiro-Wilk Test: p = %.4f\\n\", shapiro_test$p.value))\n\nShapiro-Wilk Test: p = 0.0746\n\ncat(\"Normality:\", ifelse(shapiro_test$p.value &gt; 0.05, \"✓ Satisfied\", \"⚠ Questionable\"), \"\\n\")\n\nNormality: ✓ Satisfied \n\ncat(\"Note: Large samples may show significant p-values despite practical normality.\\n\")\n\nNote: Large samples may show significant p-values despite practical normality.\n\n\n “Some observations seem different from the rest - let’s investigate!”"
  },
  {
    "objectID": "posts/palmerpenguinspart4/index.html#normality-of-residuals-1",
    "href": "posts/palmerpenguinspart4/index.html#normality-of-residuals-1",
    "title": "Palmer Penguins Data Analysis Series (Part 4): Model Diagnostics and Interpretation",
    "section": "4.1 4. Normality of Residuals",
    "text": "4.1 4. Normality of Residuals\nResiduals should follow a normal distribution:\n\n# Q-Q plot for normality\np4 &lt;- ggplot(penguins_diagnostics, aes(sample = standardized_residuals)) +\n  stat_qq(aes(color = species), alpha = 0.7) +\n  stat_qq_line(color = \"red\", linetype = \"dashed\") +\n  scale_color_manual(values = penguin_colors) +\n  labs(title = \"Q-Q Plot of Standardized Residuals\",\n       subtitle = \"Checking normality assumption\",\n       x = \"Theoretical Quantiles\", y = \"Sample Quantiles\",\n       color = \"Species\") +\n  theme_minimal()\n\n# Histogram of residuals\np5 &lt;- ggplot(penguins_diagnostics, aes(x = residuals)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 30, \n                 fill = \"lightblue\", alpha = 0.7, color = \"white\") +\n  geom_density(color = \"blue\", size = 1) +\n  stat_function(fun = dnorm, \n                args = list(mean = mean(penguins_diagnostics$residuals), \n                           sd = sd(penguins_diagnostics$residuals)),\n                color = \"red\", linetype = \"dashed\", size = 1) +\n  labs(title = \"Distribution of Residuals\",\n       subtitle = \"Blue = actual density, Red = normal distribution\",\n       x = \"Residuals (g)\", y = \"Density\") +\n  theme_minimal()\n\nprint(p5)\n\n\n\n\n\n\n\n\n “Our diagnostics confirm the model is statistically sound!”"
  },
  {
    "objectID": "posts/palmerpenguinspart2/index.html",
    "href": "posts/palmerpenguinspart2/index.html",
    "title": "Palmer Penguins Data Analysis Series (Part 2): Multiple Regression and Species Effects",
    "section": "",
    "text": "Two penguins collaborating on their regression analysis - because multiple predictors work better together!\nPhoto: African penguins at Boulders Beach, South Africa. Licensed under CC BY 2.0 via Wikimedia Commons"
  },
  {
    "objectID": "posts/palmerpenguinspart2/index.html#building-multiple-predictor-models",
    "href": "posts/palmerpenguinspart2/index.html#building-multiple-predictor-models",
    "title": "Palmer Penguins Data Analysis Series (Part 2): Multiple Regression and Species Effects",
    "section": "3.1 Building Multiple Predictor Models",
    "text": "3.1 Building Multiple Predictor Models\n\n# Build multiple regression with all morphometric variables\nmultiple_model &lt;- lm(body_mass_g ~ bill_length_mm + bill_depth_mm + flipper_length_mm, \n                     data = penguins_clean)\n\n# Extract key metrics with confidence intervals\nmultiple_metrics &lt;- glance(multiple_model)\nmultiple_coef &lt;- tidy(multiple_model, conf.int = TRUE)\n\ncat(\"📊 Multiple Regression Results:\\n\")\n\n📊 Multiple Regression Results:\n\ncat(\"R² improvement:\", round(multiple_metrics$r.squared, 3), \"vs\", round(glance(simple_model)$r.squared, 3), \"(simple)\\n\")\n\nR² improvement: 0.764 vs 0.762 (simple)\n\ncat(\"RMSE:\", round(sigma(multiple_model), 1), \"grams\\n\")\n\nRMSE: 393 grams\n\n# Check multicollinearity\nvif_values &lt;- vif(multiple_model)\ncat(\"\\n🔍 Multicollinearity Check (VIF):\\n\")\n\n\n🔍 Multicollinearity Check (VIF):\n\nfor(i in 1:length(vif_values)) {\n  cat(sprintf(\"%s: %.1f %s\\n\", names(vif_values)[i], vif_values[i],\n              ifelse(vif_values[i] &lt; 5, \"✅\", \"⚠️\")))\n}\n\nbill_length_mm: 1.9 ✅\nbill_depth_mm: 1.6 ✅\nflipper_length_mm: 2.6 ✅\n\n# Key coefficients with confidence intervals\ncat(\"\\n📝 Key Effects (95% CI):\\n\")\n\n\n📝 Key Effects (95% CI):\n\nfor(i in 2:nrow(multiple_coef)) {\n  term &lt;- multiple_coef$term[i]\n  est &lt;- multiple_coef$estimate[i]\n  ci_low &lt;- multiple_coef$conf.low[i]\n  ci_high &lt;- multiple_coef$conf.high[i]\n  cat(sprintf(\"%s: %.1f [%.1f, %.1f] g/mm\\n\", term, est, ci_low, ci_high))\n}\n\nbill_length_mm: 3.3 [-7.3, 13.8] g/mm\nbill_depth_mm: 17.8 [-9.4, 45.0] g/mm\nflipper_length_mm: 50.8 [45.8, 55.7] g/mm"
  },
  {
    "objectID": "posts/palmerpenguinspart2/index.html#adding-species-information",
    "href": "posts/palmerpenguinspart2/index.html#adding-species-information",
    "title": "Palmer Penguins Data Analysis Series (Part 2): Multiple Regression and Species Effects",
    "section": "4.1 Adding Species Information",
    "text": "4.1 Adding Species Information\n\n# Model with species as predictor\nspecies_model &lt;- lm(body_mass_g ~ bill_length_mm + bill_depth_mm + \n                    flipper_length_mm + species, data = penguins_clean)\n\nspecies_metrics &lt;- glance(species_model)\nspecies_coef &lt;- tidy(species_model, conf.int = TRUE)\n\ncat(\"🚀 Species Model - Dramatic Improvement:\\n\")\n\n🚀 Species Model - Dramatic Improvement:\n\ncat(\"R² jump:\", round(multiple_metrics$r.squared, 3), \"→\", round(species_metrics$r.squared, 3))\n\nR² jump: 0.764 → 0.849\n\ncat(\" (+\", round((species_metrics$r.squared - multiple_metrics$r.squared) * 100, 1), \"%)\\n\")\n\n (+ 8.6 %)\n\ncat(\"RMSE reduction:\", round(sigma(multiple_model), 1), \"→\", round(sigma(species_model), 1), \"grams\\n\")\n\nRMSE reduction: 393 → 314.8 grams\n\n# Species effects with confidence intervals\nspecies_effects &lt;- species_coef[grepl(\"species\", species_coef$term), ]\ncat(\"\\n📊 Species Effects (vs Adelie baseline):\\n\")\n\n\n📊 Species Effects (vs Adelie baseline):\n\nfor(i in 1:nrow(species_effects)) {\n  term &lt;- gsub(\"species\", \"\", species_effects$term[i])\n  est &lt;- species_effects$estimate[i]\n  ci_low &lt;- species_effects$conf.low[i]\n  ci_high &lt;- species_effects$conf.high[i]\n  cat(sprintf(\"%s: %+.0f [%+.0f, %+.0f] grams\\n\", term, est, ci_low, ci_high))\n}\n\nChinstrap: -497 [-659, -335] grams\nGentoo: +965 [+686, +1244] grams\n\nknit_exit()"
  },
  {
    "objectID": "posts/markdowntoblog/analysis/paper/index.html",
    "href": "posts/markdowntoblog/analysis/paper/index.html",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "",
    "text": "Converting your documentation into a professional blog post\nA structured workflow for publishing technical documentation with reproducibility built-in"
  },
  {
    "objectID": "posts/markdowntoblog/analysis/paper/index.html#what-youll-learn",
    "href": "posts/markdowntoblog/analysis/paper/index.html#what-youll-learn",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "1.1 What You’ll Learn",
    "text": "1.1 What You’ll Learn\n\nThe complete workflow: Step-by-step process for converting markdown to blog post\nKey decisions: How to structure content for blog format\nTechnical integration: Symlinks, Quarto compatibility, and git workflow\nVerification steps: Testing and validation before publication\nTroubleshooting: Common issues and solutions\nTimeline: Realistic expectations (40 minutes total)"
  },
  {
    "objectID": "posts/markdowntoblog/analysis/paper/index.html#why-this-matters",
    "href": "posts/markdowntoblog/analysis/paper/index.html#why-this-matters",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "1.2 Why This Matters",
    "text": "1.2 Why This Matters\nBlog posts discoverable through search and social sharing reach a broader audience than markdown files in your project directories. Using ZZCOLLAB ensures:\n\nReproducibility - Docker + renv lock exact environment\nDiscoverability - Integrated with your blog’s search and tagging\nProfessionalism - Proper metadata, structure, and styling\nMaintainability - Version controlled and easily updated\nExtensibility - Can add data analysis, visualizations, or interactive elements later"
  },
  {
    "objectID": "posts/markdowntoblog/analysis/paper/index.html#tools-youll-need",
    "href": "posts/markdowntoblog/analysis/paper/index.html#tools-youll-need",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "2.1 Tools You’ll Need",
    "text": "2.1 Tools You’ll Need\nMust Have: - ZZCOLLAB framework (installed) - Quarto (for rendering) - Git (version control) - Bash shell\nNice to Have: - GitHub CLI (gh) for automation - Docker (for full reproducibility) - Local Quarto preview capability"
  },
  {
    "objectID": "posts/markdowntoblog/analysis/paper/index.html#initial-assessment-5-minutes",
    "href": "posts/markdowntoblog/analysis/paper/index.html#initial-assessment-5-minutes",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "2.2 Initial Assessment: 5 Minutes",
    "text": "2.2 Initial Assessment: 5 Minutes\nBefore starting, evaluate your source markdown:\nAsk Yourself: 1. How long is it? (200 lines? 600 lines?) 2. Does it have code examples? (bash, R, Python?) 3. Does it need images? (hero image, embedded diagrams?) 4. Is it a tutorial, reference guide, or how-to? 5. What’s the target audience?\nExample: GitHub Archive Post\n- Length: ~600 lines (substantial)\n- Code: Yes (bash script)\n- Images: Yes (hero + concept)\n- Type: How-to guide + reference\n- Audience: DevOps engineers\n- Estimate: 10 minutes to convert content"
  },
  {
    "objectID": "posts/markdowntoblog/analysis/paper/index.html#make-key-decisions",
    "href": "posts/markdowntoblog/analysis/paper/index.html#make-key-decisions",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "2.3 Make Key Decisions",
    "text": "2.3 Make Key Decisions\nBefore initializing the project, decide:\n\n\n\n\n\n\n\n\nDecision\nGitHub Archive\nYour Post\n\n\n\n\nBlog slug\ngithub_archive\n[kebab-case-name]\n\n\nZZCOLLAB profile\nubuntu_standard_publishing\nSame (has Quarto)\n\n\nInclude analysis?\nNo\nYes / No\n\n\nMedia assets\n2 images\nHow many?\n\n\nCategories\nDevOps, Git, etc\n?"
  },
  {
    "objectID": "posts/markdowntoblog/analysis/paper/index.html#step-1-create-directory",
    "href": "posts/markdowntoblog/analysis/paper/index.html#step-1-create-directory",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "3.1 Step 1: Create Directory",
    "text": "3.1 Step 1: Create Directory\ncd ~/prj/qblog/posts\nmkdir my_blog_post && cd my_blog_post\nReplace my_blog_post with your kebab-case slug (e.g., github_archive, python_async, data_pipeline)."
  },
  {
    "objectID": "posts/markdowntoblog/analysis/paper/index.html#step-2-initialize-zzcollab",
    "href": "posts/markdowntoblog/analysis/paper/index.html#step-2-initialize-zzcollab",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "3.2 Step 2: Initialize ZZCOLLAB",
    "text": "3.2 Step 2: Initialize ZZCOLLAB\nzzcollab -r ubuntu_standard_publishing\nThis creates: - Standard project structure (analysis/, R/, tests/) - Dockerfile (reproducible environment) - renv.lock (R package versions) - Makefile (build automation) - .zzcollab/manifest.json (project metadata)\nExpected output: ~20 lines of checkmarks showing successful initialization"
  },
  {
    "objectID": "posts/markdowntoblog/analysis/paper/index.html#step-3-set-up-blog-structure",
    "href": "posts/markdowntoblog/analysis/paper/index.html#step-3-set-up-blog-structure",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "3.3 Step 3: Set Up Blog Structure",
    "text": "3.3 Step 3: Set Up Blog Structure\n./modules/setup_symlinks.sh\nThis creates the dual-symlink system that makes everything work:\nAt post root (for Quarto):\nindex.qmd → analysis/paper/index.qmd\nfigures/  → analysis/figures/\nmedia/    → analysis/media/\ndata/     → analysis/data/\nIn analysis/paper/ (for editing):\nfigures/ → ../figures/\nmedia/   → ../media/\ndata/    → ../data/\nWhy symlinks? - Quarto expects posts/*/index.qmd at root - ZZCOLLAB/rrtools puts content in analysis/paper/ - Symlinks bridge both conventions - Image paths like ![](media/images/hero.jpg) work automatically"
  },
  {
    "objectID": "posts/markdowntoblog/analysis/paper/index.html#step-4-verify-structure",
    "href": "posts/markdowntoblog/analysis/paper/index.html#step-4-verify-structure",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "3.4 Step 4: Verify Structure",
    "text": "3.4 Step 4: Verify Structure\n# Check root symlinks\nls -la | grep \"^l\"\n\n# Should show all 4 symlinks ✓\n# index.qmd → analysis/paper/index.qmd\n# figures → analysis/figures\n# media → analysis/media\n# data → analysis/data\n\n# Check paper directory\nls -la analysis/paper/\n\n# Should show:\n# figures → ../figures\n# media → ../media\n# data → ../data\n# index.qmd (actual file)"
  },
  {
    "objectID": "posts/markdowntoblog/analysis/paper/index.html#create-proper-yaml-frontmatter",
    "href": "posts/markdowntoblog/analysis/paper/index.html#create-proper-yaml-frontmatter",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "4.1 Create Proper YAML Frontmatter",
    "text": "4.1 Create Proper YAML Frontmatter\nReplace the template YAML with your blog post metadata:\n---\ntitle: \"Your Blog Post Title\"\nsubtitle: \"Optional subtitle (2-3 words)\"\nauthor: \"Your Name\"\ndate: \"2025-12-02\"\ncategories: [Category1, Category2, Category3]\ndescription: \"2-3 sentence summary of the post\"\nimage: \"media/images/hero-image.jpg\"\ndocument-type: \"blog\"\ndraft: false\nexecute:\n  echo: true\n  warning: false\n  message: false\nformat:\n  html:\n    toc: true\n    toc-depth: 3\n    code-fold: false\n    code-tools: false\n---\nKey fields: - title: 50-70 characters, searchable, descriptive - categories: 2-4 relevant categories for filtering - description: Appears in blog listing, capture value proposition - draft: false: Set to true to hide from publication - date: Publish date (or last-modified for auto-update) - code-fold: false: Show code by default (good for tutorials)"
  },
  {
    "objectID": "posts/markdowntoblog/analysis/paper/index.html#structure-your-content",
    "href": "posts/markdowntoblog/analysis/paper/index.html#structure-your-content",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "4.2 Structure Your Content",
    "text": "4.2 Structure Your Content\nBlog post structure (proven effective):\n1. Hero image + subtitle (1 paragraph)\n2. Introduction (2-3 paragraphs)\n   - What this post covers\n   - Why it matters\n   - What they'll learn\n3. Problem statement or background (2-3 sections)\n4. Solution/main content (multiple detailed sections)\n5. Code examples or implementation (complete, runnable)\n6. Usage instructions (step-by-step)\n7. Examples with actual output\n8. Best practices (tips and recommendations)\n9. Troubleshooting (common issues)\n10. Key takeaways (callout box summary)\n11. Further reading (links to related docs)"
  },
  {
    "objectID": "posts/markdowntoblog/analysis/paper/index.html#adapt-your-markdown",
    "href": "posts/markdowntoblog/analysis/paper/index.html#adapt-your-markdown",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "4.3 Adapt Your Markdown",
    "text": "4.3 Adapt Your Markdown\nMarkdown → Quarto conversions:\n# Simple heading → stays the same\n## Sub-heading → stays the same\n\n[Link text](url) → stays the same\n![Alt text](path) → stays the same\n- Bullet → stays the same\n\n&gt; Blockquote → becomes :::callout-note\nSpecial: Convert blockquotes to Quarto callouts\n::: {.callout-note}\n## Summary\n\nYour key takeaway goes here\n:::"
  },
  {
    "objectID": "posts/markdowntoblog/analysis/paper/index.html#code-blocks-matter",
    "href": "posts/markdowntoblog/analysis/paper/index.html#code-blocks-matter",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "4.4 Code Blocks Matter",
    "text": "4.4 Code Blocks Matter\nMake code blocks production-ready:\n#!/bin/bash\n# Complete, runnable example\n# Comments explaining non-obvious parts\nset -e  # Exit on error\n\n# Actual working code\nyour_command --with-options\nBest practices: - Specify language (bash, r, python, sql, etc) - Make examples self-contained - Include comments for clarity - Test them before publishing"
  },
  {
    "objectID": "posts/markdowntoblog/analysis/paper/index.html#prepare-images",
    "href": "posts/markdowntoblog/analysis/paper/index.html#prepare-images",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "5.1 Prepare Images",
    "text": "5.1 Prepare Images\n# Copy your hero image\ncp ~/path/to/hero-image.jpg analysis/media/images/my-post-hero.jpg\n\n# Copy other images (concept art, diagrams, etc)\ncp ~/path/to/concept-image.png analysis/media/images/my-post-concept.png"
  },
  {
    "objectID": "posts/markdowntoblog/analysis/paper/index.html#document-image-sources",
    "href": "posts/markdowntoblog/analysis/paper/index.html#document-image-sources",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "5.2 Document Image Sources",
    "text": "5.2 Document Image Sources\nCreate a README in the images directory:\ncat &gt; analysis/media/images/README.md &lt;&lt; 'EOF'\n# Image Sources and Attribution\n\n## my-post-hero.jpg\n- Source: Unsplash / Pixabay / Pexels\n- Creator: [Creator Name if known]\n- License: [License type]\n- URL: [Link to original if available]\n\n## my-post-concept.png\n- Source: [Where it came from]\n- License: [CC0, CC-BY, etc]\n- Notes: [Any other relevant info]\nEOF\nWhy document sources? - Respect creator attribution - Document licensing compliance - Make it easy to replace if needed - Good practice for professional blogs"
  },
  {
    "objectID": "posts/markdowntoblog/analysis/paper/index.html#reference-images-in-blog-post",
    "href": "posts/markdowntoblog/analysis/paper/index.html#reference-images-in-blog-post",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "5.3 Reference Images in Blog Post",
    "text": "5.3 Reference Images in Blog Post\n![Descriptive alt text](media/images/my-post-hero.jpg){.img-fluid}\n\n*Subtitle or caption for the image*\nThe {.img-fluid} class makes images responsive on mobile."
  },
  {
    "objectID": "posts/markdowntoblog/analysis/paper/index.html#customize-readme.md",
    "href": "posts/markdowntoblog/analysis/paper/index.html#customize-readme.md",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "6.1 Customize README.md",
    "text": "6.1 Customize README.md\nThe ZZCOLLAB template creates a generic README. Personalize it:\n# Your Blog Post Title\n\n&gt; A blog post about [topic]\n\n## Quick Start\n\nTo read this blog post:\n\n```bash\nquarto render analysis/paper/index.qmd\nopen index.html"
  },
  {
    "objectID": "posts/markdowntoblog/analysis/paper/index.html#structure",
    "href": "posts/markdowntoblog/analysis/paper/index.html#structure",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "6.2 Structure",
    "text": "6.2 Structure\n\nanalysis/paper/index.qmd - Main blog post content\nanalysis/media/images/ - Hero image and diagrams\nDockerfile - Reproducible environment\nrenv.lock - R package versions"
  },
  {
    "objectID": "posts/markdowntoblog/analysis/paper/index.html#to-extend-this-post",
    "href": "posts/markdowntoblog/analysis/paper/index.html#to-extend-this-post",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "6.3 To Extend This Post",
    "text": "6.3 To Extend This Post\nAdd R analysis:\nRscript analysis/scripts/01_prepare_data.R\nRscript analysis/scripts/02_generate_figures.R\nquarto render analysis/paper/index.qmd\n\n## Update DESCRIPTION File\n\n```r\nPackage: myblogpost\nTitle: Your Blog Post Title Here\nDescription: Brief description of what the post covers\nVersion: 1.0.0\nAuthors@R: person(\"Your Name\", role = c(\"aut\", \"cre\"))"
  },
  {
    "objectID": "posts/markdowntoblog/analysis/paper/index.html#quick-validation",
    "href": "posts/markdowntoblog/analysis/paper/index.html#quick-validation",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "7.1 Quick Validation",
    "text": "7.1 Quick Validation\n# Verify symlinks are correct\nls -la index.qmd\n# Should show: index.qmd -&gt; analysis/paper/index.qmd ✓\n\n# Check YAML syntax (proper indentation, no tabs)\nhead -25 analysis/paper/index.qmd\n\n# Verify image paths are relative\ngrep \"images/\" analysis/paper/index.qmd\n# Should show: media/images/filename.jpg (not absolute path)"
  },
  {
    "objectID": "posts/markdowntoblog/analysis/paper/index.html#render-locally-if-quarto-installed",
    "href": "posts/markdowntoblog/analysis/paper/index.html#render-locally-if-quarto-installed",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "7.2 Render Locally (If Quarto Installed)",
    "text": "7.2 Render Locally (If Quarto Installed)\ncd ~/prj/qblog/posts/my_blog_post\nquarto render analysis/paper/index.qmd\n\n# Preview in browser\nopen index.html\nCommon issues:\n\n\n\n\n\n\n\n\nError\nCause\nFix\n\n\n\n\nindex.qmd not found\nSymlink broken\nCheck: ls -la index.qmd\n\n\nImage not found\nWrong path\nUse relative: media/images/file.jpg\n\n\nYAML parse error\nIndentation (tabs?)\nUse spaces only\n\n\nCode not highlighted\nWrong language\nSpecify: ```bash not ```"
  },
  {
    "objectID": "posts/markdowntoblog/analysis/paper/index.html#commit-your-work",
    "href": "posts/markdowntoblog/analysis/paper/index.html#commit-your-work",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "9.1 Commit Your Work",
    "text": "9.1 Commit Your Work\ncd ~/prj/qblog/posts/my_blog_post\n\n# Stage all changes\ngit add .\n\n# Check what you're committing\ngit status\n\n# Commit with descriptive message\ngit commit -m \"Add blog post: Your Post Title\n\n- analysis/paper/index.qmd: Main content\n- analysis/media/images/: Hero and supporting images\n- README.md: Post documentation\n- Docker/renv: Reproducible environment\n\nTopics covered:\n- [Main topic 1]\n- [Main topic 2]\n- [Main topic 3]\""
  },
  {
    "objectID": "posts/markdowntoblog/analysis/paper/index.html#push-to-remote",
    "href": "posts/markdowntoblog/analysis/paper/index.html#push-to-remote",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "9.2 Push to Remote",
    "text": "9.2 Push to Remote\ngit push origin main\nGit handles symlinks automatically - they’re stored as text files containing the target path. When someone clones your blog, symlinks are recreated correctly."
  },
  {
    "objectID": "posts/markdowntoblog/analysis/paper/index.html#timeline-and-checklist",
    "href": "posts/markdowntoblog/analysis/paper/index.html#timeline-and-checklist",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "10.1 Timeline and Checklist",
    "text": "10.1 Timeline and Checklist\n\n\n\nPhase\nTime\nTask\n\n\n\n\nAssessment\n5 min\nRead markdown, plan structure\n\n\nInitialize\n3 min\nCreate directory, run zzcollab\n\n\nSetup\n1 min\nRun setup_symlinks.sh\n\n\nConvert\n10 min\nAdapt markdown → Quarto\n\n\nMedia\n5 min\nAdd images, document sources\n\n\nMetadata\n3 min\nUpdate README, DESCRIPTION\n\n\nTest\n5 min\nVerify structure, validate\n\n\nCommit\n2 min\nGit add/commit/push\n\n\nTotal\n~40 min\nBlog post ready"
  },
  {
    "objectID": "posts/markdowntoblog/analysis/paper/index.html#pre-publication-checklist",
    "href": "posts/markdowntoblog/analysis/paper/index.html#pre-publication-checklist",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "10.2 Pre-Publication Checklist",
    "text": "10.2 Pre-Publication Checklist\nContent: - [ ] Blog post written (analysis/paper/index.qmd) - [ ] All sections complete - [ ] Code examples tested - [ ] Links verified (no 404s) - [ ] Grammar/spelling checked\nMetadata: - [ ] YAML frontmatter correct - [ ] Title descriptive (50-70 chars) - [ ] Categories relevant - [ ] Description captures value - [ ] draft: false (to publish)\nMedia: - [ ] Hero image added - [ ] Images documented in README.md - [ ] All image paths relative (media/images/...) - [ ] Image alt text present - [ ] File sizes reasonable (&lt; 500KB each)\nStructure: - [ ] Symlinks verified (4 at root) - [ ] README.md customized - [ ] DESCRIPTION updated - [ ] .gitignore appropriate\nTesting: - [ ] Local render successful (if Quarto installed) - [ ] All images display - [ ] All links work - [ ] Code syntax highlighted - [ ] TOC generates correctly"
  },
  {
    "objectID": "posts/markdowntoblog/analysis/paper/index.html#add-r-analysis",
    "href": "posts/markdowntoblog/analysis/paper/index.html#add-r-analysis",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "11.1 Add R Analysis",
    "text": "11.1 Add R Analysis\nCreate analysis scripts:\n# analysis/scripts/01_prepare_data.R\nlibrary(tidyverse)\n# ... your analysis code ...\nwrite_csv(results, \"analysis/data/derived_data/results.csv\")\nUpdate Makefile:\npost-analysis:\n    Rscript analysis/scripts/01_prepare_data.R\n    Rscript analysis/scripts/02_generate_figures.R\n\npost-render: post-analysis\n    quarto render index.qmd"
  },
  {
    "objectID": "posts/markdowntoblog/analysis/paper/index.html#add-interactive-content",
    "href": "posts/markdowntoblog/analysis/paper/index.html#add-interactive-content",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "11.2 Add Interactive Content",
    "text": "11.2 Add Interactive Content\nQuarto supports multiple languages:\n\n// Observable JS for interactive visualizations\nPlot.plot({\n  // ... your D3/Observable code ...\n})\n\n\n\n\n\n\n\n# R code with Shiny for interactive elements\nlibrary(shiny)\nlibrary(plotly)"
  },
  {
    "objectID": "posts/markdowntoblog/analysis/paper/index.html#version-updates",
    "href": "posts/markdowntoblog/analysis/paper/index.html#version-updates",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "11.3 Version Updates",
    "text": "11.3 Version Updates\nWhen you update the post:\n# Edit content\nvim analysis/paper/index.qmd\n\n# Update date (if using last-modified)\n# Or change manually: date: \"2025-12-15\"\n\n# Render and test\nquarto render analysis/paper/index.qmd\n\n# Commit update\ngit add .\ngit commit -m \"Update post: [what changed]\"\ngit push"
  },
  {
    "objectID": "posts/markdowntoblog/analysis/paper/index.html#symlink-issues",
    "href": "posts/markdowntoblog/analysis/paper/index.html#symlink-issues",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "12.1 Symlink Issues",
    "text": "12.1 Symlink Issues\nSymlink appears broken after cloning:\n# Git may not have recreated symlinks correctly\n# Recreate manually:\ncd ~/prj/qblog/posts/my_blog_post\nrm index.qmd media figures data\nln -s analysis/paper/index.qmd index.qmd\nln -s analysis/media media\nln -s analysis/figures figures\nln -s analysis/data data"
  },
  {
    "objectID": "posts/markdowntoblog/analysis/paper/index.html#yaml-errors",
    "href": "posts/markdowntoblog/analysis/paper/index.html#yaml-errors",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "12.2 YAML Errors",
    "text": "12.2 YAML Errors\n“YAML parsing error”:\n# Check for tabs (they're not allowed)\ncat -A analysis/paper/index.qmd | head -30\n# Tabs show as ^I, spaces don't\n\n# Fix by using your editor in spaces-only mode\nvim analysis/paper/index.qmd\n:set expandtab"
  },
  {
    "objectID": "posts/markdowntoblog/analysis/paper/index.html#quarto-issues",
    "href": "posts/markdowntoblog/analysis/paper/index.html#quarto-issues",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "12.3 Quarto Issues",
    "text": "12.3 Quarto Issues\n“Quarto executable not found”:\n# Install Quarto\n# macOS:\nbrew install quarto\n\n# Or download from: https://quarto.org/docs/get-started/\nCode blocks not syntax-highlighted:\n# Wrong:\n```shell\ncode"
  },
  {
    "objectID": "posts/markdowntoblog/analysis/paper/index.html#structure-used-here",
    "href": "posts/markdowntoblog/analysis/paper/index.html#structure-used-here",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "14.1 Structure Used Here",
    "text": "14.1 Structure Used Here\nmarkdown_to_blog/\n├── index.qmd (symlink) → analysis/paper/index.qmd\n├── media/ (symlink) → analysis/media/\n├── analysis/\n│   ├── paper/\n│   │   └── index.qmd (THIS FILE, ~3000 words)\n│   └── media/\n│       └── images/\n│           ├── README.md (document image sources)\n│           └── [hero and concept images]"
  },
  {
    "objectID": "posts/markdowntoblog/analysis/paper/index.html#what-was-converted",
    "href": "posts/markdowntoblog/analysis/paper/index.html#what-was-converted",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "14.2 What Was Converted",
    "text": "14.2 What Was Converted\nSource: markdown_to_blogpost.md (process documentation, ~1300 lines)\nConverted to: This blog post (teaching audience how to do it)"
  },
  {
    "objectID": "posts/markdowntoblog/analysis/paper/index.html#key-decisions-made",
    "href": "posts/markdowntoblog/analysis/paper/index.html#key-decisions-made",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "14.3 Key Decisions Made",
    "text": "14.3 Key Decisions Made\n\n\n\nDecision\nOutcome\n\n\n\n\nSlug\nmarkdown_to_blog (kebab-case)\n\n\nProfile\nubuntu_standard_publishing\n\n\nContent\nEducational (teaching, not documenting)\n\n\nStructure\nProgressive (overview → details → checklist)\n\n\nExamples\nReference actual GitHub archive post"
  },
  {
    "objectID": "posts/lssinceutility/analysis/paper/index.html",
    "href": "posts/lssinceutility/analysis/paper/index.html",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "",
    "text": "Finding files within specific time windows is a common task in research computing. Whether you’re auditing a research project, discovering recent changes, or managing data across timeframes, traditional Unix tools like find and ls require complex syntax and date format conversions.\nls_since.sh solves this problem with an intuitive, feature-rich utility that combines the power of Unix tools with thoughtful interface design. This post documents the utility’s architecture, features, and practical use cases—demonstrating why well-designed command-line tools can significantly improve productivity.\n\n\n\nProblem Space: Why date-based file filtering matters\nCore Features: The complete feature set explained\nTechnical Architecture: How the utility works internally\nPractical Examples: Real-world use cases and code recipes\nIntegration Patterns: Combining with other tools like fzf\nPerformance: Scalability and optimization considerations"
  },
  {
    "objectID": "posts/lssinceutility/analysis/paper/index.html#what-this-post-covers",
    "href": "posts/lssinceutility/analysis/paper/index.html#what-this-post-covers",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "",
    "text": "Problem Space: Why date-based file filtering matters\nCore Features: The complete feature set explained\nTechnical Architecture: How the utility works internally\nPractical Examples: Real-world use cases and code recipes\nIntegration Patterns: Combining with other tools like fzf\nPerformance: Scalability and optimization considerations"
  },
  {
    "objectID": "posts/lssinceutility/analysis/paper/index.html#why-date-based-file-filtering-matters",
    "href": "posts/lssinceutility/analysis/paper/index.html#why-date-based-file-filtering-matters",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "2.1 Why Date-Based File Filtering Matters",
    "text": "2.1 Why Date-Based File Filtering Matters\nResearch workflows generate thousands of files. Organizing and discovering them by creation or modification date is essential for:\n\nResearch Project Auditing\n\nFinding all files generated during a specific analysis phase\n\nVersion Control Workflows\n\nLocating uncommitted changes within a date range\n\nData Management\n\nIdentifying stale or recent files for archival or backup\n\nCollaboration Tracking\n\nDiscovering contributions from team members during specific periods\n\nLog Analysis\n\nFinding application-generated artifacts within time windows"
  },
  {
    "objectID": "posts/lssinceutility/analysis/paper/index.html#limitations-of-standard-tools",
    "href": "posts/lssinceutility/analysis/paper/index.html#limitations-of-standard-tools",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "2.2 Limitations of Standard Tools",
    "text": "2.2 Limitations of Standard Tools\nStandard Unix utilities have significant limitations for this task:\n\n\n\n\n\n\n\n\nTool\nStrength\nLimitation\n\n\n\n\nfind -newermt\nPowerful filtering\nComplex date format requirements\n\n\nls -lt\nSimple output\nSorts all files, doesn’t filter by date range\n\n\nstat\nDetailed information\nRequires per-file examination\n\n\nDate comparisons\nFlexible\nError-prone and platform-specific\n\n\n\n\n\n\nStreamlined file discovery workflow"
  },
  {
    "objectID": "posts/lssinceutility/analysis/paper/index.html#the-three-stage-filtering-pipeline",
    "href": "posts/lssinceutility/analysis/paper/index.html#the-three-stage-filtering-pipeline",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "3.1 The Three-Stage Filtering Pipeline",
    "text": "3.1 The Three-Stage Filtering Pipeline\nls_since.sh implements a streamlined filtering architecture:\nFind Phase → Timestamp Comparison → Output Formatting\n\n3.1.1 Stage 1: Find Phase\n\nRecursively discovers files in directory hierarchy\nFilters by file extension (configurable or all files)\nExcludes .git directories automatically (saves 30-40% processing time)\nReturns canonical file paths for processing\n\n\n\n3.1.2 Stage 2: Timestamp Comparison\nThe utility supports three orthogonal timestamp sources:\n\nbirth (default): File creation/copy time\nmtime: Last modification time\natime: Last access time\n\nDates are converted to Unix epoch timestamps for efficient integer comparisons:\n# Input: 01nov2025 → Internal: YYYY-MM-DD → Unix timestamp\nTARGET_TIMESTAMP=$(date -j -f \"%Y-%m-%d\" \"$TARGET_DATE\" \"+%s\")\n\n\n3.1.3 Stage 3: Output Formatting\nFour output modes for different use cases:\n\nNormal: TIMESTAMP - filepath for human readability\nCount: Total file count for statistics\nPaths-only: Raw file paths for piping\nfzf: Interactive selection interface"
  },
  {
    "objectID": "posts/lssinceutility/analysis/paper/index.html#date-input-format",
    "href": "posts/lssinceutility/analysis/paper/index.html#date-input-format",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "3.2 Date Input Format",
    "text": "3.2 Date Input Format\nThe utility standardizes on DDmmmYYYY format with lowercase months:\n01nov2025    # November 1, 2025\n15dec2024    # December 15, 2024\n28feb2025    # February 28, 2025\nThis approach: - Eliminates ambiguity (01/02/2025 is ambiguous; 01feb2025 is not) - Works consistently across locales - Avoids numeric month errors"
  },
  {
    "objectID": "posts/lssinceutility/analysis/paper/index.html#extension-filtering",
    "href": "posts/lssinceutility/analysis/paper/index.html#extension-filtering",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "3.3 Extension Filtering",
    "text": "3.3 Extension Filtering\nDefault file types optimized for research computing:\nEXTENSIONS=(\"md\" \"Rmd\" \"qmd\" \"sh\" \"pdf\" \"R\")\nSupports three filtering modes:\n\nDefault extensions: Works without flags\nCustom extensions: -t sh,py,txt or -t json,yaml\nAll files: -t all for any file type"
  },
  {
    "objectID": "posts/lssinceutility/analysis/paper/index.html#basic-syntax",
    "href": "posts/lssinceutility/analysis/paper/index.html#basic-syntax",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "4.1 Basic Syntax",
    "text": "4.1 Basic Syntax\nls_since.sh [OPTIONS] [directory] [date]"
  },
  {
    "objectID": "posts/lssinceutility/analysis/paper/index.html#essential-options",
    "href": "posts/lssinceutility/analysis/paper/index.html#essential-options",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "4.2 Essential Options",
    "text": "4.2 Essential Options\n# Filtering\n-t, --type STR           File extensions (comma-separated or 'all')\n-s, --start DATE         Start date in DDmmmYYYY format\n-e, --end DATE           End date (optional, creates date range)\n-T, --timestamp TYPE     Type: birth, mtime, atime\n\n# Output\n-c, --count              Count files instead of listing\n-p, --paths-only         Output paths only (no headers)\n--fzf                    Pipe to fzf for interactive selection\n\n# Utilities\n-C, --calendar           Show ASCII calendars as reference\n--no-color               Suppress green highlighting\n-h, --help               Display help and examples"
  },
  {
    "objectID": "posts/lssinceutility/analysis/paper/index.html#positional-arguments",
    "href": "posts/lssinceutility/analysis/paper/index.html#positional-arguments",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "4.3 Positional Arguments",
    "text": "4.3 Positional Arguments\n\nNo arguments: Interactive mode (prompts for everything)\nOne argument: Treated as date\nTwo arguments: Directory and date"
  },
  {
    "objectID": "posts/lssinceutility/analysis/paper/index.html#example-1-interactive-mode-with-defaults",
    "href": "posts/lssinceutility/analysis/paper/index.html#example-1-interactive-mode-with-defaults",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "5.1 Example 1: Interactive Mode with Defaults",
    "text": "5.1 Example 1: Interactive Mode with Defaults\n# Start interactive mode with 8-day window\nls_since.sh\n\n# Prompts for:\n# - Start date (default: 8 days ago)\n# - End date (default: today)\n# - File extensions (default: md,Rmd,qmd,sh,pdf,R)\nPerfect for exploring recent changes without command syntax."
  },
  {
    "objectID": "posts/lssinceutility/analysis/paper/index.html#example-2-research-project-auditing",
    "href": "posts/lssinceutility/analysis/paper/index.html#example-2-research-project-auditing",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "5.2 Example 2: Research Project Auditing",
    "text": "5.2 Example 2: Research Project Auditing\n# Find all R analysis files from November 2025\nls_since.sh -s 01nov2025 -e 30nov2025 -t R,Rmd ~/research/analysis\n\n# Output: R files with timestamps\n# 2025-11-15 10:23:45 - ~/research/analysis/main_analysis.R\n# 2025-11-12 14:12:33 - ~/research/analysis/utils.R"
  },
  {
    "objectID": "posts/lssinceutility/analysis/paper/index.html#example-3-track-recent-modifications",
    "href": "posts/lssinceutility/analysis/paper/index.html#example-3-track-recent-modifications",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "5.3 Example 3: Track Recent Modifications",
    "text": "5.3 Example 3: Track Recent Modifications\n# Find markdown docs modified in the last 2 weeks\nls_since.sh -T mtime -s 18nov2025 -t md ~/docs\n\n# Captures the last editing session for each file"
  },
  {
    "objectID": "posts/lssinceutility/analysis/paper/index.html#example-4-interactive-file-selection",
    "href": "posts/lssinceutility/analysis/paper/index.html#example-4-interactive-file-selection",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "5.4 Example 4: Interactive File Selection",
    "text": "5.4 Example 4: Interactive File Selection\n# Browse and select shell scripts using fzf\nls_since.sh --fzf -t sh 01jan2025\n\n# Opens fzf interface for interactive selection\n# Selected file can be piped to other commands"
  },
  {
    "objectID": "posts/lssinceutility/analysis/paper/index.html#example-5-pipeline-integration",
    "href": "posts/lssinceutility/analysis/paper/index.html#example-5-pipeline-integration",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "5.5 Example 5: Pipeline Integration",
    "text": "5.5 Example 5: Pipeline Integration\n# Edit recently modified scripts in vim\nls_since.sh -p -T mtime -s 01nov2025 -t sh | xargs vim\n\n# Opens all recently modified shell scripts in vim editor"
  },
  {
    "objectID": "posts/lssinceutility/analysis/paper/index.html#example-6-file-count-statistics",
    "href": "posts/lssinceutility/analysis/paper/index.html#example-6-file-count-statistics",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "5.6 Example 6: File Count Statistics",
    "text": "5.6 Example 6: File Count Statistics\n# Count all files generated in December 2024\nls_since.sh -c -s 01dec2024 -e 31dec2024 -t all\n\n# Output:\n# Total files found: 347"
  },
  {
    "objectID": "posts/lssinceutility/analysis/paper/index.html#example-7-access-time-analysis",
    "href": "posts/lssinceutility/analysis/paper/index.html#example-7-access-time-analysis",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "5.7 Example 7: Access Time Analysis",
    "text": "5.7 Example 7: Access Time Analysis\n# Find frequently accessed log files\nls_since.sh -T atime -s 15nov2025 ~/logs\n\n# Shows files accessed in the last 17 days\n\n\n\nInteractive exploration in action"
  },
  {
    "objectID": "posts/lssinceutility/analysis/paper/index.html#flow",
    "href": "posts/lssinceutility/analysis/paper/index.html#flow",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "6.1 Flow",
    "text": "6.1 Flow\n\nCalculate defaults: 8 days ago to today\nDisplay calendar reference (if -C flag used)\nPrompt for start date: Press Enter for default or type date\nPrompt for end date: Optional, press Enter for today\nSelect extensions: Choose defaults or customize\nDisplay selected dates: Confirm before processing\nExecute search: Begin file discovery"
  },
  {
    "objectID": "posts/lssinceutility/analysis/paper/index.html#visual-calendar-reference",
    "href": "posts/lssinceutility/analysis/paper/index.html#visual-calendar-reference",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "6.2 Visual Calendar Reference",
    "text": "6.2 Visual Calendar Reference\nREFERENCE CALENDARS:\n\nPrevious Month:\n    November 2025\nSu Mo Tu We Th Fr Sa\n                   1\n 2  3  4  5  6  7  8\n 9 10 11 12 13 14 15\n16 17 18 19 20 21 22\n23 24 25 26 27 28 29\n30\n\nCurrent Month:\n    December 2025\nSu Mo Tu We Th Fr Sa\n    1  2  3  4  5  6\n 7  8  9 10 11 12 13\n14 15 16 17 18 19 20\n21 22 23 24 25 26 27\n28 29 30 31\nSelected dates highlighted in green for visual confirmation."
  },
  {
    "objectID": "posts/lssinceutility/analysis/paper/index.html#platform-compatibility",
    "href": "posts/lssinceutility/analysis/paper/index.html#platform-compatibility",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "7.1 Platform Compatibility",
    "text": "7.1 Platform Compatibility\nThe utility seamlessly handles platform differences:\nmacOS-specific stat syntax:\nstat -f %B \"$file\"  # Birth time\nstat -f %m \"$file\"  # Modification time\nstat -f %a \"$file\"  # Access time\nLinux-specific stat syntax:\nstat -c %W \"$file\"  # Birth time (with fallback to mtime)\nstat -c %Y \"$file\"  # Modification time\nstat -c %X \"$file\"  # Access time\nAutomatic detection via [[ \"$OSTYPE\" == \"darwin\"* ]]"
  },
  {
    "objectID": "posts/lssinceutility/analysis/paper/index.html#fzf-integration-architecture",
    "href": "posts/lssinceutility/analysis/paper/index.html#fzf-integration-architecture",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "7.2 fzf Integration Architecture",
    "text": "7.2 fzf Integration Architecture\nWhen --fzf flag is used, the utility implements silent output collection:\n\nCreate temporary file at startup\nRedirect file paths to temp file (not stdout)\nSuppress headers/footers in fzf mode\nPipe temp file to fzf at completion\nClean up temporary file after selection\n\nThis ensures: - No duplicate output (files not listed then piped) - Clean terminal for fzf UI - Proper file path transmission to fzf"
  },
  {
    "objectID": "posts/lssinceutility/analysis/paper/index.html#temporary-file-management",
    "href": "posts/lssinceutility/analysis/paper/index.html#temporary-file-management",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "7.3 Temporary File Management",
    "text": "7.3 Temporary File Management\nThe utility uses secure temporary files for: - File path collection (fzf mode) - File count tracking (prevents subshell variable loss) - Extension filtering\nAll temporary files are cleaned up with rm -f at completion."
  },
  {
    "objectID": "posts/lssinceutility/analysis/paper/index.html#error-handling",
    "href": "posts/lssinceutility/analysis/paper/index.html#error-handling",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "7.4 Error Handling",
    "text": "7.4 Error Handling\nComprehensive validation for: - Missing or invalid directories - Invalid date formats with descriptive messages - Missing dependencies (fzf validation on --fzf) - Invalid timestamp types - Subshell context preservation"
  },
  {
    "objectID": "posts/lssinceutility/analysis/paper/index.html#benchmark-results",
    "href": "posts/lssinceutility/analysis/paper/index.html#benchmark-results",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "8.1 Benchmark Results",
    "text": "8.1 Benchmark Results\nOn typical research project directories (10,000 files):\n\n\n\nScenario\nTime\nNotes\n\n\n\n\nSmall range (1-day)\n~200ms\nMinimal filtering\n\n\nMedium range (30-day)\n~200ms\nStandard use case\n\n\nLarge range (1-year)\n~250ms\nFull year search\n\n\nStartup overhead\n~10ms\nNegligible\n\n\n\nResults consistent across macOS and Linux with SSD storage."
  },
  {
    "objectID": "posts/lssinceutility/analysis/paper/index.html#time-complexity",
    "href": "posts/lssinceutility/analysis/paper/index.html#time-complexity",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "8.2 Time Complexity",
    "text": "8.2 Time Complexity\n\nOverall: O(n) where n = number of files in tree\nPer-file: O(1) timestamp comparison\nSingle pass through directory hierarchy\nConstant-time integer comparisons"
  },
  {
    "objectID": "posts/lssinceutility/analysis/paper/index.html#space-complexity",
    "href": "posts/lssinceutility/analysis/paper/index.html#space-complexity",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "8.3 Space Complexity",
    "text": "8.3 Space Complexity\n\nOutput: O(m) where m = number of matching files\nfzf mode: Requires temporary file storage\nNormal mode: Streaming output (minimal memory)"
  },
  {
    "objectID": "posts/lssinceutility/analysis/paper/index.html#optimization-tips",
    "href": "posts/lssinceutility/analysis/paper/index.html#optimization-tips",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "8.4 Optimization Tips",
    "text": "8.4 Optimization Tips\n\nUse specific dates: Narrow ranges reduce file checks\nFilter by extension: Fewer files to examine with -t flag\nAutomatic .git exclusion: Saves 30-40% processing time\nUse mtime on Linux: Faster than birth time (no fallback needed)"
  },
  {
    "objectID": "posts/lssinceutility/analysis/paper/index.html#vs.-find--newermt",
    "href": "posts/lssinceutility/analysis/paper/index.html#vs.-find--newermt",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "9.1 vs. find -newermt",
    "text": "9.1 vs. find -newermt\nAdvantages of ls_since.sh: - Simpler syntax (no date format conversion required) - Multiple timestamp type support - Interactive mode with defaults - Integrated calendar reference - fzf integration built-in\nAdvantages of find: - Available on all systems - Minimal dependencies - More extensive filtering options"
  },
  {
    "objectID": "posts/lssinceutility/analysis/paper/index.html#vs.-ls--lt",
    "href": "posts/lssinceutility/analysis/paper/index.html#vs.-ls--lt",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "9.2 vs. ls -lt",
    "text": "9.2 vs. ls -lt\nAdvantages of ls_since.sh: - Date range filtering - Recursive directory traversal - Extensible filtering options - fzf integration\nAdvantages of ls: - No dependencies - Simpler for interactive use"
  },
  {
    "objectID": "posts/lssinceutility/analysis/paper/index.html#vs.-locatemlocate",
    "href": "posts/lssinceutility/analysis/paper/index.html#vs.-locatemlocate",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "9.3 vs. locate/mlocate",
    "text": "9.3 vs. locate/mlocate\nAdvantages of ls_since.sh: - Real-time results (no database needed) - Date range filtering - Timestamp type selection\nAdvantages of locate: - Faster for very large filesystems - Pre-built database"
  },
  {
    "objectID": "posts/lssinceutility/analysis/paper/index.html#pattern-1-monthly-audit-reports",
    "href": "posts/lssinceutility/analysis/paper/index.html#pattern-1-monthly-audit-reports",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "10.1 Pattern 1: Monthly Audit Reports",
    "text": "10.1 Pattern 1: Monthly Audit Reports\n# Generate audit for each month\nfor month in {01..12}; do\n  count=$(ls_since.sh -c -s ${month}jan2025 \\\n          -e 31${month}2025 -t all 2&gt;/dev/null | \\\n          tail -1 | awk '{print $NF}')\n  echo \"Month $month: $count files\"\ndone"
  },
  {
    "objectID": "posts/lssinceutility/analysis/paper/index.html#pattern-2-recent-work-summary",
    "href": "posts/lssinceutility/analysis/paper/index.html#pattern-2-recent-work-summary",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "10.2 Pattern 2: Recent Work Summary",
    "text": "10.2 Pattern 2: Recent Work Summary\n# Show recent modifications by file type\necho \"=== Shell Scripts ===\"\nls_since.sh -s 01nov2025 -t sh | head -5\n\necho \"=== Documentation ===\"\nls_since.sh -s 01nov2025 -t md,Rmd | head -5\n\necho \"=== Analysis ===\"\nls_since.sh -s 01nov2025 -t R,Rmd,qmd | head -5"
  },
  {
    "objectID": "posts/lssinceutility/analysis/paper/index.html#pattern-3-git-aware-file-discovery",
    "href": "posts/lssinceutility/analysis/paper/index.html#pattern-3-git-aware-file-discovery",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "10.3 Pattern 3: Git-aware File Discovery",
    "text": "10.3 Pattern 3: Git-aware File Discovery\n# Find unstaged files modified after date\ngit ls-files -m | while read file; do\n  ls_since.sh -p -s 01nov2025 | grep -q \"$file\" && echo \"$file\"\ndone"
  },
  {
    "objectID": "posts/lssinceutility/analysis/paper/index.html#pattern-4-backup-selection",
    "href": "posts/lssinceutility/analysis/paper/index.html#pattern-4-backup-selection",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "10.4 Pattern 4: Backup Selection",
    "text": "10.4 Pattern 4: Backup Selection\n# Backup files modified in last week\nls_since.sh -p -T mtime -s 25nov2025 -t all | \\\n  tar -czf backup_nov25.tar.gz -T -"
  },
  {
    "objectID": "posts/lssinceutility/analysis/paper/index.html#pattern-5-code-review-workflow",
    "href": "posts/lssinceutility/analysis/paper/index.html#pattern-5-code-review-workflow",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "10.5 Pattern 5: Code Review Workflow",
    "text": "10.5 Pattern 5: Code Review Workflow\n# Review recent changes in specific file type\nls_since.sh --fzf -t R -s 01nov2025 | \\\n  xargs git diff HEAD~1..HEAD --"
  },
  {
    "objectID": "posts/lssinceutility/analysis/paper/index.html#installation",
    "href": "posts/lssinceutility/analysis/paper/index.html#installation",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "11.1 Installation",
    "text": "11.1 Installation\nCopy ls_since.sh to your bin directory:\n# Copy to personal bin\ncp ls_since.sh ~/bin/\nchmod +x ~/bin/ls_since.sh\n\n# Or add to project\ncp ls_since.sh ./scripts/"
  },
  {
    "objectID": "posts/lssinceutility/analysis/paper/index.html#first-use-interactive-mode",
    "href": "posts/lssinceutility/analysis/paper/index.html#first-use-interactive-mode",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "11.2 First Use: Interactive Mode",
    "text": "11.2 First Use: Interactive Mode\n# Start with no arguments for guided experience\nls_since.sh\n\n# Prompts you through:\n# 1. Start date selection (with default)\n# 2. End date selection (with default)\n# 3. File type selection (with defaults)\n# 4. Displays calendars for reference"
  },
  {
    "objectID": "posts/lssinceutility/analysis/paper/index.html#common-commands-cheat-sheet",
    "href": "posts/lssinceutility/analysis/paper/index.html#common-commands-cheat-sheet",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "11.3 Common Commands Cheat Sheet",
    "text": "11.3 Common Commands Cheat Sheet\n# List all default types since 8 days ago\nls_since.sh\n\n# List shell scripts from November\nls_since.sh -s 01nov2025 -e 30nov2025 -t sh\n\n# Count files in last 2 weeks\nls_since.sh -c -s 18nov2025\n\n# Interactive file selection\nls_since.sh --fzf -t R,Rmd 01jan2025\n\n# Pipe to editor\nls_since.sh -p -T mtime -s 01nov2025 -t md | xargs vim\n\n# Display help\nls_since.sh -h\n\n\n\nReady to explore your file system"
  },
  {
    "objectID": "posts/lssinceutility/analysis/paper/index.html#calendar-dates-not-highlighted-in-green",
    "href": "posts/lssinceutility/analysis/paper/index.html#calendar-dates-not-highlighted-in-green",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "12.1 Calendar dates not highlighted in green",
    "text": "12.1 Calendar dates not highlighted in green\nCause: Terminal doesn’t support ANSI color codes\nSolution: Use --no-color flag to suppress coloring\nls_since.sh -C --no-color -s 01nov2025"
  },
  {
    "objectID": "posts/lssinceutility/analysis/paper/index.html#birth-time-unavailable-on-linux",
    "href": "posts/lssinceutility/analysis/paper/index.html#birth-time-unavailable-on-linux",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "12.2 Birth time unavailable on Linux",
    "text": "12.2 Birth time unavailable on Linux\nCause: Linux filesystems may not store birth time\nSolution: Use modification time instead\nls_since.sh -T mtime -s 01nov2025"
  },
  {
    "objectID": "posts/lssinceutility/analysis/paper/index.html#fzf-not-found-error",
    "href": "posts/lssinceutility/analysis/paper/index.html#fzf-not-found-error",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "12.3 fzf not found error",
    "text": "12.3 fzf not found error\nCause: fzf not installed\nSolution: Install with appropriate package manager\n# macOS\nbrew install fzf\n\n# Ubuntu/Debian\nsudo apt-get install fzf\n\n# Then use\nls_since.sh --fzf -t sh 01jan2025"
  },
  {
    "objectID": "posts/lssinceutility/analysis/paper/index.html#no-files-found-in-date-range",
    "href": "posts/lssinceutility/analysis/paper/index.html#no-files-found-in-date-range",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "12.4 No files found in date range",
    "text": "12.4 No files found in date range\nCause: Files don’t exist in range or extension doesn’t match\nSolution: Check date format and try broader type\n# Try all file types\nls_since.sh -t all -s 01nov2025 -e 30nov2025\n\n# Check file dates\nls_since.sh -p -s 01jan2024 -t all | head"
  },
  {
    "objectID": "posts/lssinceutility/analysis/paper/index.html#related-utilities",
    "href": "posts/lssinceutility/analysis/paper/index.html#related-utilities",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "14.1 Related Utilities",
    "text": "14.1 Related Utilities\n\nfind command: man find for advanced filtering\nstat command: man stat for detailed file information\nfzf: junegunn/fzf for interactive selection\nQuarto: For research computing workflows"
  },
  {
    "objectID": "posts/lssinceutility/analysis/paper/index.html#best-practices",
    "href": "posts/lssinceutility/analysis/paper/index.html#best-practices",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "14.2 Best Practices",
    "text": "14.2 Best Practices\n\nUse interactive mode for first-time exploration\nVerify date ranges with calendar reference (-C flag)\nTest pipelines before integrating into scripts\nCombine with other tools for powerful workflows\nVerify output before destructive operations"
  },
  {
    "objectID": "posts/lssinceutility/analysis/paper/index.html#using-standard-unix-tools-for-similar-functionality",
    "href": "posts/lssinceutility/analysis/paper/index.html#using-standard-unix-tools-for-similar-functionality",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "15.1 Using Standard Unix Tools for Similar Functionality",
    "text": "15.1 Using Standard Unix Tools for Similar Functionality\nWhile ls_since.sh provides an intuitive, feature-rich interface, the same date-range filtering can be accomplished using the standard find command available on all Unix/Linux systems. This appendix demonstrates how to replicate the core functionality using only built-in tools."
  },
  {
    "objectID": "posts/lssinceutility/analysis/paper/index.html#enhanced-find-command-for-rmd-files-with-date-range",
    "href": "posts/lssinceutility/analysis/paper/index.html#enhanced-find-command-for-rmd-files-with-date-range",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "15.2 Enhanced find Command for Rmd Files with Date Range",
    "text": "15.2 Enhanced find Command for Rmd Files with Date Range\nfind . -type f -name \"*.Rmd\" -newermt \"2025-11-01\" ! -newermt \"2025-12-01\" \\\n  -printf '%T@ %TY-%Tm-%Td %TH:%TM:%.2TS %p\\n' | \\\n  sort -rn | \\\n  cut -d' ' -f4- | \\\n  fzf --preview 'head -20 {}'"
  },
  {
    "objectID": "posts/lssinceutility/analysis/paper/index.html#command-breakdown",
    "href": "posts/lssinceutility/analysis/paper/index.html#command-breakdown",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "15.3 Command Breakdown",
    "text": "15.3 Command Breakdown\n\n\n\n\n\n\n\n\nComponent\nPurpose\nDetails\n\n\n\n\nfind .\nSearch current directory\nChange . to specify different directory\n\n\n-type f\nOnly regular files\nExcludes directories and symlinks\n\n\n-name \"*.Rmd\"\nFilter by file extension\nChange extension pattern as needed\n\n\n-newermt \"2025-11-01\"\nStart date (inclusive)\nFiles modified after this date\n\n\n! -newermt \"2025-12-01\"\nEnd date (exclusive)\nFiles NOT modified on/after this date\n\n\n-printf '%T@ %TY-%Tm-%Td %TH:%TM:%.2TS %p\\n'\nFormat output with timestamp\nShows: epoch, ISO date, time, filepath\n\n\nsort -rn\nSort by timestamp, newest first\n-rn = reverse numeric sort\n\n\ncut -d' ' -f4-\nExtract readable date and path\nRemoves epoch timestamp for cleaner output\n\n\nfzf --preview 'head -20 {}'\nInteractive file selection\nPreview shows first 20 lines of file"
  },
  {
    "objectID": "posts/lssinceutility/analysis/paper/index.html#practical-usage-examples",
    "href": "posts/lssinceutility/analysis/paper/index.html#practical-usage-examples",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "15.4 Practical Usage Examples",
    "text": "15.4 Practical Usage Examples\nFind and edit Rmd files from November in your research directory:\nfind ~/research -type f -name \"*.Rmd\" -newermt \"2025-11-01\" ! -newermt \"2025-12-01\" | \\\n  fzf | xargs vim\nSelect and copy multiple files to backup:\nfind . -type f -name \"*.Rmd\" -newermt \"2025-11-15\" ! -newermt \"2025-11-30\" | \\\n  fzf -m | xargs -I {} cp {} ~/backup/\nPreview analysis files before processing:\nfind ~/analysis -type f -name \"*.Rmd\" -newermt \"2025-11-01\" ! -newermt \"2025-12-01\" | \\\n  fzf --preview 'wc -l {}; head -30 {}'"
  },
  {
    "objectID": "posts/lssinceutility/analysis/paper/index.html#advantages-and-trade-offs",
    "href": "posts/lssinceutility/analysis/paper/index.html#advantages-and-trade-offs",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "15.5 Advantages and Trade-offs",
    "text": "15.5 Advantages and Trade-offs\nAdvantages of find: - No additional dependencies (built into all Unix/Linux systems) - Powerful filtering options (combine multiple conditions) - Direct pipeline to other commands - Excellent for scripting and automation - Consistent behavior across platforms\nAdvantages of ls_since.sh: - Friendlier date format (01nov2025 vs 2025-11-01) - Interactive mode with sensible defaults - Calendar reference visualization - Built-in extension presets - Cleaner error messages and help text - Optimized for human interaction\nWhen to use find: - Scripts and automation workflows - Systems without ls_since.sh installed - Complex filtering combining multiple file attributes - Batch operations on large file sets\nWhen to use ls_since.sh: - Interactive exploration and discovery - One-off searches without remembering syntax - Research projects requiring date range audits - Workflows with multiple file type categories"
  },
  {
    "objectID": "posts/lssinceutility/analysis/paper/index.html#creating-a-reusable-function",
    "href": "posts/lssinceutility/analysis/paper/index.html#creating-a-reusable-function",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "15.6 Creating a Reusable Function",
    "text": "15.6 Creating a Reusable Function\nAdd this to your .zshrc or .bashrc for a convenient wrapper:\nfind_rmd_range() {\n  local start_date=\"${1:-2025-11-01}\"\n  local end_date=\"${2:-2025-12-01}\"\n  local directory=\"${3:-.}\"\n\n  find \"$directory\" -type f -name \"*.Rmd\" \\\n    -newermt \"$start_date\" ! -newermt \"$end_date\" \\\n    -printf '%T@ %TY-%Tm-%Td %TH:%TM:%.2TS %p\\n' | \\\n    sort -rn | \\\n    cut -d' ' -f4- | \\\n    fzf --preview 'head -20 {}' \\\n        --header \"Rmd files: $start_date to $end_date\"\n}\n\n# Usage examples:\n# find_rmd_range \"2025-11-01\" \"2025-11-30\" ~/analysis\n# find_rmd_range \"2025-10-01\" \"2025-12-31\"              # Current directory\n# find_rmd_range                                        # Uses defaults"
  },
  {
    "objectID": "posts/lssinceutility/analysis/paper/index.html#key-takeaway",
    "href": "posts/lssinceutility/analysis/paper/index.html#key-takeaway",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "15.7 Key Takeaway",
    "text": "15.7 Key Takeaway\nThe find command demonstrates that date-based file filtering doesn’t require specialized tools—standard Unix utilities can accomplish the same results. The choice between find and ls_since.sh depends on your workflow: use find for scripting and edge cases, and ls_since.sh for interactive exploration and human-friendly defaults."
  },
  {
    "objectID": "posts/lssinceutility/analysis/paper/index.html#about-this-post",
    "href": "posts/lssinceutility/analysis/paper/index.html#about-this-post",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "15.8 About This Post",
    "text": "15.8 About This Post\nThis blog post was generated from a comprehensive white paper documenting the ls_since.sh utility. The white paper provides deeper technical details, implementation patterns, and advanced use cases beyond what’s covered here.\nFor the complete reference documentation, see the white paper at /Users/zenn/Dropbox/bin/date_filtering.md.\nDate published: December 2, 2025 Last updated: December 2, 2025"
  },
  {
    "objectID": "posts/githubarchive/index.html",
    "href": "posts/githubarchive/index.html",
    "title": "Archiving 400 GitHub Repos Locally: A Complete Backup Strategy",
    "section": "",
    "text": "Archiving your GitHub repositories safely\nA pragmatic approach to backing up hundreds of repositories with confidence"
  },
  {
    "objectID": "posts/githubarchive/index.html#problem-statement",
    "href": "posts/githubarchive/index.html#problem-statement",
    "title": "Archiving 400 GitHub Repos Locally: A Complete Backup Strategy",
    "section": "1.1 Problem Statement",
    "text": "1.1 Problem Statement\nIf you have many GitHub private repos, you might want to: - Free up GitHub’s storage or account space - Archive old projects for compliance or reference - Consolidate to a simpler repo structure - Create an offline backup of your work\nBut the manual process is tedious and risky: - ❌ Cloning 400 repos one-by-one - ❌ Manually selecting which ones to delete - ❌ No verification that backups are valid - ❌ Accidentally deleting the wrong repos\nWe need a solution that: - ✓ Backs up everything automatically - ✓ Verifies backups are complete - ✓ Lets you preview before any deletion - ✓ Allows selective preservation of important repos - ✓ Has a dry-run mode for safety"
  },
  {
    "objectID": "posts/githubarchive/index.html#core-features",
    "href": "posts/githubarchive/index.html#core-features",
    "title": "Archiving 400 GitHub Repos Locally: A Complete Backup Strategy",
    "section": "2.1 Core Features",
    "text": "2.1 Core Features\nThe script automates a three-phase process:\n\n2.1.1 Phase 1: Backup Everything\nFor each repo, it creates:\n\nFull git mirror - Complete history, all branches and tags\nPortable bundle - Single-file git archive for easy transfer\nWiki content - If the repo has a wiki\nMetadata exports - Issues, PRs, releases, labels, milestones, workflows\nRelease assets - Downloaded binaries and artifacts\n\nAll stored in a organized directory structure per repo.\n\n\n2.1.2 Phase 2: Verify Backups\nBefore any deletion, the script:\n\nChecks every bundle with git bundle verify\nEnsures complete data transfer\nAborts if any backup fails\nPrevents accidental deletion of incomplete backups\n\n\n\n2.1.3 Phase 3: Selective Deletion\nOnly delete repos marked for deletion:\n\nExclude important repos - Keep active projects on GitHub\nInteractive confirmation - User must explicitly confirm\nTyped verification - Requires typing ‘DELETE’ to proceed\nClear preview - Shows exactly what will be deleted"
  },
  {
    "objectID": "posts/githubarchive/index.html#using-the-script",
    "href": "posts/githubarchive/index.html#using-the-script",
    "title": "Archiving 400 GitHub Repos Locally: A Complete Backup Strategy",
    "section": "3.1 Using the Script",
    "text": "3.1 Using the Script\n\n3.1.1 Basic Usage\n# Preview what would happen (no changes)\n./github-archive.sh --dry-run\n\n# Run for real (backs up all, asks before deleting)\n./github-archive.sh\n\n# Specify custom owner or directory\n./github-archive.sh --owner myorg --dir /external/drive/backup\n\n\n3.1.2 Configuration\nEdit the KEEP_ON_GITHUB array:\nKEEP_ON_GITHUB=(\n    \"important-project\"\n    \"active-work\"\n    \"shared-with-team\"\n)\n\n\n3.1.3 Dry-Run Example Output\n============================================\n   DRY-RUN MODE - No changes will be made\n============================================\n\n=== REPO CATEGORIZATION ===\n\nWill be DELETED from GitHub after backup (397):\n  ✗ old-project-1\n  ✗ old-project-2\n  ...\n\nWill be KEPT on GitHub (3):\n  ✓ important-project\n  ✓ active-work\n  ✓ shared-with-team"
  },
  {
    "objectID": "posts/githubarchive/index.html#what-gets-backed-up",
    "href": "posts/githubarchive/index.html#what-gets-backed-up",
    "title": "Archiving 400 GitHub Repos Locally: A Complete Backup Strategy",
    "section": "4.1 What Gets Backed Up",
    "text": "4.1 What Gets Backed Up\n\n\n\nContent\nFormat\nUse Case\n\n\n\n\nGit history\nrepo.git/ + repo.bundle\nFull reproducibility\n\n\nWiki\nwiki.git/\nDocumentation\n\n\nIssues\nissues.json\nDiscussion archive\n\n\nPull requests\npull-requests.json\nCode review history\n\n\nReleases\nreleases.json\nVersion history\n\n\nRelease assets\nrelease-assets/\nBinaries, artifacts\n\n\nMetadata\nrepo-info.json\nRepository config\n\n\nLabels\nlabels.json\nIssue classification\n\n\nMilestones\nmilestones.json\nProject tracking"
  },
  {
    "objectID": "posts/githubarchive/index.html#before-you-run",
    "href": "posts/githubarchive/index.html#before-you-run",
    "title": "Archiving 400 GitHub Repos Locally: A Complete Backup Strategy",
    "section": "6.1 Before You Run",
    "text": "6.1 Before You Run\n\nTest with dry-run first\n./github-archive.sh --dry-run\nVerify backup directory has space\n\n400 repos × 50-100MB average = 20-40GB\nConsider external storage\n\nEnsure GitHub CLI is authenticated\ngh auth status\nBack up the backups\ntar -czf github-backup-$(date +%Y%m%d).tar.gz ~/github-archive"
  },
  {
    "objectID": "posts/githubarchive/index.html#during-execution",
    "href": "posts/githubarchive/index.html#during-execution",
    "title": "Archiving 400 GitHub Repos Locally: A Complete Backup Strategy",
    "section": "6.2 During Execution",
    "text": "6.2 During Execution\n\nScript verifies before deletion - No backup = no delete\nRequires typed confirmation - Prevents accidental execution\nDetailed logging - Check archive_YYYYMMDD.log if issues occur\nRepos in KEEP_ON_GITHUB are preserved - Still backed up but not deleted"
  },
  {
    "objectID": "posts/githubarchive/index.html#organize-your-repos",
    "href": "posts/githubarchive/index.html#organize-your-repos",
    "title": "Archiving 400 GitHub Repos Locally: A Complete Backup Strategy",
    "section": "7.1 Organize Your Repos",
    "text": "7.1 Organize Your Repos\nDecide what to keep:\nKEEP_ON_GITHUB=(\n    \"active-projects\"           # Currently maintained\n    \"shared-with-team\"          # Collaboration repos\n    \"client-work\"               # Client projects\n    \"portfolio\"                 # Showcase projects\n)"
  },
  {
    "objectID": "posts/githubarchive/index.html#use-external-storage",
    "href": "posts/githubarchive/index.html#use-external-storage",
    "title": "Archiving 400 GitHub Repos Locally: A Complete Backup Strategy",
    "section": "7.2 Use External Storage",
    "text": "7.2 Use External Storage\n# Back up to external drive\n./github-archive.sh --dir /Volumes/External/github-backups\n\n# Create compressed archive\ntar -czf github-backup-2025.tar.gz /Volumes/External/github-backups"
  },
  {
    "objectID": "posts/githubarchive/index.html#automate-periodically",
    "href": "posts/githubarchive/index.html#automate-periodically",
    "title": "Archiving 400 GitHub Repos Locally: A Complete Backup Strategy",
    "section": "7.3 Automate Periodically",
    "text": "7.3 Automate Periodically\n# Add to crontab (monthly backup)\n0 0 1 * * /path/to/github-archive.sh &gt;&gt; $HOME/.logs/github-archive.log 2&gt;&1"
  },
  {
    "objectID": "posts/githubarchive/index.html#github-cli-authentication-fails",
    "href": "posts/githubarchive/index.html#github-cli-authentication-fails",
    "title": "Archiving 400 GitHub Repos Locally: A Complete Backup Strategy",
    "section": "8.1 GitHub CLI authentication fails",
    "text": "8.1 GitHub CLI authentication fails\n# Check authentication status\ngh auth status\n\n# Re-authenticate\ngh auth logout\ngh auth login"
  },
  {
    "objectID": "posts/githubarchive/index.html#bundle-verification-fails",
    "href": "posts/githubarchive/index.html#bundle-verification-fails",
    "title": "Archiving 400 GitHub Repos Locally: A Complete Backup Strategy",
    "section": "8.2 Bundle verification fails",
    "text": "8.2 Bundle verification fails\nCheck network connection and available disk space, then rerun:\n# Resume backing up specific repo\n./github-archive.sh --owner myuser --dry-run"
  },
  {
    "objectID": "posts/githubarchive/index.html#permissions-issues",
    "href": "posts/githubarchive/index.html#permissions-issues",
    "title": "Archiving 400 GitHub Repos Locally: A Complete Backup Strategy",
    "section": "8.3 Permissions issues",
    "text": "8.3 Permissions issues\nEnsure you have: - Read access to all repos - Delete permissions for repos you want to remove\n# Check token permissions\ngh api user"
  },
  {
    "objectID": "misc/index.html",
    "href": "misc/index.html",
    "title": "Miscellaneous",
    "section": "",
    "text": "A collection of command-line tools, scripts, and utilities developed for research computing and productivity."
  },
  {
    "objectID": "misc/index.html#tools-utilities",
    "href": "misc/index.html#tools-utilities",
    "title": "Miscellaneous",
    "section": "",
    "text": "A collection of command-line tools, scripts, and utilities developed for research computing and productivity."
  },
  {
    "objectID": "misc/index.html#explorations-experiments",
    "href": "misc/index.html#explorations-experiments",
    "title": "Miscellaneous",
    "section": "2 Explorations & Experiments",
    "text": "2 Explorations & Experiments\nTechnical explorations, proof-of-concept projects, and experimental work in progress."
  },
  {
    "objectID": "misc/index.html#resources-references",
    "href": "misc/index.html#resources-references",
    "title": "Miscellaneous",
    "section": "3 Resources & References",
    "text": "3 Resources & References\nCurated lists of useful resources, references, and recommended materials across various domains."
  },
  {
    "objectID": "misc/index.html#projects-archive",
    "href": "misc/index.html#projects-archive",
    "title": "Miscellaneous",
    "section": "4 Projects Archive",
    "text": "4 Projects Archive\nOlder projects, historical work, and previous explorations maintained for reference and documentation purposes.\n\nThis section contains miscellaneous projects, tools, experiments, and resources that don’t fit into the primary blog, research, or teaching categories."
  },
  {
    "objectID": "guides/index.html",
    "href": "guides/index.html",
    "title": "Guides",
    "section": "",
    "text": "Practical guides for solving specific problems and accomplishing particular tasks. These step-by-step instructions help you tackle real-world challenges.\nEach guide provides: - Clear problem definition - Prerequisites and assumptions - Detailed implementation steps - Alternative approaches - Common pitfalls and solutions\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nFixing Common R Errors: A Troubleshooting Guide\n\n\nStep-by-step solutions for frequent R programming problems\n\n\n\nR\n\ntroubleshooting\n\ndebugging\n\nguide\n\n\n\nPractical solutions for the most common R errors encountered by data scientists and analysts.\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/research-management/index.html",
    "href": "blog/research-management/index.html",
    "title": "Mac Workflow for Tracking Daily Research Progress",
    "section": "",
    "text": "quarto"
  },
  {
    "objectID": "blog/research-management/index.html#step-3.1-initialize-a-chatgpt-dictation-prompt-by",
    "href": "blog/research-management/index.html#step-3.1-initialize-a-chatgpt-dictation-prompt-by",
    "title": "Mac Workflow for Tracking Daily Research Progress",
    "section": "4.1 Step 3.1: Initialize a chatGPT dictation prompt by",
    "text": "4.1 Step 3.1: Initialize a chatGPT dictation prompt by\nrunning this bash script to copy a prelude to the chatGPT prompt to your clipboard: Call it dp (dictation prompt).\n#!/bin/bash\n\n# Get current date and time\ncurrent_time=$(date +\"%Y-%m-%d %H:%M:%S\")\n\n# Get the current directory name\ncurrent_dir=$(basename \"$PWD\")\n\n# Define the prompt with explicit instructions\nprompt=\"I'm an academic biostatistician. I'm working on a data analysis project.\nI'm about to dictate daily research progress notes.  \nWhen I'm done, provide a concise summary that includes:  \n\n1. The date  and time of dictation ($current_time).  The line with date and time\nshould be the second line of the summary. The first line should be blank. The\ndate and time line shound be enclosed in a box of ascii characters to set it apart.\n2. The name of the current research project directory ($current_dir).  \n3. Each line of the summary including the blank line and the date and time line\nand enclosing box lines should begin with \\\"$current_dir:\\\" so that it can be\nextracted using ripgrep.  \n\nThe notes start here: \"\n\n# Copy the prompt to clipboard (MacOS pbcopy)\necho -n \"$prompt\" | pbcopy\n\n# Notify the user\necho \"Prompt copied to clipboard. Paste it into ChatGPT when ready.\"\n\n\n\n\n\nworkflow"
  },
  {
    "objectID": "blog/research-management/index.html#step-3.2-dictating-notes",
    "href": "blog/research-management/index.html#step-3.2-dictating-notes",
    "title": "Mac Workflow for Tracking Daily Research Progress",
    "section": "4.2 Step 3.2: Dictating Notes",
    "text": "4.2 Step 3.2: Dictating Notes\n\nOpen ChatGPT (done automatically by “dp” script) and follow these steps:\ncopy text from clipboard into the prompt box.\nsubmit prompt to prep chatGPT for summarization.\nClick chatGPT microphone and Dictate your research notes.\nWhen finished dictating submit prompt to ChatGPT for summarization.\nCopy and generated summary onto the clipboard.\n\nUse the following script to append the summary to your daily log: and push the changes to daily_log.md to the remote repository on GitHub.\n#!/bin/bash\n\n# Get the current directory name\ncurrent_dir=$(basename \"$PWD\")\n\n# Get the current date and time\ncurrent_time=$(date +\"%Y-%m-%d %H:%M:%S\")\n\n# Get the clipboard content (MacOS pbpaste)\nclipboard_content=$(pbpaste)\n\n# Echo the output\n#\necho \"$clipboard_content\" &gt;&gt; ~/prj/research_update/daily_log.md\necho \"\" &gt;&gt; ~/prj/research_update/daily_log.md\n\n# Confirm success\necho \"Update for $current_dir appended to daily_log.md in ~/prj/research_update\"\ncd ~/prj/research_update\n  git add .\n    git commit -a -m \"Daily log update $(date +'%Y-%m-%d')\"\n    git push"
  },
  {
    "objectID": "blog/research-management/index.html#prerequisites",
    "href": "blog/research-management/index.html#prerequisites",
    "title": "Mac Workflow for Tracking Daily Research Progress",
    "section": "7.1 Prerequisites",
    "text": "7.1 Prerequisites\nIn development"
  },
  {
    "objectID": "blog/research-management/index.html#step-by-step-implementation",
    "href": "blog/research-management/index.html#step-by-step-implementation",
    "title": "Mac Workflow for Tracking Daily Research Progress",
    "section": "7.2 Step-by-Step Implementation",
    "text": "7.2 Step-by-Step Implementation\nIn development"
  },
  {
    "objectID": "blog/research-management/index.html#key-takeaways",
    "href": "blog/research-management/index.html#key-takeaways",
    "title": "Mac Workflow for Tracking Daily Research Progress",
    "section": "7.3 Key Takeaways",
    "text": "7.3 Key Takeaways\nIn development"
  },
  {
    "objectID": "blog/research-management/index.html#further-reading",
    "href": "blog/research-management/index.html#further-reading",
    "title": "Mac Workflow for Tracking Daily Research Progress",
    "section": "7.4 Further Reading",
    "text": "7.4 Further Reading\nIn development"
  },
  {
    "objectID": "blog/coding-with-genai/index.html",
    "href": "blog/coding-with-genai/index.html",
    "title": "Coding with Generative AI",
    "section": "",
    "text": "View this post in multiple formats:\n\n\n\n  HTML    PDF    Word"
  },
  {
    "objectID": "blog/coding-with-genai/index.html#prerequisites",
    "href": "blog/coding-with-genai/index.html#prerequisites",
    "title": "Coding with Generative AI",
    "section": "1.1 Prerequisites",
    "text": "1.1 Prerequisites\nIn development"
  },
  {
    "objectID": "blog/coding-with-genai/index.html#step-by-step-implementation",
    "href": "blog/coding-with-genai/index.html#step-by-step-implementation",
    "title": "Coding with Generative AI",
    "section": "1.2 Step-by-Step Implementation",
    "text": "1.2 Step-by-Step Implementation\nIn development"
  },
  {
    "objectID": "blog/coding-with-genai/index.html#key-takeaways",
    "href": "blog/coding-with-genai/index.html#key-takeaways",
    "title": "Coding with Generative AI",
    "section": "1.3 Key Takeaways",
    "text": "1.3 Key Takeaways\nIn development"
  },
  {
    "objectID": "blog/coding-with-genai/index.html#further-reading",
    "href": "blog/coding-with-genai/index.html#further-reading",
    "title": "Coding with Generative AI",
    "section": "1.4 Further Reading",
    "text": "1.4 Further Reading\nIn development"
  },
  {
    "objectID": "blog/2025-01-01-year-ahead.html",
    "href": "blog/2025-01-01-year-ahead.html",
    "title": "Looking Ahead: 2025 Plans for R and Data Science",
    "section": "",
    "text": "As we start 2025, I’m excited to share some thoughts on where I’m heading with R programming, data science tools, and research computing workflows."
  },
  {
    "objectID": "blog/2025-01-01-year-ahead.html#this-years-focus-areas",
    "href": "blog/2025-01-01-year-ahead.html#this-years-focus-areas",
    "title": "Looking Ahead: 2025 Plans for R and Data Science",
    "section": "1 This Year’s Focus Areas",
    "text": "1 This Year’s Focus Areas\nPackage Development: Planning to release two new R packages focusing on statistical visualization and research workflow automation.\nDocker Integration: Expanding my containerization work to include more complex multi-service setups for data science teams.\nAI-Assisted Coding: Exploring how LLMs can enhance R development workflows without replacing fundamental programming skills."
  },
  {
    "objectID": "blog/2025-01-01-year-ahead.html#whats-coming-to-the-blog",
    "href": "blog/2025-01-01-year-ahead.html#whats-coming-to-the-blog",
    "title": "Looking Ahead: 2025 Plans for R and Data Science",
    "section": "2 What’s Coming to the Blog",
    "text": "2 What’s Coming to the Blog\nYou’ll see more content in our new structure: - Tutorials on advanced R topics - References for quick command lookups\n- Guides for solving specific problems - Blog posts like this for timely thoughts and updates\nLooking forward to sharing the journey!"
  },
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "About Thomas Lab",
    "section": "",
    "text": "Twitter\n  \n  \n    \n     GitHub\n  \n  \n    \n     Email\n  \n\n  \n  \nThe Thomas Lab in the Herbert Wertheim School of Public Health and Human Longevity Science at UC San Diego focuses on developing data science methodology and educational materials. Our work spans statistical computing, reproducible research practices, and modern tools for data analysis."
  },
  {
    "objectID": "about/index.html#research-focus",
    "href": "about/index.html#research-focus",
    "title": "About Thomas Lab",
    "section": "1 Research Focus",
    "text": "1 Research Focus\nOur lab specializes in:\n\nStatistical methodologies for health research\nR package development for specialized analysis needs\nReproducible research workflows and best practices\nEducational materials for data science skills in public health\nApplications of machine learning in longitudinal studies"
  },
  {
    "objectID": "about/index.html#tools-expertise",
    "href": "about/index.html#tools-expertise",
    "title": "About Thomas Lab",
    "section": "2 Tools & Expertise",
    "text": "2 Tools & Expertise\n\n\n2.1 R Programming\n\nPackage development\nStatistical modeling\nData visualization\nReproducible reporting\n\n\n\n2.2 Research Computing\n\nDocker containerization\nCloud-based computing\nHigh-performance computing\nCollaborative workflows\n\n\n\n2.3 Education\n\nWorkshop development\nTutorial creation\nOpen educational resources\nMentoring and guidance"
  },
  {
    "objectID": "about/index.html#team",
    "href": "about/index.html#team",
    "title": "About Thomas Lab",
    "section": "3 Team",
    "text": "3 Team\nOur interdisciplinary team brings together expertise in statistics, computer science, and public health research to address complex challenges in health data analysis."
  },
  {
    "objectID": "about/index.html#collaborations",
    "href": "about/index.html#collaborations",
    "title": "About Thomas Lab",
    "section": "4 Collaborations",
    "text": "4 Collaborations\nWe actively collaborate with researchers across disciplines to apply novel methodological approaches to real-world health and longevity challenges. If you’re interested in working together, please get in touch!"
  },
  {
    "objectID": "about/index.html#publications",
    "href": "about/index.html#publications",
    "title": "About Thomas Lab",
    "section": "5 Publications",
    "text": "5 Publications\nSelected recent publications:\n\nAuthor A, Author B, Thomas RG (2024). Title of paper. Journal Name, Volume(Issue), pages.\nAuthor C, Author D, Thomas RG (2023). Title of paper. Journal Name, Volume(Issue), pages.\nAuthor E, Author F, Thomas RG (2023). Title of paper. Journal Name, Volume(Issue), pages."
  },
  {
    "objectID": "about/index.html#contact",
    "href": "about/index.html#contact",
    "title": "About Thomas Lab",
    "section": "6 Contact",
    "text": "6 Contact\nFor inquiries about collaboration, research opportunities, or educational resources, please reach out through the social media links above or email us directly."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Welcome to my blog! Here you’ll find my latest thoughts on R programming, data science, statistical computing, and research workflows.\nUse the search box above to find specific topics, or click on category tags to filter posts by theme.\n\n\n\n\n\n\n    \n      \n      \n    \n\n\n\n\n\nMaking optimal use of ChatGPT and other chatbots for data science\n\n\n\n\n\n\n\n\nDec 8, 2025\n\n\n\n\n\n\n\ncoding with genAI\n\n\n\n\n\n\n\n\nDec 8, 2025\n\n\n\n\n\n\n\nSetting up a vim environment for R data science code development\n\n\n\nVim\n\n\n\nThis is another in a series of posts offering suggested strategies for setting up key tools for data science code development\n\n\n\n\n\nDec 8, 2025\n\n\n\n\n\n\n\nSet Names to Lowercase for Multiple Dataframes in R\n\n\n\nData Analysis & Visualization\n\n\n\n\n\n\n\n\n\nDec 8, 2025\n\n\n\n\n\n\n\nSetting up Git for Solo Data Science Workflow\n\n\n\nDevelopment-Environment\n\nProgramming-Development\n\n\n\nA comprehensive guide to configuring Git for individual data science projects and research workflows\n\n\n\n\n\nDec 8, 2025\n\n\n\n\n\n\n\nSetting up a neovim environment for data science code development\n\n\n\nNeovim\n\nVim\n\n\n\nThis is the first in a series of posts offering suggested strategies for setting up key tools for data science code development\n\n\n\n\n\nDec 8, 2025\n\n\n\n\n\n\n\nSetting Up a Minimal Quarto Blog: A Learning Journey\n\n\n\nR Programming\n\nQuarto\n\nTutorial\n\nBlogging\n\n\n\nI didn’t really know how simple it could be to set up a Quarto blog until I actually tried it. Here’s what I learned about the minimal setup needed to start blogging.\n\n\n\n\n\nDec 8, 2025\n\n\n\n\n\n\n\nConfigure the tiling window manager yabai for macos\n\n\n\n\n\n\n\n\nDec 8, 2025\n\n\n\n\n\n\n\nA simple shiny app to explore Palmer Penguin data using ChatGPT to prototype.\n\n\n\n\n\n\n\n\nDec 8, 2025\n\n\n\n\n\n\n\nConverting R data.frames to pdf for better placement control in latex draft: true pdf report\n\n\n\n\n\n\n\n\nDec 8, 2025\n\n\n\n\n\n\n\nTaking Control of Your Clinical Trial: Running ZZedc Independently\n\n\n\nClinical Research\n\nEDC\n\nAWS\n\nOpen Source\n\nResearch Tools\n\nData Management\n\n\n\nLearn how to deploy and manage ZZedc, an open-source EDC system designed for investigator independence. No vendor lock-in, no ongoing licensing costs—just secure, compliant data management under your control.\n\n\n\n\n\nDec 7, 2025\n\n\n\n\n\n\n\nArchiving 400 GitHub Repos Locally: A Complete Backup Strategy\n\n\n\nDevOps\n\nGit\n\nGitHub\n\nAutomation\n\nBash\n\n\n\nA comprehensive guide to creating verified backups of all your GitHub private repositories, with selective deletion, dry-run mode, and complete metadata export.\n\n\n\n\n\nDec 2, 2025\n\n\n\n\n\n\n\nls_since.sh: Advanced File Date Filtering for Research Computing\n\n\n\nBash\n\nCommand-Line\n\nUtilities\n\nFile Management\n\nResearch Tools\n\n\n\nDiscover how ls_since.sh simplifies date-based file filtering with interactive modes, flexible timestamp selection, and seamless fzf integration—a modern solution for research computing workflows.\n\n\n\n\n\nDec 2, 2025\n\n\n\n\n\n\n\nFrom Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow\n\n\n\nQuarto\n\nZZCOLLAB\n\nBlog Development\n\nDocumentation\n\nWorkflow\n\n\n\nLearn how to convert standalone markdown documentation into a fully-featured, reproducible blog post using ZZCOLLAB and Quarto. Includes the complete workflow, decision-making process, and a detailed checklist.\n\n\n\n\n\nDec 2, 2025\n\n\n\n\n\n\n\nCombining Observable JS and Shiny in One Quarto Document\n\n\n\nQuarto\n\nObservable JS\n\nShiny\n\nInteractive Visualization\n\nR Programming\n\n\n\nI didn’t really know much about combining Observable JS and Shiny until I tried to create a side-by-side comparison. What should have taken 30 minutes took several hours of debugging. Here’s what I learned.\n\n\n\n\n\nDec 1, 2025\n\n\n\n\n\n\n\nCreating a GitHub Dotfiles Repository for Configuration Management\n\n\n\nGitHub\n\nDotfiles\n\nDevOps\n\nConfiguration Management\n\nVersion Control\n\n\n\nLearn how to create a comprehensive GitHub dotfiles repository to efficiently share configuration files, automate development environment setup, and maintain consistency across multiple machines and team members.\n\n\n\n\n\nJul 28, 2025\n\n\n\n\n\n\n\nConfigure the Command Line for Data Science Development\n\n\n\nDevelopment Tools\n\nCommand Line\n\nData Science\n\nTerminal Setup\n\n\n\nA comprehensive guide to configuring Kitty terminal and Zsh shell with plugins, aliases, and customizations for data science development on macOS and Linux\n\n\n\n\n\nJul 24, 2025\n\n\n\n\n\n\n\nPalmer Penguins Data Analysis Series (Part 5): Random Forest vs Linear Models - The Final Comparison\n\n\n\nR Programming\n\nData Science\n\nStatistical Computing\n\nMachine Learning\n\nRandom Forest\n\nModel Comparison\n\nPalmer Penguins\n\n\n\nThe final part of our 5-part series comparing linear models with random forests and providing guidance for model selection in ecological research\n\n\n\n\n\nJan 5, 2025\n\n\n\n\n\n\n\nPalmer Penguins Data Analysis Series (Part 4): Model Diagnostics and Interpretation\n\n\n\nR Programming\n\nData Science\n\nStatistical Computing\n\nModel Diagnostics\n\nRegression Analysis\n\nPalmer Penguins\n\n\n\nPart 4 of our 5-part series focusing on rigorous diagnostic procedures, assumption checking, and biological interpretation of our penguin models\n\n\n\n\n\nJan 4, 2025\n\n\n\n\n\n\n\nPalmer Penguins Data Analysis Series (Part 3): Advanced Models and Cross-Validation\n\n\n\nR Programming\n\nData Science\n\nStatistical Computing\n\nCross-Validation\n\nMachine Learning\n\nPalmer Penguins\n\n\n\nPart 3 of our 5-part series where we rigorously validate our models and introduce polynomial features and random forest competitors\n\n\n\n\n\nJan 3, 2025\n\n\n\n\n\n\n\nPalmer Penguins Data Analysis Series (Part 2): Multiple Regression and Species Effects\n\n\n\nR Programming\n\nData Science\n\nStatistical Computing\n\nMultiple Regression\n\nPalmer Penguins\n\n\n\nPart 2 of our 5-part series exploring how multiple predictors and species information dramatically improve penguin body mass predictions\n\n\n\n\n\nJan 2, 2025\n\n\n\n\n\n\n\nPalmer Penguins Data Analysis Series (Part 1): Exploratory Data Analysis and Simple Regression\n\n\n\nR Programming\n\nData Science\n\nStatistical Computing\n\nExploratory Data Analysis\n\nPalmer Penguins\n\n\n\nPart 1 of a comprehensive 5-part series exploring Palmer penguin morphometrics through exploratory data analysis and simple regression modeling\n\n\n\n\n\nJan 1, 2025\n\n\n\n\n\n\n\nPalmer Penguins Data Analysis Series (Part 1): Exploratory Data Analysis and Simple Regression\n\n\n\nR Programming\n\nData Science\n\nStatistical Computing\n\nExploratory Data Analysis\n\nPalmer Penguins\n\n\n\nPart 1 of a comprehensive 5-part series exploring Palmer penguin morphometrics through exploratory data analysis and simple regression modeling\n\n\n\n\n\nJan 1, 2025\n\n\n\n\n\n\n\nYour Engaging Title Here: A Learning Journey\n\n\n\nR Programming\n\nData Science\n\nStatistical Computing\n\n\n\nI didn’t really know much about [topic] until I tried to [implement/understand] it myself. Here’s what I learned along the way.\n\n\n\n\n\nJan 1, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "guides/fixing-common-r-errors.html",
    "href": "guides/fixing-common-r-errors.html",
    "title": "Fixing Common R Errors: A Troubleshooting Guide",
    "section": "",
    "text": "Problem: R can’t find the variable or function you’re trying to use.\nCommon Causes: - Typo in variable name (R is case-sensitive) - Variable not created yet - Variable created in different environment\nSolutions:\n\nCheck spelling and case:\n# Wrong\nmyData &lt;- data.frame(x = 1:5)\nprint(mydata)  # Error: object 'mydata' not found\n\n# Correct\nprint(myData)\nList current objects:\nls()  # See what objects exist\nCheck if package is loaded:\n# If using dplyr functions\nlibrary(dplyr)"
  },
  {
    "objectID": "guides/fixing-common-r-errors.html#object-not-found-errors",
    "href": "guides/fixing-common-r-errors.html#object-not-found-errors",
    "title": "Fixing Common R Errors: A Troubleshooting Guide",
    "section": "",
    "text": "Problem: R can’t find the variable or function you’re trying to use.\nCommon Causes: - Typo in variable name (R is case-sensitive) - Variable not created yet - Variable created in different environment\nSolutions:\n\nCheck spelling and case:\n# Wrong\nmyData &lt;- data.frame(x = 1:5)\nprint(mydata)  # Error: object 'mydata' not found\n\n# Correct\nprint(myData)\nList current objects:\nls()  # See what objects exist\nCheck if package is loaded:\n# If using dplyr functions\nlibrary(dplyr)"
  },
  {
    "objectID": "guides/fixing-common-r-errors.html#packagefunction-not-found",
    "href": "guides/fixing-common-r-errors.html#packagefunction-not-found",
    "title": "Fixing Common R Errors: A Troubleshooting Guide",
    "section": "2 Package/Function Not Found",
    "text": "2 Package/Function Not Found\n\n2.1 Error: could not find function \"function_name\"\nProblem: Function doesn’t exist or package isn’t loaded.\nSolutions:\n\nInstall missing package:\ninstall.packages(\"package_name\")\nlibrary(package_name)\nUse package::function notation:\n# Instead of loading entire package\ndplyr::filter(data, condition)\nCheck function spelling:\n# Wrong\nsummery(data)\n\n# Correct\nsummary(data)"
  },
  {
    "objectID": "guides/fixing-common-r-errors.html#data-type-errors",
    "href": "guides/fixing-common-r-errors.html#data-type-errors",
    "title": "Fixing Common R Errors: A Troubleshooting Guide",
    "section": "3 Data Type Errors",
    "text": "3 Data Type Errors\n\n3.1 Error: non-numeric argument to mathematical function\nProblem: Trying to do math on text or factor data.\nSolutions:\n\nCheck data types:\nstr(data)          # See structure\nclass(data$column) # Check specific column\nConvert to numeric:\n# If column should be numeric\ndata$column &lt;- as.numeric(data$column)\n\n# Handle warnings about NAs\ndata$column &lt;- as.numeric(as.character(data$column))\nRemove non-numeric characters:\n# Remove dollar signs, commas, etc.\ndata$price &lt;- as.numeric(gsub(\"[^0-9.]\", \"\", data$price_text))"
  },
  {
    "objectID": "guides/fixing-common-r-errors.html#subsetting-errors",
    "href": "guides/fixing-common-r-errors.html#subsetting-errors",
    "title": "Fixing Common R Errors: A Troubleshooting Guide",
    "section": "4 Subsetting Errors",
    "text": "4 Subsetting Errors\n\n4.1 Error: subscript out of bounds\nProblem: Trying to access row/column that doesn’t exist.\nSolutions:\n\nCheck dimensions:\ndim(data)        # Rows and columns\nnrow(data)       # Number of rows\nncol(data)       # Number of columns\nUse safe subsetting:\n# Instead of data[100, ] which might not exist\nif (nrow(data) &gt;= 100) {\n  result &lt;- data[100, ]\n}\nCheck column names:\nnames(data)      # See actual column names\n\"column_name\" %in% names(data)  # Check if column exists"
  },
  {
    "objectID": "guides/fixing-common-r-errors.html#missing-values-issues",
    "href": "guides/fixing-common-r-errors.html#missing-values-issues",
    "title": "Fixing Common R Errors: A Troubleshooting Guide",
    "section": "5 Missing Values Issues",
    "text": "5 Missing Values Issues\n\n5.1 Error: missing values in object\nProblem: Functions can’t handle NA values.\nSolutions:\n\nRemove NAs explicitly:\nmean(data$column, na.rm = TRUE)\nsum(data$column, na.rm = TRUE)\nCheck for missing values:\nsum(is.na(data$column))    # Count NAs\ncomplete.cases(data)       # Rows without NAs\nHandle missing data:\n# Remove rows with any NA\nclean_data &lt;- na.omit(data)\n\n# Remove rows with NA in specific column\nclean_data &lt;- data[!is.na(data$column), ]"
  },
  {
    "objectID": "guides/fixing-common-r-errors.html#file-reading-errors",
    "href": "guides/fixing-common-r-errors.html#file-reading-errors",
    "title": "Fixing Common R Errors: A Troubleshooting Guide",
    "section": "6 File Reading Errors",
    "text": "6 File Reading Errors\n\n6.1 Error: cannot open the connection\nProblem: R can’t find or access the file.\nSolutions:\n\nCheck file path:\ngetwd()                    # Current directory\nfile.exists(\"filename.csv\") # Check if file exists\nUse correct path separators:\n# Windows - use forward slashes or double backslashes\ndata &lt;- read.csv(\"C:/Users/name/data.csv\")\n# or\ndata &lt;- read.csv(\"C:\\\\Users\\\\name\\\\data.csv\")\nCheck file permissions:\n# Make sure file isn't open in Excel\n# Check that you have read permissions"
  },
  {
    "objectID": "guides/fixing-common-r-errors.html#memory-issues",
    "href": "guides/fixing-common-r-errors.html#memory-issues",
    "title": "Fixing Common R Errors: A Troubleshooting Guide",
    "section": "7 Memory Issues",
    "text": "7 Memory Issues\n\n7.1 Error: cannot allocate vector of size X\nProblem: Not enough memory for the operation.\nSolutions:\n\nCheck memory usage:\nmemory.size()      # Current usage (Windows)\nobject.size(data)  # Size of specific object\nFree up memory:\nrm(large_object)   # Remove unneeded objects\ngc()               # Force garbage collection\nWork with smaller chunks:\n# Read file in chunks\nlibrary(readr)\ndata &lt;- read_csv_chunked(\"large_file.csv\", \n                        chunk_size = 1000,\n                        callback = DataFrameCallback$new())"
  },
  {
    "objectID": "guides/fixing-common-r-errors.html#package-installation-issues",
    "href": "guides/fixing-common-r-errors.html#package-installation-issues",
    "title": "Fixing Common R Errors: A Troubleshooting Guide",
    "section": "8 Package Installation Issues",
    "text": "8 Package Installation Issues\n\n8.1 Error: package installation failed\nProblem: Package won’t install due to dependencies or system issues.\nSolutions:\n\nUpdate R and packages:\nupdate.packages(ask = FALSE)\nInstall from different repository:\n# Try different CRAN mirror\ninstall.packages(\"package_name\", repos = \"https://cloud.r-project.org\")\n\n# Install from GitHub\ndevtools::install_github(\"user/package\")\nInstall dependencies manually:\n# Install suggested dependencies\ninstall.packages(\"package_name\", dependencies = TRUE)"
  },
  {
    "objectID": "guides/fixing-common-r-errors.html#general-debugging-tips",
    "href": "guides/fixing-common-r-errors.html#general-debugging-tips",
    "title": "Fixing Common R Errors: A Troubleshooting Guide",
    "section": "9 General Debugging Tips",
    "text": "9 General Debugging Tips\n\nUse debugging tools:\ntraceback()        # See where error occurred\ndebug(function)    # Step through function\nBreak down complex operations:\n# Instead of chaining everything\nresult &lt;- data %&gt;% filter(...) %&gt;% mutate(...) %&gt;% summarise(...)\n\n# Do step by step\nstep1 &lt;- filter(data, ...)\nstep2 &lt;- mutate(step1, ...)\nresult &lt;- summarise(step2, ...)\nCheck intermediate results:\n# Print intermediate steps\nprint(dim(data))\nhead(data)\nsummary(data)"
  },
  {
    "objectID": "guides/fixing-common-r-errors.html#prevention-strategies",
    "href": "guides/fixing-common-r-errors.html#prevention-strategies",
    "title": "Fixing Common R Errors: A Troubleshooting Guide",
    "section": "10 Prevention Strategies",
    "text": "10 Prevention Strategies\n\nAlways check data structure after reading files\nUse meaningful variable names to avoid confusion\nComment your code to remember what you were doing\nSave your work frequently in case R crashes\nUse version control (Git) to track changes"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Thomas Lab",
    "section": "",
    "text": "I’m Ronald G. Thomas, a researcher and data scientist focused on statistical methods, reproducible research, and computational tools. I write about R programming, statistical analysis, research workflows, and modern data science practices.\nThis site organizes content into focused areas:\n\nBlog - Technical articles and explorations\nWhite Papers - In-depth technical reports and methodological frameworks\nResearch - Publications and academic work\nTeaching - Courses, workshops, and educational materials\nMisc - Tools, references, and other useful resources\n\n\n\n\nCoding with Generative AI - Best practices for AI-assisted programming\nResearch Management Workflows - Organizing academic projects\nR Package Development - Building robust R packages\n\n\n\n\nGitHub • Twitter • About"
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Thomas Lab",
    "section": "",
    "text": "I’m Ronald G. Thomas, a researcher and data scientist focused on statistical methods, reproducible research, and computational tools. I write about R programming, statistical analysis, research workflows, and modern data science practices.\nThis site organizes content into focused areas:\n\nBlog - Technical articles and explorations\nWhite Papers - In-depth technical reports and methodological frameworks\nResearch - Publications and academic work\nTeaching - Courses, workshops, and educational materials\nMisc - Tools, references, and other useful resources\n\n\n\n\nCoding with Generative AI - Best practices for AI-assisted programming\nResearch Management Workflows - Organizing academic projects\nR Package Development - Building robust R packages\n\n\n\n\nGitHub • Twitter • About"
  },
  {
    "objectID": "posts/configtermzshp11/index.html",
    "href": "posts/configtermzshp11/index.html",
    "title": "Configure the Command Line for Data Science Development",
    "section": "",
    "text": "UCSD Geisel Library - A hub for research and academic discovery\nThe Geisel Library at UC San Diego, where research and innovation converge - a perfect setting for learning advanced terminal configuration techniques that enhance data science productivity"
  },
  {
    "objectID": "posts/configtermzshp11/index.html#function-categories",
    "href": "posts/configtermzshp11/index.html#function-categories",
    "title": "Configure the Command Line for Data Science Development",
    "section": "4.1 Function Categories",
    "text": "4.1 Function Categories\n\nDirectory Management: Efficient navigation and workspace organization\nVersion Control: Streamlined git operations for iterative data analysis\nEnvironment Management: Quick activation of conda/virtual environments\nTool Integration: Seamless launching of Jupyter, R, and computational tools"
  },
  {
    "objectID": "posts/configtermzshp11/index.html#system-dependencies",
    "href": "posts/configtermzshp11/index.html#system-dependencies",
    "title": "Configure the Command Line for Data Science Development",
    "section": "6.1 System Dependencies",
    "text": "6.1 System Dependencies\n\nPackage managers required: Homebrew (macOS) or equivalent Linux package managers\nFont requirements: FiraCode Nerd Font needed for optimal visual experience\nPlugin compatibility: Some plugins may require specific versions of Zsh"
  },
  {
    "objectID": "posts/configtermzshp11/index.html#performance-considerations",
    "href": "posts/configtermzshp11/index.html#performance-considerations",
    "title": "Configure the Command Line for Data Science Development",
    "section": "6.2 Performance Considerations",
    "text": "6.2 Performance Considerations\n\nStartup time: Multiple plugin loading can increase shell initialization time\nMemory usage: Enhanced features consume additional system resources\nCompatibility: Configuration may need adjustment across different operating systems"
  },
  {
    "objectID": "posts/configtermzshp11/index.html#customization-limitations",
    "href": "posts/configtermzshp11/index.html#customization-limitations",
    "title": "Configure the Command Line for Data Science Development",
    "section": "6.3 Customization Limitations",
    "text": "6.3 Customization Limitations\n\nPersonal preferences: Aliases and shortcuts reflect individual workflow patterns\nProject-specific needs: Some configurations may not suit all data science project types\nTeam collaboration: Highly customized environments may not transfer to shared systems"
  },
  {
    "objectID": "posts/configtermzshp11/index.html#technical-documentation",
    "href": "posts/configtermzshp11/index.html#technical-documentation",
    "title": "Configure the Command Line for Data Science Development",
    "section": "9.1 Technical Documentation",
    "text": "9.1 Technical Documentation\n\nTerminal and Shell Configuration:\n\nKitty Terminal Documentation - Comprehensive configuration guide\nZsh Documentation - Official Zsh manual and scripting guide\nOh My Zsh Framework - Popular Zsh configuration framework\n\nProductivity Tools:\n\nFZF Documentation - Command-line fuzzy finder\nRipgrep User Guide - Fast text search tool\nEza Documentation - Modern replacement for ls command"
  },
  {
    "objectID": "posts/configtermzshp11/index.html#blog-posts-and-tutorials",
    "href": "posts/configtermzshp11/index.html#blog-posts-and-tutorials",
    "title": "Configure the Command Line for Data Science Development",
    "section": "9.2 Blog Posts and Tutorials",
    "text": "9.2 Blog Posts and Tutorials\n\nTerminal Setup Guides:\n\nConfiguring Zsh Without Dependencies - Minimal Zsh configuration approach\nMy Terminal Setup: iTerm2 + ZSH + Powerlevel10k - Comprehensive terminal customization\nSettings For a Better iTerm2 Experience - Performance and usability optimization\n\nDevelopment Workflow:\n\niTerm2 Setup and Customization - Detailed configuration walkthrough\nCommand Line Tools for Data Science - Comprehensive CLI data science guide\nTerminal-Based Data Science Workflows - Integration with Jupyter and other tools\n\nAdvanced Configuration:\n\nDotfiles Management Best Practices - Version control for configuration files\nShell Scripting for Data Scientists - Automation and scripting techniques\nCross-Platform Terminal Configuration - Example configurations"
  },
  {
    "objectID": "posts/configtermzshp11/index.html#community-resources",
    "href": "posts/configtermzshp11/index.html#community-resources",
    "title": "Configure the Command Line for Data Science Development",
    "section": "9.3 Community Resources",
    "text": "9.3 Community Resources\n\nForums and Discussion:\n\nr/commandline - Terminal tools and configuration discussions\nUnix & Linux Stack Exchange - Technical troubleshooting and tips\nZsh Users Mailing List - Official community support\n\nConfiguration Repositories:\n\nAwesome Dotfiles - Curated list of configuration examples\nDotfiles.org - Community-shared configurations\nGitHub Dotfiles - Version control best practices"
  },
  {
    "objectID": "posts/configtermzshp11/index.html#environment-requirements",
    "href": "posts/configtermzshp11/index.html#environment-requirements",
    "title": "Configure the Command Line for Data Science Development",
    "section": "10.1 Environment Requirements",
    "text": "10.1 Environment Requirements\n\nOperating System: macOS 10.15+ or Linux (Ubuntu 18.04+, Fedora 30+)\nPackage Manager: Homebrew (macOS) or system package manager (Linux)\nDependencies: Git, Zsh 5.0+, basic development tools"
  },
  {
    "objectID": "posts/configtermzshp11/index.html#configuration-files",
    "href": "posts/configtermzshp11/index.html#configuration-files",
    "title": "Configure the Command Line for Data Science Development",
    "section": "10.2 Configuration Files",
    "text": "10.2 Configuration Files\n\nRepository: Configuration files available in the post’s code blocks\nInstallation: Copy configurations to appropriate directories (~/.config/zsh/, ~/.zshrc)\nBackup: Always backup existing configurations before implementing changes"
  },
  {
    "objectID": "posts/configtermzshp11/index.html#version-information",
    "href": "posts/configtermzshp11/index.html#version-information",
    "title": "Configure the Command Line for Data Science Development",
    "section": "10.3 Version Information",
    "text": "10.3 Version Information\n# Check versions of key components\nzsh --version          # Zsh version\nkitty --version        # Kitty terminal version\nbrew --version         # Homebrew version (macOS)"
  },
  {
    "objectID": "posts/configtermzshp11/index.html#connect-and-discuss",
    "href": "posts/configtermzshp11/index.html#connect-and-discuss",
    "title": "Configure the Command Line for Data Science Development",
    "section": "10.4 Connect and Discuss",
    "text": "10.4 Connect and Discuss\nHave questions about terminal configuration or suggestions for improvements?\n\nTwitter: @rgt47 - Quick questions and discussions\nLinkedIn: Ronald Glenn Thomas - Professional networking\nGitHub: rgt47 - Code, issues, and contributions\nEmail: Contact through website - Detailed inquiries"
  },
  {
    "objectID": "posts/configtermzshp11/index.html#about-the-author",
    "href": "posts/configtermzshp11/index.html#about-the-author",
    "title": "Configure the Command Line for Data Science Development",
    "section": "10.5 About the Author",
    "text": "10.5 About the Author\nRonald (Ryy) Glenn Thomas is a biostatistician and data scientist at UC San Diego, specializing in statistical computing, machine learning applications in healthcare, and reproducible research methods. He develops R packages and conducts research at the intersection of statistics, data science, and clinical research.\nConnect: Website | ORCID | Google Scholar"
  },
  {
    "objectID": "posts/configultisnips/pp_index.html",
    "href": "posts/configultisnips/pp_index.html",
    "title": "Palmer Penguins Data Analysis Series (Part 1): Exploratory Data Analysis and Simple Regression",
    "section": "",
    "text": "Curious Adelie penguins beginning their data science journey - because every great analysis starts with getting to know your data!\nPhoto: African penguins at Boulders Beach, South Africa. Licensed under CC BY 2.0 via Wikimedia Commons"
  },
  {
    "objectID": "posts/configultisnips/pp_index.html#species-and-morphometric-overview",
    "href": "posts/configultisnips/pp_index.html#species-and-morphometric-overview",
    "title": "Palmer Penguins Data Analysis Series (Part 1): Exploratory Data Analysis and Simple Regression",
    "section": "4.1 Species and Morphometric Overview",
    "text": "4.1 Species and Morphometric Overview\nLet’s understand our penguin community composition and key measurements:\n\n# Species summary with key statistics\nspecies_summary &lt;- penguins_clean %&gt;%\n  group_by(species) %&gt;%\n  summarise(\n    n = n(),\n    body_mass_mean = round(mean(body_mass_g), 0),\n    flipper_length_mean = round(mean(flipper_length_mm), 1),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(percentage = round(n / sum(n) * 100, 1))\n\nkable(species_summary, \n      caption = \"Species Distribution and Key Morphometrics\",\n      col.names = c(\"Species\", \"N\", \"Body Mass (g)\", \"Flipper Length (mm)\", \"% of Dataset\"))\n\n\nSpecies Distribution and Key Morphometrics\n\n\nSpecies\nN\nBody Mass (g)\nFlipper Length (mm)\n% of Dataset\n\n\n\n\nAdelie\n146\n3706\n190.1\n43.8\n\n\nChinstrap\n68\n3733\n195.8\n20.4\n\n\nGentoo\n119\n5092\n217.2\n35.7\n\n\n\n\n# Combined visualization: species distribution and key relationships\np_species &lt;- ggplot(species_summary, aes(x = species, y = n, fill = species)) +\n  geom_col(alpha = 0.8) +\n  geom_text(aes(label = paste0(n, \"\\n(\", percentage, \"%)\")), \n            vjust = -0.5, size = 3.5) +\n  scale_fill_manual(values = penguin_colors) +\n  labs(title = \"Species Distribution\", x = \"Species\", y = \"Count\") +\n  theme_minimal() + theme(legend.position = \"none\")\n\n# Flipper length vs body mass by species\np_relationship &lt;- ggplot(penguins_clean, aes(x = flipper_length_mm, y = body_mass_g, color = species)) +\n  geom_point(alpha = 0.7, size = 1.5) +\n  geom_smooth(method = \"lm\", se = FALSE, size = 0.8) +\n  scale_color_manual(values = penguin_colors) +\n  labs(title = \"Flipper Length vs Body Mass\", \n       x = \"Flipper Length (mm)\", y = \"Body Mass (g)\", color = \"Species\") +\n  theme_minimal()\n\n# Combine plots\neda_overview &lt;- p_species + p_relationship\nprint(eda_overview)\n\n\n\n\n\n\n\n# Save the plot\nggsave(\"eda-overview.png\", plot = eda_overview, width = 10, height = 5, dpi = 300)\n\n\n\n\nSpecies distribution and morphometric relationship overview showing sample sizes and the key flipper-body mass relationship across species"
  },
  {
    "objectID": "posts/configultisnips/pp_index.html#building-and-interpreting-the-model",
    "href": "posts/configultisnips/pp_index.html#building-and-interpreting-the-model",
    "title": "Palmer Penguins Data Analysis Series (Part 1): Exploratory Data Analysis and Simple Regression",
    "section": "7.1 Building and Interpreting the Model",
    "text": "7.1 Building and Interpreting the Model\n\n# Fit simple linear regression model\nsimple_model &lt;- lm(body_mass_g ~ flipper_length_mm, data = penguins_clean)\n\n# Extract coefficients with confidence intervals\nmodel_coefficients &lt;- tidy(simple_model, conf.int = TRUE)\nmodel_metrics &lt;- glance(simple_model)\n\n# Display key results\ncat(\"📊 Simple Linear Model Results:\\n\")\n\n📊 Simple Linear Model Results:\n\ncat(\"===============================\\n\")\n\n===============================\n\ncat(sprintf(\"R-squared: %.3f (%.1f%% of variance explained)\\n\", \n            model_metrics$r.squared, model_metrics$r.squared * 100))\n\nR-squared: 0.762 (76.2% of variance explained)\n\ncat(sprintf(\"RMSE: %.1f grams\\n\", sigma(simple_model)))\n\nRMSE: 393.3 grams\n\ncat(sprintf(\"F-statistic: %.1f (p &lt; 0.001)\\n\", model_metrics$statistic))\n\nF-statistic: 1060.3 (p &lt; 0.001)\n\n# Model equation with confidence intervals\nintercept &lt;- model_coefficients$estimate[1]\nslope &lt;- model_coefficients$estimate[2]\nslope_ci_lower &lt;- model_coefficients$conf.low[2]\nslope_ci_upper &lt;- model_coefficients$conf.high[2]\n\ncat(\"\\n🧮 Model Equation:\\n\")\n\n\n🧮 Model Equation:\n\ncat(sprintf(\"Body Mass = %.1f + %.1f × Flipper Length\\n\", intercept, slope))\n\nBody Mass = -5872.1 + 50.2 × Flipper Length\n\ncat(sprintf(\"Slope 95%% CI: [%.1f, %.1f] grams/mm\\n\", slope_ci_lower, slope_ci_upper))\n\nSlope 95% CI: [47.1, 53.2] grams/mm\n\n# Generate predictions with confidence intervals\nnew_data &lt;- tibble(flipper_length_mm = c(180, 200, 220))\npredictions &lt;- predict(simple_model, newdata = new_data, interval = \"confidence\")\n\ncat(\"\\n📝 Example Predictions (95% CI):\\n\")\n\n\n📝 Example Predictions (95% CI):\n\nfor(i in 1:nrow(new_data)) {\n  cat(sprintf(\"• %dmm flippers: %.0f g [%.0f, %.0f]\\n\", \n              new_data$flipper_length_mm[i], \n              predictions[i, \"fit\"], \n              predictions[i, \"lwr\"], \n              predictions[i, \"upr\"]))\n}\n\n• 180mm flippers: 3155 g [3079, 3232]\n• 200mm flippers: 4159 g [4116, 4201]\n• 220mm flippers: 5162 g [5090, 5233]\n\n# Visualize model with confidence bands\nmodel_plot &lt;- ggplot(penguins_clean, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(aes(color = species), alpha = 0.6) +\n  geom_smooth(method = \"lm\", color = \"black\", fill = \"gray80\") +\n  scale_color_manual(values = penguin_colors) +\n  labs(title = \"Simple Linear Regression: Body Mass ~ Flipper Length\",\n       subtitle = \"Gray band shows 95% confidence interval\",\n       x = \"Flipper Length (mm)\", y = \"Body Mass (g)\", color = \"Species\") +\n  theme_minimal()\n\nprint(model_plot)\n\n\n\n\n\n\n\nggsave(\"simple-regression-model.png\", plot = model_plot, width = 8, height = 5, dpi = 300)\n\n\n\n\nSimple linear regression model showing body mass predicted by flipper length with 95% confidence interval"
  },
  {
    "objectID": "posts/configultisnips/pp_index.html#statistical-limitations",
    "href": "posts/configultisnips/pp_index.html#statistical-limitations",
    "title": "Palmer Penguins Data Analysis Series (Part 1): Exploratory Data Analysis and Simple Regression",
    "section": "8.1 Statistical Limitations",
    "text": "8.1 Statistical Limitations\n\n# Model diagnostic checks\npenguins_with_predictions &lt;- penguins_clean %&gt;%\n  mutate(\n    predicted = predict(simple_model),\n    residuals = residuals(simple_model),\n    standardized_residuals = rstandard(simple_model)\n  )\n\n# Check for outliers and influential points\noutliers &lt;- which(abs(penguins_with_predictions$standardized_residuals) &gt; 2.5)\ncat(\"⚠️  Model Assumption Checks:\\n\")\n\n⚠️  Model Assumption Checks:\n\ncat(sprintf(\"• Potential outliers: %d observations (&gt;2.5 SD from mean)\\n\", length(outliers)))\n\n• Potential outliers: 5 observations (&gt;2.5 SD from mean)\n\ncat(sprintf(\"• Residual standard error: %.1f grams\\n\", sigma(simple_model)))\n\n• Residual standard error: 393.3 grams\n\n# Residuals diagnostic plot\ndiagnostic_plot &lt;- ggplot(penguins_with_predictions, aes(x = predicted, y = standardized_residuals)) +\n  geom_point(aes(color = species), alpha = 0.6) +\n  geom_hline(yintercept = c(-2, 0, 2), linetype = c(\"dashed\", \"solid\", \"dashed\"), \n             color = c(\"red\", \"black\", \"red\")) +\n  scale_color_manual(values = penguin_colors) +\n  labs(title = \"Model Residuals Diagnostic\",\n       subtitle = \"Species clustering suggests missing predictors\",\n       x = \"Predicted Body Mass (g)\", y = \"Standardized Residuals\", color = \"Species\") +\n  theme_minimal()\n\nprint(diagnostic_plot)\n\n\n\n\n\n\n\nggsave(\"model-diagnostics.png\", plot = diagnostic_plot, width = 8, height = 5, dpi = 300)\n\n\n\n\nModel diagnostic plot showing residuals clustered by species, indicating model limitations"
  },
  {
    "objectID": "posts/configultisnips/pp_index.html#key-limitations",
    "href": "posts/configultisnips/pp_index.html#key-limitations",
    "title": "Palmer Penguins Data Analysis Series (Part 1): Exploratory Data Analysis and Simple Regression",
    "section": "8.2 Key Limitations",
    "text": "8.2 Key Limitations\n\nSimpson’s Paradox Risk: The model ignores species differences, potentially masking important biological relationships\nModel Assumptions:\n\nLinear relationship assumption appears reasonable\nResidual clustering by species indicates missing predictors\nHomoscedasticity assumption may be violated across species\n\nTemporal Generalizability: Data spans 2007-2009; climate change may affect current relationships\nGeographic Scope: Limited to Palmer Station region; may not generalize to other penguin populations\nMeasurement Precision: Morphometric measurements have inherent measurement error not captured in model\nBiological Constraints: Model predictions outside observed flipper length range (172-231mm) should be interpreted cautiously"
  },
  {
    "objectID": "posts/configultisnips/pp_index.html#real-world-applications",
    "href": "posts/configultisnips/pp_index.html#real-world-applications",
    "title": "Palmer Penguins Data Analysis Series (Part 1): Exploratory Data Analysis and Simple Regression",
    "section": "9.1 Real-World Applications",
    "text": "9.1 Real-World Applications\nOur simple regression model has several practical applications in Antarctic research:\n\n# Calculate effect sizes and practical significance\neffect_size &lt;- slope / sd(penguins_clean$body_mass_g)\ncat(\"🌍 Practical Applications:\\n\")\n\n🌍 Practical Applications:\n\ncat(sprintf(\"• Field Assessment: Flipper measurements can estimate body condition (effect size: %.2f)\\n\", effect_size))\n\n• Field Assessment: Flipper measurements can estimate body condition (effect size: 0.06)\n\ncat(sprintf(\"• Population Monitoring: Track penguin health trends using flipper-mass relationships\\n\"))\n\n• Population Monitoring: Track penguin health trends using flipper-mass relationships\n\ncat(sprintf(\"• Climate Research: Changes in morphometric relationships may indicate environmental stress\\n\"))\n\n• Climate Research: Changes in morphometric relationships may indicate environmental stress\n\ncat(sprintf(\"• Conservation Planning: Identify underweight individuals for targeted intervention\\n\"))\n\n• Conservation Planning: Identify underweight individuals for targeted intervention\n\n# Practical thresholds based on model\nlow_threshold &lt;- quantile(penguins_clean$body_mass_g, 0.25)\nhigh_threshold &lt;- quantile(penguins_clean$body_mass_g, 0.75)\n\ncat(\"\\n📊 Clinical Thresholds:\\n\")\n\n\n📊 Clinical Thresholds:\n\ncat(sprintf(\"• Low body condition: &lt;%.0f g (based on 25th percentile)\\n\", low_threshold))\n\n• Low body condition: &lt;3550 g (based on 25th percentile)\n\ncat(sprintf(\"• Normal range: %.0f-%.0f g\\n\", low_threshold, high_threshold))\n\n• Normal range: 3550-4775 g\n\ncat(sprintf(\"• High body condition: &gt;%.0f g (based on 75th percentile)\\n\", high_threshold))\n\n• High body condition: &gt;4775 g (based on 75th percentile)"
  },
  {
    "objectID": "posts/configultisnips/pp_index.html#what-weve-learned-in-part-1",
    "href": "posts/configultisnips/pp_index.html#what-weve-learned-in-part-1",
    "title": "Palmer Penguins Data Analysis Series (Part 1): Exploratory Data Analysis and Simple Regression",
    "section": "10.1 What We’ve Learned in Part 1",
    "text": "10.1 What We’ve Learned in Part 1\n\nStrong Predictive Relationship: Flipper length explains 76% of body mass variance (R² = 0.762), providing a reliable field assessment tool\nSpecies-Specific Patterns: Residual clustering by species suggests important biological differences not captured by flipper length alone\nModel Performance: RMSE of 393g indicates reasonable prediction accuracy for most applications\nResearch Implications: Simple morphometric relationships can support field research and conservation efforts"
  },
  {
    "objectID": "posts/configultisnips/pp_index.html#looking-ahead-to-part-2",
    "href": "posts/configultisnips/pp_index.html#looking-ahead-to-part-2",
    "title": "Palmer Penguins Data Analysis Series (Part 1): Exploratory Data Analysis and Simple Regression",
    "section": "10.2 Looking Ahead to Part 2",
    "text": "10.2 Looking Ahead to Part 2\nOur residual analysis reveals clear opportunities for improvement through:\n\nSpecies Integration: Accounting for biological differences between penguin species\nMultiple Predictors: Incorporating bill measurements for enhanced accuracy\n\nInteraction Effects: Exploring how predictors work together\nModel Validation: Comparing simple vs. complex model performance\n\n\n\n\n\n\n\nTip🎯 Preview: Dramatic Model Improvement\n\n\n\nIn Part 2, adding species information will improve our model’s R² from 0.762 to over 0.860 - demonstrating why biological context matters in ecological modeling!"
  },
  {
    "objectID": "posts/drafts.html",
    "href": "posts/drafts.html",
    "title": "Draft Posts",
    "section": "",
    "text": "NoteNo Draft Posts Currently\n\n\n\nThere are currently no draft posts in development. Posts marked with draft: true in their YAML frontmatter will appear here instead of the main blog listing."
  },
  {
    "objectID": "posts/drafts.html#how-to-create-draft-posts",
    "href": "posts/drafts.html#how-to-create-draft-posts",
    "title": "Draft Posts",
    "section": "1 How to Create Draft Posts",
    "text": "1 How to Create Draft Posts\nPosts marked as drafts are excluded from: - Main blog listing - RSS feeds\n- Site search indexing - External link sharing\nTo create a draft post: 1. Add draft: true to the post’s YAML frontmatter 2. The post will appear on this page instead of the main blog\nTo publish a draft: 1. Remove the draft: true line from the post’s YAML frontmatter 2. The post will appear on the main blog listing\nUsing the draft management script:\n# Mark a post as draft\n./manage_drafts.sh draft posts/my-post/\n\n# Publish a draft\n./manage_drafts.sh publish posts/my-post/"
  },
  {
    "objectID": "posts/githubarchive/analysis/paper/index.html",
    "href": "posts/githubarchive/analysis/paper/index.html",
    "title": "Archiving 400 GitHub Repos Locally: A Complete Backup Strategy",
    "section": "",
    "text": "Archiving your GitHub repositories safely\nA pragmatic approach to backing up hundreds of repositories with confidence"
  },
  {
    "objectID": "posts/githubarchive/analysis/paper/index.html#problem-statement",
    "href": "posts/githubarchive/analysis/paper/index.html#problem-statement",
    "title": "Archiving 400 GitHub Repos Locally: A Complete Backup Strategy",
    "section": "1.1 Problem Statement",
    "text": "1.1 Problem Statement\nIf you have many GitHub private repos, you might want to: - Free up GitHub’s storage or account space - Archive old projects for compliance or reference - Consolidate to a simpler repo structure - Create an offline backup of your work\nBut the manual process is tedious and risky: - ❌ Cloning 400 repos one-by-one - ❌ Manually selecting which ones to delete - ❌ No verification that backups are valid - ❌ Accidentally deleting the wrong repos\nWe need a solution that: - ✓ Backs up everything automatically - ✓ Verifies backups are complete - ✓ Lets you preview before any deletion - ✓ Allows selective preservation of important repos - ✓ Has a dry-run mode for safety"
  },
  {
    "objectID": "posts/githubarchive/analysis/paper/index.html#core-features",
    "href": "posts/githubarchive/analysis/paper/index.html#core-features",
    "title": "Archiving 400 GitHub Repos Locally: A Complete Backup Strategy",
    "section": "2.1 Core Features",
    "text": "2.1 Core Features\nThe script automates a three-phase process:\n\n2.1.1 Phase 1: Backup Everything\nFor each repo, it creates:\n\nFull git mirror - Complete history, all branches and tags\nPortable bundle - Single-file git archive for easy transfer\nWiki content - If the repo has a wiki\nMetadata exports - Issues, PRs, releases, labels, milestones, workflows\nRelease assets - Downloaded binaries and artifacts\n\nAll stored in a organized directory structure per repo.\n\n\n2.1.2 Phase 2: Verify Backups\nBefore any deletion, the script:\n\nChecks every bundle with git bundle verify\nEnsures complete data transfer\nAborts if any backup fails\nPrevents accidental deletion of incomplete backups\n\n\n\n2.1.3 Phase 3: Selective Deletion\nOnly delete repos marked for deletion:\n\nExclude important repos - Keep active projects on GitHub\nInteractive confirmation - User must explicitly confirm\nTyped verification - Requires typing ‘DELETE’ to proceed\nClear preview - Shows exactly what will be deleted"
  },
  {
    "objectID": "posts/githubarchive/analysis/paper/index.html#using-the-script",
    "href": "posts/githubarchive/analysis/paper/index.html#using-the-script",
    "title": "Archiving 400 GitHub Repos Locally: A Complete Backup Strategy",
    "section": "3.1 Using the Script",
    "text": "3.1 Using the Script\n\n3.1.1 Basic Usage\n# Preview what would happen (no changes)\n./github-archive.sh --dry-run\n\n# Run for real (backs up all, asks before deleting)\n./github-archive.sh\n\n# Specify custom owner or directory\n./github-archive.sh --owner myorg --dir /external/drive/backup\n\n\n3.1.2 Configuration\nEdit the KEEP_ON_GITHUB array:\nKEEP_ON_GITHUB=(\n    \"important-project\"\n    \"active-work\"\n    \"shared-with-team\"\n)\n\n\n3.1.3 Dry-Run Example Output\n============================================\n   DRY-RUN MODE - No changes will be made\n============================================\n\n=== REPO CATEGORIZATION ===\n\nWill be DELETED from GitHub after backup (397):\n  ✗ old-project-1\n  ✗ old-project-2\n  ...\n\nWill be KEPT on GitHub (3):\n  ✓ important-project\n  ✓ active-work\n  ✓ shared-with-team"
  },
  {
    "objectID": "posts/githubarchive/analysis/paper/index.html#what-gets-backed-up",
    "href": "posts/githubarchive/analysis/paper/index.html#what-gets-backed-up",
    "title": "Archiving 400 GitHub Repos Locally: A Complete Backup Strategy",
    "section": "4.1 What Gets Backed Up",
    "text": "4.1 What Gets Backed Up\n\n\n\nContent\nFormat\nUse Case\n\n\n\n\nGit history\nrepo.git/ + repo.bundle\nFull reproducibility\n\n\nWiki\nwiki.git/\nDocumentation\n\n\nIssues\nissues.json\nDiscussion archive\n\n\nPull requests\npull-requests.json\nCode review history\n\n\nReleases\nreleases.json\nVersion history\n\n\nRelease assets\nrelease-assets/\nBinaries, artifacts\n\n\nMetadata\nrepo-info.json\nRepository config\n\n\nLabels\nlabels.json\nIssue classification\n\n\nMilestones\nmilestones.json\nProject tracking"
  },
  {
    "objectID": "posts/githubarchive/analysis/paper/index.html#before-you-run",
    "href": "posts/githubarchive/analysis/paper/index.html#before-you-run",
    "title": "Archiving 400 GitHub Repos Locally: A Complete Backup Strategy",
    "section": "6.1 Before You Run",
    "text": "6.1 Before You Run\n\nTest with dry-run first\n./github-archive.sh --dry-run\nVerify backup directory has space\n\n400 repos × 50-100MB average = 20-40GB\nConsider external storage\n\nEnsure GitHub CLI is authenticated\ngh auth status\nBack up the backups\ntar -czf github-backup-$(date +%Y%m%d).tar.gz ~/github-archive"
  },
  {
    "objectID": "posts/githubarchive/analysis/paper/index.html#during-execution",
    "href": "posts/githubarchive/analysis/paper/index.html#during-execution",
    "title": "Archiving 400 GitHub Repos Locally: A Complete Backup Strategy",
    "section": "6.2 During Execution",
    "text": "6.2 During Execution\n\nScript verifies before deletion - No backup = no delete\nRequires typed confirmation - Prevents accidental execution\nDetailed logging - Check archive_YYYYMMDD.log if issues occur\nRepos in KEEP_ON_GITHUB are preserved - Still backed up but not deleted"
  },
  {
    "objectID": "posts/githubarchive/analysis/paper/index.html#organize-your-repos",
    "href": "posts/githubarchive/analysis/paper/index.html#organize-your-repos",
    "title": "Archiving 400 GitHub Repos Locally: A Complete Backup Strategy",
    "section": "7.1 Organize Your Repos",
    "text": "7.1 Organize Your Repos\nDecide what to keep:\nKEEP_ON_GITHUB=(\n    \"active-projects\"           # Currently maintained\n    \"shared-with-team\"          # Collaboration repos\n    \"client-work\"               # Client projects\n    \"portfolio\"                 # Showcase projects\n)"
  },
  {
    "objectID": "posts/githubarchive/analysis/paper/index.html#use-external-storage",
    "href": "posts/githubarchive/analysis/paper/index.html#use-external-storage",
    "title": "Archiving 400 GitHub Repos Locally: A Complete Backup Strategy",
    "section": "7.2 Use External Storage",
    "text": "7.2 Use External Storage\n# Back up to external drive\n./github-archive.sh --dir /Volumes/External/github-backups\n\n# Create compressed archive\ntar -czf github-backup-2025.tar.gz /Volumes/External/github-backups"
  },
  {
    "objectID": "posts/githubarchive/analysis/paper/index.html#automate-periodically",
    "href": "posts/githubarchive/analysis/paper/index.html#automate-periodically",
    "title": "Archiving 400 GitHub Repos Locally: A Complete Backup Strategy",
    "section": "7.3 Automate Periodically",
    "text": "7.3 Automate Periodically\n# Add to crontab (monthly backup)\n0 0 1 * * /path/to/github-archive.sh &gt;&gt; $HOME/.logs/github-archive.log 2&gt;&1"
  },
  {
    "objectID": "posts/githubarchive/analysis/paper/index.html#github-cli-authentication-fails",
    "href": "posts/githubarchive/analysis/paper/index.html#github-cli-authentication-fails",
    "title": "Archiving 400 GitHub Repos Locally: A Complete Backup Strategy",
    "section": "8.1 GitHub CLI authentication fails",
    "text": "8.1 GitHub CLI authentication fails\n# Check authentication status\ngh auth status\n\n# Re-authenticate\ngh auth logout\ngh auth login"
  },
  {
    "objectID": "posts/githubarchive/analysis/paper/index.html#bundle-verification-fails",
    "href": "posts/githubarchive/analysis/paper/index.html#bundle-verification-fails",
    "title": "Archiving 400 GitHub Repos Locally: A Complete Backup Strategy",
    "section": "8.2 Bundle verification fails",
    "text": "8.2 Bundle verification fails\nCheck network connection and available disk space, then rerun:\n# Resume backing up specific repo\n./github-archive.sh --owner myuser --dry-run"
  },
  {
    "objectID": "posts/githubarchive/analysis/paper/index.html#permissions-issues",
    "href": "posts/githubarchive/analysis/paper/index.html#permissions-issues",
    "title": "Archiving 400 GitHub Repos Locally: A Complete Backup Strategy",
    "section": "8.3 Permissions issues",
    "text": "8.3 Permissions issues\nEnsure you have: - Read access to all repos - Delete permissions for repos you want to remove\n# Check token permissions\ngh api user"
  },
  {
    "objectID": "posts/lssinceutility/index.html",
    "href": "posts/lssinceutility/index.html",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "",
    "text": "Finding files within specific time windows is a common task in research computing. Whether you’re auditing a research project, discovering recent changes, or managing data across timeframes, traditional Unix tools like find and ls require complex syntax and date format conversions.\nls_since.sh solves this problem with an intuitive, feature-rich utility that combines the power of Unix tools with thoughtful interface design. This post documents the utility’s architecture, features, and practical use cases—demonstrating why well-designed command-line tools can significantly improve productivity.\n\n\n\nProblem Space: Why date-based file filtering matters\nCore Features: The complete feature set explained\nTechnical Architecture: How the utility works internally\nPractical Examples: Real-world use cases and code recipes\nIntegration Patterns: Combining with other tools like fzf\nPerformance: Scalability and optimization considerations"
  },
  {
    "objectID": "posts/lssinceutility/index.html#what-this-post-covers",
    "href": "posts/lssinceutility/index.html#what-this-post-covers",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "",
    "text": "Problem Space: Why date-based file filtering matters\nCore Features: The complete feature set explained\nTechnical Architecture: How the utility works internally\nPractical Examples: Real-world use cases and code recipes\nIntegration Patterns: Combining with other tools like fzf\nPerformance: Scalability and optimization considerations"
  },
  {
    "objectID": "posts/lssinceutility/index.html#why-date-based-file-filtering-matters",
    "href": "posts/lssinceutility/index.html#why-date-based-file-filtering-matters",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "2.1 Why Date-Based File Filtering Matters",
    "text": "2.1 Why Date-Based File Filtering Matters\nResearch workflows generate thousands of files. Organizing and discovering them by creation or modification date is essential for:\n\nResearch Project Auditing\n\nFinding all files generated during a specific analysis phase\n\nVersion Control Workflows\n\nLocating uncommitted changes within a date range\n\nData Management\n\nIdentifying stale or recent files for archival or backup\n\nCollaboration Tracking\n\nDiscovering contributions from team members during specific periods\n\nLog Analysis\n\nFinding application-generated artifacts within time windows"
  },
  {
    "objectID": "posts/lssinceutility/index.html#limitations-of-standard-tools",
    "href": "posts/lssinceutility/index.html#limitations-of-standard-tools",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "2.2 Limitations of Standard Tools",
    "text": "2.2 Limitations of Standard Tools\nStandard Unix utilities have significant limitations for this task:\n\n\n\n\n\n\n\n\nTool\nStrength\nLimitation\n\n\n\n\nfind -newermt\nPowerful filtering\nComplex date format requirements\n\n\nls -lt\nSimple output\nSorts all files, doesn’t filter by date range\n\n\nstat\nDetailed information\nRequires per-file examination\n\n\nDate comparisons\nFlexible\nError-prone and platform-specific\n\n\n\n\n\n\nStreamlined file discovery workflow"
  },
  {
    "objectID": "posts/lssinceutility/index.html#the-three-stage-filtering-pipeline",
    "href": "posts/lssinceutility/index.html#the-three-stage-filtering-pipeline",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "3.1 The Three-Stage Filtering Pipeline",
    "text": "3.1 The Three-Stage Filtering Pipeline\nls_since.sh implements a streamlined filtering architecture:\nFind Phase → Timestamp Comparison → Output Formatting\n\n3.1.1 Stage 1: Find Phase\n\nRecursively discovers files in directory hierarchy\nFilters by file extension (configurable or all files)\nExcludes .git directories automatically (saves 30-40% processing time)\nReturns canonical file paths for processing\n\n\n\n3.1.2 Stage 2: Timestamp Comparison\nThe utility supports three orthogonal timestamp sources:\n\nbirth (default): File creation/copy time\nmtime: Last modification time\natime: Last access time\n\nDates are converted to Unix epoch timestamps for efficient integer comparisons:\n# Input: 01nov2025 → Internal: YYYY-MM-DD → Unix timestamp\nTARGET_TIMESTAMP=$(date -j -f \"%Y-%m-%d\" \"$TARGET_DATE\" \"+%s\")\n\n\n3.1.3 Stage 3: Output Formatting\nFour output modes for different use cases:\n\nNormal: TIMESTAMP - filepath for human readability\nCount: Total file count for statistics\nPaths-only: Raw file paths for piping\nfzf: Interactive selection interface"
  },
  {
    "objectID": "posts/lssinceutility/index.html#date-input-format",
    "href": "posts/lssinceutility/index.html#date-input-format",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "3.2 Date Input Format",
    "text": "3.2 Date Input Format\nThe utility standardizes on DDmmmYYYY format with lowercase months:\n01nov2025    # November 1, 2025\n15dec2024    # December 15, 2024\n28feb2025    # February 28, 2025\nThis approach: - Eliminates ambiguity (01/02/2025 is ambiguous; 01feb2025 is not) - Works consistently across locales - Avoids numeric month errors"
  },
  {
    "objectID": "posts/lssinceutility/index.html#extension-filtering",
    "href": "posts/lssinceutility/index.html#extension-filtering",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "3.3 Extension Filtering",
    "text": "3.3 Extension Filtering\nDefault file types optimized for research computing:\nEXTENSIONS=(\"md\" \"Rmd\" \"qmd\" \"sh\" \"pdf\" \"R\")\nSupports three filtering modes:\n\nDefault extensions: Works without flags\nCustom extensions: -t sh,py,txt or -t json,yaml\nAll files: -t all for any file type"
  },
  {
    "objectID": "posts/lssinceutility/index.html#basic-syntax",
    "href": "posts/lssinceutility/index.html#basic-syntax",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "4.1 Basic Syntax",
    "text": "4.1 Basic Syntax\nls_since.sh [OPTIONS] [directory] [date]"
  },
  {
    "objectID": "posts/lssinceutility/index.html#essential-options",
    "href": "posts/lssinceutility/index.html#essential-options",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "4.2 Essential Options",
    "text": "4.2 Essential Options\n# Filtering\n-t, --type STR           File extensions (comma-separated or 'all')\n-s, --start DATE         Start date in DDmmmYYYY format\n-e, --end DATE           End date (optional, creates date range)\n-T, --timestamp TYPE     Type: birth, mtime, atime\n\n# Output\n-c, --count              Count files instead of listing\n-p, --paths-only         Output paths only (no headers)\n--fzf                    Pipe to fzf for interactive selection\n\n# Utilities\n-C, --calendar           Show ASCII calendars as reference\n--no-color               Suppress green highlighting\n-h, --help               Display help and examples"
  },
  {
    "objectID": "posts/lssinceutility/index.html#positional-arguments",
    "href": "posts/lssinceutility/index.html#positional-arguments",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "4.3 Positional Arguments",
    "text": "4.3 Positional Arguments\n\nNo arguments: Interactive mode (prompts for everything)\nOne argument: Treated as date\nTwo arguments: Directory and date"
  },
  {
    "objectID": "posts/lssinceutility/index.html#example-1-interactive-mode-with-defaults",
    "href": "posts/lssinceutility/index.html#example-1-interactive-mode-with-defaults",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "5.1 Example 1: Interactive Mode with Defaults",
    "text": "5.1 Example 1: Interactive Mode with Defaults\n# Start interactive mode with 8-day window\nls_since.sh\n\n# Prompts for:\n# - Start date (default: 8 days ago)\n# - End date (default: today)\n# - File extensions (default: md,Rmd,qmd,sh,pdf,R)\nPerfect for exploring recent changes without command syntax."
  },
  {
    "objectID": "posts/lssinceutility/index.html#example-2-research-project-auditing",
    "href": "posts/lssinceutility/index.html#example-2-research-project-auditing",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "5.2 Example 2: Research Project Auditing",
    "text": "5.2 Example 2: Research Project Auditing\n# Find all R analysis files from November 2025\nls_since.sh -s 01nov2025 -e 30nov2025 -t R,Rmd ~/research/analysis\n\n# Output: R files with timestamps\n# 2025-11-15 10:23:45 - ~/research/analysis/main_analysis.R\n# 2025-11-12 14:12:33 - ~/research/analysis/utils.R"
  },
  {
    "objectID": "posts/lssinceutility/index.html#example-3-track-recent-modifications",
    "href": "posts/lssinceutility/index.html#example-3-track-recent-modifications",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "5.3 Example 3: Track Recent Modifications",
    "text": "5.3 Example 3: Track Recent Modifications\n# Find markdown docs modified in the last 2 weeks\nls_since.sh -T mtime -s 18nov2025 -t md ~/docs\n\n# Captures the last editing session for each file"
  },
  {
    "objectID": "posts/lssinceutility/index.html#example-4-interactive-file-selection",
    "href": "posts/lssinceutility/index.html#example-4-interactive-file-selection",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "5.4 Example 4: Interactive File Selection",
    "text": "5.4 Example 4: Interactive File Selection\n# Browse and select shell scripts using fzf\nls_since.sh --fzf -t sh 01jan2025\n\n# Opens fzf interface for interactive selection\n# Selected file can be piped to other commands"
  },
  {
    "objectID": "posts/lssinceutility/index.html#example-5-pipeline-integration",
    "href": "posts/lssinceutility/index.html#example-5-pipeline-integration",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "5.5 Example 5: Pipeline Integration",
    "text": "5.5 Example 5: Pipeline Integration\n# Edit recently modified scripts in vim\nls_since.sh -p -T mtime -s 01nov2025 -t sh | xargs vim\n\n# Opens all recently modified shell scripts in vim editor"
  },
  {
    "objectID": "posts/lssinceutility/index.html#example-6-file-count-statistics",
    "href": "posts/lssinceutility/index.html#example-6-file-count-statistics",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "5.6 Example 6: File Count Statistics",
    "text": "5.6 Example 6: File Count Statistics\n# Count all files generated in December 2024\nls_since.sh -c -s 01dec2024 -e 31dec2024 -t all\n\n# Output:\n# Total files found: 347"
  },
  {
    "objectID": "posts/lssinceutility/index.html#example-7-access-time-analysis",
    "href": "posts/lssinceutility/index.html#example-7-access-time-analysis",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "5.7 Example 7: Access Time Analysis",
    "text": "5.7 Example 7: Access Time Analysis\n# Find frequently accessed log files\nls_since.sh -T atime -s 15nov2025 ~/logs\n\n# Shows files accessed in the last 17 days\n\n\n\nInteractive exploration in action"
  },
  {
    "objectID": "posts/lssinceutility/index.html#flow",
    "href": "posts/lssinceutility/index.html#flow",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "6.1 Flow",
    "text": "6.1 Flow\n\nCalculate defaults: 8 days ago to today\nDisplay calendar reference (if -C flag used)\nPrompt for start date: Press Enter for default or type date\nPrompt for end date: Optional, press Enter for today\nSelect extensions: Choose defaults or customize\nDisplay selected dates: Confirm before processing\nExecute search: Begin file discovery"
  },
  {
    "objectID": "posts/lssinceutility/index.html#visual-calendar-reference",
    "href": "posts/lssinceutility/index.html#visual-calendar-reference",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "6.2 Visual Calendar Reference",
    "text": "6.2 Visual Calendar Reference\nREFERENCE CALENDARS:\n\nPrevious Month:\n    November 2025\nSu Mo Tu We Th Fr Sa\n                   1\n 2  3  4  5  6  7  8\n 9 10 11 12 13 14 15\n16 17 18 19 20 21 22\n23 24 25 26 27 28 29\n30\n\nCurrent Month:\n    December 2025\nSu Mo Tu We Th Fr Sa\n    1  2  3  4  5  6\n 7  8  9 10 11 12 13\n14 15 16 17 18 19 20\n21 22 23 24 25 26 27\n28 29 30 31\nSelected dates highlighted in green for visual confirmation."
  },
  {
    "objectID": "posts/lssinceutility/index.html#platform-compatibility",
    "href": "posts/lssinceutility/index.html#platform-compatibility",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "7.1 Platform Compatibility",
    "text": "7.1 Platform Compatibility\nThe utility seamlessly handles platform differences:\nmacOS-specific stat syntax:\nstat -f %B \"$file\"  # Birth time\nstat -f %m \"$file\"  # Modification time\nstat -f %a \"$file\"  # Access time\nLinux-specific stat syntax:\nstat -c %W \"$file\"  # Birth time (with fallback to mtime)\nstat -c %Y \"$file\"  # Modification time\nstat -c %X \"$file\"  # Access time\nAutomatic detection via [[ \"$OSTYPE\" == \"darwin\"* ]]"
  },
  {
    "objectID": "posts/lssinceutility/index.html#fzf-integration-architecture",
    "href": "posts/lssinceutility/index.html#fzf-integration-architecture",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "7.2 fzf Integration Architecture",
    "text": "7.2 fzf Integration Architecture\nWhen --fzf flag is used, the utility implements silent output collection:\n\nCreate temporary file at startup\nRedirect file paths to temp file (not stdout)\nSuppress headers/footers in fzf mode\nPipe temp file to fzf at completion\nClean up temporary file after selection\n\nThis ensures: - No duplicate output (files not listed then piped) - Clean terminal for fzf UI - Proper file path transmission to fzf"
  },
  {
    "objectID": "posts/lssinceutility/index.html#temporary-file-management",
    "href": "posts/lssinceutility/index.html#temporary-file-management",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "7.3 Temporary File Management",
    "text": "7.3 Temporary File Management\nThe utility uses secure temporary files for: - File path collection (fzf mode) - File count tracking (prevents subshell variable loss) - Extension filtering\nAll temporary files are cleaned up with rm -f at completion."
  },
  {
    "objectID": "posts/lssinceutility/index.html#error-handling",
    "href": "posts/lssinceutility/index.html#error-handling",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "7.4 Error Handling",
    "text": "7.4 Error Handling\nComprehensive validation for: - Missing or invalid directories - Invalid date formats with descriptive messages - Missing dependencies (fzf validation on --fzf) - Invalid timestamp types - Subshell context preservation"
  },
  {
    "objectID": "posts/lssinceutility/index.html#benchmark-results",
    "href": "posts/lssinceutility/index.html#benchmark-results",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "8.1 Benchmark Results",
    "text": "8.1 Benchmark Results\nOn typical research project directories (10,000 files):\n\n\n\nScenario\nTime\nNotes\n\n\n\n\nSmall range (1-day)\n~200ms\nMinimal filtering\n\n\nMedium range (30-day)\n~200ms\nStandard use case\n\n\nLarge range (1-year)\n~250ms\nFull year search\n\n\nStartup overhead\n~10ms\nNegligible\n\n\n\nResults consistent across macOS and Linux with SSD storage."
  },
  {
    "objectID": "posts/lssinceutility/index.html#time-complexity",
    "href": "posts/lssinceutility/index.html#time-complexity",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "8.2 Time Complexity",
    "text": "8.2 Time Complexity\n\nOverall: O(n) where n = number of files in tree\nPer-file: O(1) timestamp comparison\nSingle pass through directory hierarchy\nConstant-time integer comparisons"
  },
  {
    "objectID": "posts/lssinceutility/index.html#space-complexity",
    "href": "posts/lssinceutility/index.html#space-complexity",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "8.3 Space Complexity",
    "text": "8.3 Space Complexity\n\nOutput: O(m) where m = number of matching files\nfzf mode: Requires temporary file storage\nNormal mode: Streaming output (minimal memory)"
  },
  {
    "objectID": "posts/lssinceutility/index.html#optimization-tips",
    "href": "posts/lssinceutility/index.html#optimization-tips",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "8.4 Optimization Tips",
    "text": "8.4 Optimization Tips\n\nUse specific dates: Narrow ranges reduce file checks\nFilter by extension: Fewer files to examine with -t flag\nAutomatic .git exclusion: Saves 30-40% processing time\nUse mtime on Linux: Faster than birth time (no fallback needed)"
  },
  {
    "objectID": "posts/lssinceutility/index.html#vs.-find--newermt",
    "href": "posts/lssinceutility/index.html#vs.-find--newermt",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "9.1 vs. find -newermt",
    "text": "9.1 vs. find -newermt\nAdvantages of ls_since.sh: - Simpler syntax (no date format conversion required) - Multiple timestamp type support - Interactive mode with defaults - Integrated calendar reference - fzf integration built-in\nAdvantages of find: - Available on all systems - Minimal dependencies - More extensive filtering options"
  },
  {
    "objectID": "posts/lssinceutility/index.html#vs.-ls--lt",
    "href": "posts/lssinceutility/index.html#vs.-ls--lt",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "9.2 vs. ls -lt",
    "text": "9.2 vs. ls -lt\nAdvantages of ls_since.sh: - Date range filtering - Recursive directory traversal - Extensible filtering options - fzf integration\nAdvantages of ls: - No dependencies - Simpler for interactive use"
  },
  {
    "objectID": "posts/lssinceutility/index.html#vs.-locatemlocate",
    "href": "posts/lssinceutility/index.html#vs.-locatemlocate",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "9.3 vs. locate/mlocate",
    "text": "9.3 vs. locate/mlocate\nAdvantages of ls_since.sh: - Real-time results (no database needed) - Date range filtering - Timestamp type selection\nAdvantages of locate: - Faster for very large filesystems - Pre-built database"
  },
  {
    "objectID": "posts/lssinceutility/index.html#pattern-1-monthly-audit-reports",
    "href": "posts/lssinceutility/index.html#pattern-1-monthly-audit-reports",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "10.1 Pattern 1: Monthly Audit Reports",
    "text": "10.1 Pattern 1: Monthly Audit Reports\n# Generate audit for each month\nfor month in {01..12}; do\n  count=$(ls_since.sh -c -s ${month}jan2025 \\\n          -e 31${month}2025 -t all 2&gt;/dev/null | \\\n          tail -1 | awk '{print $NF}')\n  echo \"Month $month: $count files\"\ndone"
  },
  {
    "objectID": "posts/lssinceutility/index.html#pattern-2-recent-work-summary",
    "href": "posts/lssinceutility/index.html#pattern-2-recent-work-summary",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "10.2 Pattern 2: Recent Work Summary",
    "text": "10.2 Pattern 2: Recent Work Summary\n# Show recent modifications by file type\necho \"=== Shell Scripts ===\"\nls_since.sh -s 01nov2025 -t sh | head -5\n\necho \"=== Documentation ===\"\nls_since.sh -s 01nov2025 -t md,Rmd | head -5\n\necho \"=== Analysis ===\"\nls_since.sh -s 01nov2025 -t R,Rmd,qmd | head -5"
  },
  {
    "objectID": "posts/lssinceutility/index.html#pattern-3-git-aware-file-discovery",
    "href": "posts/lssinceutility/index.html#pattern-3-git-aware-file-discovery",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "10.3 Pattern 3: Git-aware File Discovery",
    "text": "10.3 Pattern 3: Git-aware File Discovery\n# Find unstaged files modified after date\ngit ls-files -m | while read file; do\n  ls_since.sh -p -s 01nov2025 | grep -q \"$file\" && echo \"$file\"\ndone"
  },
  {
    "objectID": "posts/lssinceutility/index.html#pattern-4-backup-selection",
    "href": "posts/lssinceutility/index.html#pattern-4-backup-selection",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "10.4 Pattern 4: Backup Selection",
    "text": "10.4 Pattern 4: Backup Selection\n# Backup files modified in last week\nls_since.sh -p -T mtime -s 25nov2025 -t all | \\\n  tar -czf backup_nov25.tar.gz -T -"
  },
  {
    "objectID": "posts/lssinceutility/index.html#pattern-5-code-review-workflow",
    "href": "posts/lssinceutility/index.html#pattern-5-code-review-workflow",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "10.5 Pattern 5: Code Review Workflow",
    "text": "10.5 Pattern 5: Code Review Workflow\n# Review recent changes in specific file type\nls_since.sh --fzf -t R -s 01nov2025 | \\\n  xargs git diff HEAD~1..HEAD --"
  },
  {
    "objectID": "posts/lssinceutility/index.html#installation",
    "href": "posts/lssinceutility/index.html#installation",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "11.1 Installation",
    "text": "11.1 Installation\nCopy ls_since.sh to your bin directory:\n# Copy to personal bin\ncp ls_since.sh ~/bin/\nchmod +x ~/bin/ls_since.sh\n\n# Or add to project\ncp ls_since.sh ./scripts/"
  },
  {
    "objectID": "posts/lssinceutility/index.html#first-use-interactive-mode",
    "href": "posts/lssinceutility/index.html#first-use-interactive-mode",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "11.2 First Use: Interactive Mode",
    "text": "11.2 First Use: Interactive Mode\n# Start with no arguments for guided experience\nls_since.sh\n\n# Prompts you through:\n# 1. Start date selection (with default)\n# 2. End date selection (with default)\n# 3. File type selection (with defaults)\n# 4. Displays calendars for reference"
  },
  {
    "objectID": "posts/lssinceutility/index.html#common-commands-cheat-sheet",
    "href": "posts/lssinceutility/index.html#common-commands-cheat-sheet",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "11.3 Common Commands Cheat Sheet",
    "text": "11.3 Common Commands Cheat Sheet\n# List all default types since 8 days ago\nls_since.sh\n\n# List shell scripts from November\nls_since.sh -s 01nov2025 -e 30nov2025 -t sh\n\n# Count files in last 2 weeks\nls_since.sh -c -s 18nov2025\n\n# Interactive file selection\nls_since.sh --fzf -t R,Rmd 01jan2025\n\n# Pipe to editor\nls_since.sh -p -T mtime -s 01nov2025 -t md | xargs vim\n\n# Display help\nls_since.sh -h\n\n\n\nReady to explore your file system"
  },
  {
    "objectID": "posts/lssinceutility/index.html#calendar-dates-not-highlighted-in-green",
    "href": "posts/lssinceutility/index.html#calendar-dates-not-highlighted-in-green",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "12.1 Calendar dates not highlighted in green",
    "text": "12.1 Calendar dates not highlighted in green\nCause: Terminal doesn’t support ANSI color codes\nSolution: Use --no-color flag to suppress coloring\nls_since.sh -C --no-color -s 01nov2025"
  },
  {
    "objectID": "posts/lssinceutility/index.html#birth-time-unavailable-on-linux",
    "href": "posts/lssinceutility/index.html#birth-time-unavailable-on-linux",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "12.2 Birth time unavailable on Linux",
    "text": "12.2 Birth time unavailable on Linux\nCause: Linux filesystems may not store birth time\nSolution: Use modification time instead\nls_since.sh -T mtime -s 01nov2025"
  },
  {
    "objectID": "posts/lssinceutility/index.html#fzf-not-found-error",
    "href": "posts/lssinceutility/index.html#fzf-not-found-error",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "12.3 fzf not found error",
    "text": "12.3 fzf not found error\nCause: fzf not installed\nSolution: Install with appropriate package manager\n# macOS\nbrew install fzf\n\n# Ubuntu/Debian\nsudo apt-get install fzf\n\n# Then use\nls_since.sh --fzf -t sh 01jan2025"
  },
  {
    "objectID": "posts/lssinceutility/index.html#no-files-found-in-date-range",
    "href": "posts/lssinceutility/index.html#no-files-found-in-date-range",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "12.4 No files found in date range",
    "text": "12.4 No files found in date range\nCause: Files don’t exist in range or extension doesn’t match\nSolution: Check date format and try broader type\n# Try all file types\nls_since.sh -t all -s 01nov2025 -e 30nov2025\n\n# Check file dates\nls_since.sh -p -s 01jan2024 -t all | head"
  },
  {
    "objectID": "posts/lssinceutility/index.html#related-utilities",
    "href": "posts/lssinceutility/index.html#related-utilities",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "14.1 Related Utilities",
    "text": "14.1 Related Utilities\n\nfind command: man find for advanced filtering\nstat command: man stat for detailed file information\nfzf: junegunn/fzf for interactive selection\nQuarto: For research computing workflows"
  },
  {
    "objectID": "posts/lssinceutility/index.html#best-practices",
    "href": "posts/lssinceutility/index.html#best-practices",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "14.2 Best Practices",
    "text": "14.2 Best Practices\n\nUse interactive mode for first-time exploration\nVerify date ranges with calendar reference (-C flag)\nTest pipelines before integrating into scripts\nCombine with other tools for powerful workflows\nVerify output before destructive operations"
  },
  {
    "objectID": "posts/lssinceutility/index.html#using-standard-unix-tools-for-similar-functionality",
    "href": "posts/lssinceutility/index.html#using-standard-unix-tools-for-similar-functionality",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "15.1 Using Standard Unix Tools for Similar Functionality",
    "text": "15.1 Using Standard Unix Tools for Similar Functionality\nWhile ls_since.sh provides an intuitive, feature-rich interface, the same date-range filtering can be accomplished using the standard find command available on all Unix/Linux systems. This appendix demonstrates how to replicate the core functionality using only built-in tools."
  },
  {
    "objectID": "posts/lssinceutility/index.html#enhanced-find-command-for-rmd-files-with-date-range",
    "href": "posts/lssinceutility/index.html#enhanced-find-command-for-rmd-files-with-date-range",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "15.2 Enhanced find Command for Rmd Files with Date Range",
    "text": "15.2 Enhanced find Command for Rmd Files with Date Range\nfind . -type f -name \"*.Rmd\" -newermt \"2025-11-01\" ! -newermt \"2025-12-01\" \\\n  -printf '%T@ %TY-%Tm-%Td %TH:%TM:%.2TS %p\\n' | \\\n  sort -rn | \\\n  cut -d' ' -f4- | \\\n  fzf --preview 'head -20 {}'"
  },
  {
    "objectID": "posts/lssinceutility/index.html#command-breakdown",
    "href": "posts/lssinceutility/index.html#command-breakdown",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "15.3 Command Breakdown",
    "text": "15.3 Command Breakdown\n\n\n\n\n\n\n\n\nComponent\nPurpose\nDetails\n\n\n\n\nfind .\nSearch current directory\nChange . to specify different directory\n\n\n-type f\nOnly regular files\nExcludes directories and symlinks\n\n\n-name \"*.Rmd\"\nFilter by file extension\nChange extension pattern as needed\n\n\n-newermt \"2025-11-01\"\nStart date (inclusive)\nFiles modified after this date\n\n\n! -newermt \"2025-12-01\"\nEnd date (exclusive)\nFiles NOT modified on/after this date\n\n\n-printf '%T@ %TY-%Tm-%Td %TH:%TM:%.2TS %p\\n'\nFormat output with timestamp\nShows: epoch, ISO date, time, filepath\n\n\nsort -rn\nSort by timestamp, newest first\n-rn = reverse numeric sort\n\n\ncut -d' ' -f4-\nExtract readable date and path\nRemoves epoch timestamp for cleaner output\n\n\nfzf --preview 'head -20 {}'\nInteractive file selection\nPreview shows first 20 lines of file"
  },
  {
    "objectID": "posts/lssinceutility/index.html#practical-usage-examples",
    "href": "posts/lssinceutility/index.html#practical-usage-examples",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "15.4 Practical Usage Examples",
    "text": "15.4 Practical Usage Examples\nFind and edit Rmd files from November in your research directory:\nfind ~/research -type f -name \"*.Rmd\" -newermt \"2025-11-01\" ! -newermt \"2025-12-01\" | \\\n  fzf | xargs vim\nSelect and copy multiple files to backup:\nfind . -type f -name \"*.Rmd\" -newermt \"2025-11-15\" ! -newermt \"2025-11-30\" | \\\n  fzf -m | xargs -I {} cp {} ~/backup/\nPreview analysis files before processing:\nfind ~/analysis -type f -name \"*.Rmd\" -newermt \"2025-11-01\" ! -newermt \"2025-12-01\" | \\\n  fzf --preview 'wc -l {}; head -30 {}'"
  },
  {
    "objectID": "posts/lssinceutility/index.html#advantages-and-trade-offs",
    "href": "posts/lssinceutility/index.html#advantages-and-trade-offs",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "15.5 Advantages and Trade-offs",
    "text": "15.5 Advantages and Trade-offs\nAdvantages of find: - No additional dependencies (built into all Unix/Linux systems) - Powerful filtering options (combine multiple conditions) - Direct pipeline to other commands - Excellent for scripting and automation - Consistent behavior across platforms\nAdvantages of ls_since.sh: - Friendlier date format (01nov2025 vs 2025-11-01) - Interactive mode with sensible defaults - Calendar reference visualization - Built-in extension presets - Cleaner error messages and help text - Optimized for human interaction\nWhen to use find: - Scripts and automation workflows - Systems without ls_since.sh installed - Complex filtering combining multiple file attributes - Batch operations on large file sets\nWhen to use ls_since.sh: - Interactive exploration and discovery - One-off searches without remembering syntax - Research projects requiring date range audits - Workflows with multiple file type categories"
  },
  {
    "objectID": "posts/lssinceutility/index.html#creating-a-reusable-function",
    "href": "posts/lssinceutility/index.html#creating-a-reusable-function",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "15.6 Creating a Reusable Function",
    "text": "15.6 Creating a Reusable Function\nAdd this to your .zshrc or .bashrc for a convenient wrapper:\nfind_rmd_range() {\n  local start_date=\"${1:-2025-11-01}\"\n  local end_date=\"${2:-2025-12-01}\"\n  local directory=\"${3:-.}\"\n\n  find \"$directory\" -type f -name \"*.Rmd\" \\\n    -newermt \"$start_date\" ! -newermt \"$end_date\" \\\n    -printf '%T@ %TY-%Tm-%Td %TH:%TM:%.2TS %p\\n' | \\\n    sort -rn | \\\n    cut -d' ' -f4- | \\\n    fzf --preview 'head -20 {}' \\\n        --header \"Rmd files: $start_date to $end_date\"\n}\n\n# Usage examples:\n# find_rmd_range \"2025-11-01\" \"2025-11-30\" ~/analysis\n# find_rmd_range \"2025-10-01\" \"2025-12-31\"              # Current directory\n# find_rmd_range                                        # Uses defaults"
  },
  {
    "objectID": "posts/lssinceutility/index.html#key-takeaway",
    "href": "posts/lssinceutility/index.html#key-takeaway",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "15.7 Key Takeaway",
    "text": "15.7 Key Takeaway\nThe find command demonstrates that date-based file filtering doesn’t require specialized tools—standard Unix utilities can accomplish the same results. The choice between find and ls_since.sh depends on your workflow: use find for scripting and edge cases, and ls_since.sh for interactive exploration and human-friendly defaults."
  },
  {
    "objectID": "posts/lssinceutility/index.html#about-this-post",
    "href": "posts/lssinceutility/index.html#about-this-post",
    "title": "ls_since.sh: Advanced File Date Filtering for Research Computing",
    "section": "15.8 About This Post",
    "text": "15.8 About This Post\nThis blog post was generated from a comprehensive white paper documenting the ls_since.sh utility. The white paper provides deeper technical details, implementation patterns, and advanced use cases beyond what’s covered here.\nFor the complete reference documentation, see the white paper at /Users/zenn/Dropbox/bin/date_filtering.md.\nDate published: December 2, 2025 Last updated: December 2, 2025"
  },
  {
    "objectID": "posts/markdowntoblog/index.html",
    "href": "posts/markdowntoblog/index.html",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "",
    "text": "Converting your documentation into a professional blog post\nA structured workflow for publishing technical documentation with reproducibility built-in"
  },
  {
    "objectID": "posts/markdowntoblog/index.html#what-youll-learn",
    "href": "posts/markdowntoblog/index.html#what-youll-learn",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "1.1 What You’ll Learn",
    "text": "1.1 What You’ll Learn\n\nThe complete workflow: Step-by-step process for converting markdown to blog post\nKey decisions: How to structure content for blog format\nTechnical integration: Symlinks, Quarto compatibility, and git workflow\nVerification steps: Testing and validation before publication\nTroubleshooting: Common issues and solutions\nTimeline: Realistic expectations (40 minutes total)"
  },
  {
    "objectID": "posts/markdowntoblog/index.html#why-this-matters",
    "href": "posts/markdowntoblog/index.html#why-this-matters",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "1.2 Why This Matters",
    "text": "1.2 Why This Matters\nBlog posts discoverable through search and social sharing reach a broader audience than markdown files in your project directories. Using ZZCOLLAB ensures:\n\nReproducibility - Docker + renv lock exact environment\nDiscoverability - Integrated with your blog’s search and tagging\nProfessionalism - Proper metadata, structure, and styling\nMaintainability - Version controlled and easily updated\nExtensibility - Can add data analysis, visualizations, or interactive elements later"
  },
  {
    "objectID": "posts/markdowntoblog/index.html#tools-youll-need",
    "href": "posts/markdowntoblog/index.html#tools-youll-need",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "2.1 Tools You’ll Need",
    "text": "2.1 Tools You’ll Need\nMust Have: - ZZCOLLAB framework (installed) - Quarto (for rendering) - Git (version control) - Bash shell\nNice to Have: - GitHub CLI (gh) for automation - Docker (for full reproducibility) - Local Quarto preview capability"
  },
  {
    "objectID": "posts/markdowntoblog/index.html#initial-assessment-5-minutes",
    "href": "posts/markdowntoblog/index.html#initial-assessment-5-minutes",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "2.2 Initial Assessment: 5 Minutes",
    "text": "2.2 Initial Assessment: 5 Minutes\nBefore starting, evaluate your source markdown:\nAsk Yourself: 1. How long is it? (200 lines? 600 lines?) 2. Does it have code examples? (bash, R, Python?) 3. Does it need images? (hero image, embedded diagrams?) 4. Is it a tutorial, reference guide, or how-to? 5. What’s the target audience?\nExample: GitHub Archive Post\n- Length: ~600 lines (substantial)\n- Code: Yes (bash script)\n- Images: Yes (hero + concept)\n- Type: How-to guide + reference\n- Audience: DevOps engineers\n- Estimate: 10 minutes to convert content"
  },
  {
    "objectID": "posts/markdowntoblog/index.html#make-key-decisions",
    "href": "posts/markdowntoblog/index.html#make-key-decisions",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "2.3 Make Key Decisions",
    "text": "2.3 Make Key Decisions\nBefore initializing the project, decide:\n\n\n\n\n\n\n\n\nDecision\nGitHub Archive\nYour Post\n\n\n\n\nBlog slug\ngithub_archive\n[kebab-case-name]\n\n\nZZCOLLAB profile\nubuntu_standard_publishing\nSame (has Quarto)\n\n\nInclude analysis?\nNo\nYes / No\n\n\nMedia assets\n2 images\nHow many?\n\n\nCategories\nDevOps, Git, etc\n?"
  },
  {
    "objectID": "posts/markdowntoblog/index.html#step-1-create-directory",
    "href": "posts/markdowntoblog/index.html#step-1-create-directory",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "3.1 Step 1: Create Directory",
    "text": "3.1 Step 1: Create Directory\ncd ~/prj/qblog/posts\nmkdir my_blog_post && cd my_blog_post\nReplace my_blog_post with your kebab-case slug (e.g., github_archive, python_async, data_pipeline)."
  },
  {
    "objectID": "posts/markdowntoblog/index.html#step-2-initialize-zzcollab",
    "href": "posts/markdowntoblog/index.html#step-2-initialize-zzcollab",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "3.2 Step 2: Initialize ZZCOLLAB",
    "text": "3.2 Step 2: Initialize ZZCOLLAB\nzzcollab -r ubuntu_standard_publishing\nThis creates: - Standard project structure (analysis/, R/, tests/) - Dockerfile (reproducible environment) - renv.lock (R package versions) - Makefile (build automation) - .zzcollab/manifest.json (project metadata)\nExpected output: ~20 lines of checkmarks showing successful initialization"
  },
  {
    "objectID": "posts/markdowntoblog/index.html#step-3-set-up-blog-structure",
    "href": "posts/markdowntoblog/index.html#step-3-set-up-blog-structure",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "3.3 Step 3: Set Up Blog Structure",
    "text": "3.3 Step 3: Set Up Blog Structure\n./modules/setup_symlinks.sh\nThis creates the dual-symlink system that makes everything work:\nAt post root (for Quarto):\nindex.qmd → analysis/paper/index.qmd\nfigures/  → analysis/figures/\nmedia/    → analysis/media/\ndata/     → analysis/data/\nIn analysis/paper/ (for editing):\nfigures/ → ../figures/\nmedia/   → ../media/\ndata/    → ../data/\nWhy symlinks? - Quarto expects posts/*/index.qmd at root - ZZCOLLAB/rrtools puts content in analysis/paper/ - Symlinks bridge both conventions - Image paths like ![](media/images/hero.jpg) work automatically"
  },
  {
    "objectID": "posts/markdowntoblog/index.html#step-4-verify-structure",
    "href": "posts/markdowntoblog/index.html#step-4-verify-structure",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "3.4 Step 4: Verify Structure",
    "text": "3.4 Step 4: Verify Structure\n# Check root symlinks\nls -la | grep \"^l\"\n\n# Should show all 4 symlinks ✓\n# index.qmd → analysis/paper/index.qmd\n# figures → analysis/figures\n# media → analysis/media\n# data → analysis/data\n\n# Check paper directory\nls -la analysis/paper/\n\n# Should show:\n# figures → ../figures\n# media → ../media\n# data → ../data\n# index.qmd (actual file)"
  },
  {
    "objectID": "posts/markdowntoblog/index.html#create-proper-yaml-frontmatter",
    "href": "posts/markdowntoblog/index.html#create-proper-yaml-frontmatter",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "4.1 Create Proper YAML Frontmatter",
    "text": "4.1 Create Proper YAML Frontmatter\nReplace the template YAML with your blog post metadata:\n---\ntitle: \"Your Blog Post Title\"\nsubtitle: \"Optional subtitle (2-3 words)\"\nauthor: \"Your Name\"\ndate: \"2025-12-02\"\ncategories: [Category1, Category2, Category3]\ndescription: \"2-3 sentence summary of the post\"\nimage: \"media/images/hero-image.jpg\"\ndocument-type: \"blog\"\ndraft: false\nexecute:\n  echo: true\n  warning: false\n  message: false\nformat:\n  html:\n    toc: true\n    toc-depth: 3\n    code-fold: false\n    code-tools: false\n---\nKey fields: - title: 50-70 characters, searchable, descriptive - categories: 2-4 relevant categories for filtering - description: Appears in blog listing, capture value proposition - draft: false: Set to true to hide from publication - date: Publish date (or last-modified for auto-update) - code-fold: false: Show code by default (good for tutorials)"
  },
  {
    "objectID": "posts/markdowntoblog/index.html#structure-your-content",
    "href": "posts/markdowntoblog/index.html#structure-your-content",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "4.2 Structure Your Content",
    "text": "4.2 Structure Your Content\nBlog post structure (proven effective):\n1. Hero image + subtitle (1 paragraph)\n2. Introduction (2-3 paragraphs)\n   - What this post covers\n   - Why it matters\n   - What they'll learn\n3. Problem statement or background (2-3 sections)\n4. Solution/main content (multiple detailed sections)\n5. Code examples or implementation (complete, runnable)\n6. Usage instructions (step-by-step)\n7. Examples with actual output\n8. Best practices (tips and recommendations)\n9. Troubleshooting (common issues)\n10. Key takeaways (callout box summary)\n11. Further reading (links to related docs)"
  },
  {
    "objectID": "posts/markdowntoblog/index.html#adapt-your-markdown",
    "href": "posts/markdowntoblog/index.html#adapt-your-markdown",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "4.3 Adapt Your Markdown",
    "text": "4.3 Adapt Your Markdown\nMarkdown → Quarto conversions:\n# Simple heading → stays the same\n## Sub-heading → stays the same\n\n[Link text](url) → stays the same\n![Alt text](path) → stays the same\n- Bullet → stays the same\n\n&gt; Blockquote → becomes :::callout-note\nSpecial: Convert blockquotes to Quarto callouts\n::: {.callout-note}\n## Summary\n\nYour key takeaway goes here\n:::"
  },
  {
    "objectID": "posts/markdowntoblog/index.html#code-blocks-matter",
    "href": "posts/markdowntoblog/index.html#code-blocks-matter",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "4.4 Code Blocks Matter",
    "text": "4.4 Code Blocks Matter\nMake code blocks production-ready:\n#!/bin/bash\n# Complete, runnable example\n# Comments explaining non-obvious parts\nset -e  # Exit on error\n\n# Actual working code\nyour_command --with-options\nBest practices: - Specify language (bash, r, python, sql, etc) - Make examples self-contained - Include comments for clarity - Test them before publishing"
  },
  {
    "objectID": "posts/markdowntoblog/index.html#prepare-images",
    "href": "posts/markdowntoblog/index.html#prepare-images",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "5.1 Prepare Images",
    "text": "5.1 Prepare Images\n# Copy your hero image\ncp ~/path/to/hero-image.jpg analysis/media/images/my-post-hero.jpg\n\n# Copy other images (concept art, diagrams, etc)\ncp ~/path/to/concept-image.png analysis/media/images/my-post-concept.png"
  },
  {
    "objectID": "posts/markdowntoblog/index.html#document-image-sources",
    "href": "posts/markdowntoblog/index.html#document-image-sources",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "5.2 Document Image Sources",
    "text": "5.2 Document Image Sources\nCreate a README in the images directory:\ncat &gt; analysis/media/images/README.md &lt;&lt; 'EOF'\n# Image Sources and Attribution\n\n## my-post-hero.jpg\n- Source: Unsplash / Pixabay / Pexels\n- Creator: [Creator Name if known]\n- License: [License type]\n- URL: [Link to original if available]\n\n## my-post-concept.png\n- Source: [Where it came from]\n- License: [CC0, CC-BY, etc]\n- Notes: [Any other relevant info]\nEOF\nWhy document sources? - Respect creator attribution - Document licensing compliance - Make it easy to replace if needed - Good practice for professional blogs"
  },
  {
    "objectID": "posts/markdowntoblog/index.html#reference-images-in-blog-post",
    "href": "posts/markdowntoblog/index.html#reference-images-in-blog-post",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "5.3 Reference Images in Blog Post",
    "text": "5.3 Reference Images in Blog Post\n![Descriptive alt text](media/images/my-post-hero.jpg){.img-fluid}\n\n*Subtitle or caption for the image*\nThe {.img-fluid} class makes images responsive on mobile."
  },
  {
    "objectID": "posts/markdowntoblog/index.html#customize-readme.md",
    "href": "posts/markdowntoblog/index.html#customize-readme.md",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "6.1 Customize README.md",
    "text": "6.1 Customize README.md\nThe ZZCOLLAB template creates a generic README. Personalize it:\n# Your Blog Post Title\n\n&gt; A blog post about [topic]\n\n## Quick Start\n\nTo read this blog post:\n\n```bash\nquarto render analysis/paper/index.qmd\nopen index.html"
  },
  {
    "objectID": "posts/markdowntoblog/index.html#structure",
    "href": "posts/markdowntoblog/index.html#structure",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "6.2 Structure",
    "text": "6.2 Structure\n\nanalysis/paper/index.qmd - Main blog post content\nanalysis/media/images/ - Hero image and diagrams\nDockerfile - Reproducible environment\nrenv.lock - R package versions"
  },
  {
    "objectID": "posts/markdowntoblog/index.html#to-extend-this-post",
    "href": "posts/markdowntoblog/index.html#to-extend-this-post",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "6.3 To Extend This Post",
    "text": "6.3 To Extend This Post\nAdd R analysis:\nRscript analysis/scripts/01_prepare_data.R\nRscript analysis/scripts/02_generate_figures.R\nquarto render analysis/paper/index.qmd\n\n## Update DESCRIPTION File\n\n```r\nPackage: myblogpost\nTitle: Your Blog Post Title Here\nDescription: Brief description of what the post covers\nVersion: 1.0.0\nAuthors@R: person(\"Your Name\", role = c(\"aut\", \"cre\"))"
  },
  {
    "objectID": "posts/markdowntoblog/index.html#quick-validation",
    "href": "posts/markdowntoblog/index.html#quick-validation",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "7.1 Quick Validation",
    "text": "7.1 Quick Validation\n# Verify symlinks are correct\nls -la index.qmd\n# Should show: index.qmd -&gt; analysis/paper/index.qmd ✓\n\n# Check YAML syntax (proper indentation, no tabs)\nhead -25 analysis/paper/index.qmd\n\n# Verify image paths are relative\ngrep \"images/\" analysis/paper/index.qmd\n# Should show: media/images/filename.jpg (not absolute path)"
  },
  {
    "objectID": "posts/markdowntoblog/index.html#render-locally-if-quarto-installed",
    "href": "posts/markdowntoblog/index.html#render-locally-if-quarto-installed",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "7.2 Render Locally (If Quarto Installed)",
    "text": "7.2 Render Locally (If Quarto Installed)\ncd ~/prj/qblog/posts/my_blog_post\nquarto render analysis/paper/index.qmd\n\n# Preview in browser\nopen index.html\nCommon issues:\n\n\n\n\n\n\n\n\nError\nCause\nFix\n\n\n\n\nindex.qmd not found\nSymlink broken\nCheck: ls -la index.qmd\n\n\nImage not found\nWrong path\nUse relative: media/images/file.jpg\n\n\nYAML parse error\nIndentation (tabs?)\nUse spaces only\n\n\nCode not highlighted\nWrong language\nSpecify: ```bash not ```"
  },
  {
    "objectID": "posts/markdowntoblog/index.html#commit-your-work",
    "href": "posts/markdowntoblog/index.html#commit-your-work",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "9.1 Commit Your Work",
    "text": "9.1 Commit Your Work\ncd ~/prj/qblog/posts/my_blog_post\n\n# Stage all changes\ngit add .\n\n# Check what you're committing\ngit status\n\n# Commit with descriptive message\ngit commit -m \"Add blog post: Your Post Title\n\n- analysis/paper/index.qmd: Main content\n- analysis/media/images/: Hero and supporting images\n- README.md: Post documentation\n- Docker/renv: Reproducible environment\n\nTopics covered:\n- [Main topic 1]\n- [Main topic 2]\n- [Main topic 3]\""
  },
  {
    "objectID": "posts/markdowntoblog/index.html#push-to-remote",
    "href": "posts/markdowntoblog/index.html#push-to-remote",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "9.2 Push to Remote",
    "text": "9.2 Push to Remote\ngit push origin main\nGit handles symlinks automatically - they’re stored as text files containing the target path. When someone clones your blog, symlinks are recreated correctly."
  },
  {
    "objectID": "posts/markdowntoblog/index.html#timeline-and-checklist",
    "href": "posts/markdowntoblog/index.html#timeline-and-checklist",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "10.1 Timeline and Checklist",
    "text": "10.1 Timeline and Checklist\n\n\n\nPhase\nTime\nTask\n\n\n\n\nAssessment\n5 min\nRead markdown, plan structure\n\n\nInitialize\n3 min\nCreate directory, run zzcollab\n\n\nSetup\n1 min\nRun setup_symlinks.sh\n\n\nConvert\n10 min\nAdapt markdown → Quarto\n\n\nMedia\n5 min\nAdd images, document sources\n\n\nMetadata\n3 min\nUpdate README, DESCRIPTION\n\n\nTest\n5 min\nVerify structure, validate\n\n\nCommit\n2 min\nGit add/commit/push\n\n\nTotal\n~40 min\nBlog post ready"
  },
  {
    "objectID": "posts/markdowntoblog/index.html#pre-publication-checklist",
    "href": "posts/markdowntoblog/index.html#pre-publication-checklist",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "10.2 Pre-Publication Checklist",
    "text": "10.2 Pre-Publication Checklist\nContent: - [ ] Blog post written (analysis/paper/index.qmd) - [ ] All sections complete - [ ] Code examples tested - [ ] Links verified (no 404s) - [ ] Grammar/spelling checked\nMetadata: - [ ] YAML frontmatter correct - [ ] Title descriptive (50-70 chars) - [ ] Categories relevant - [ ] Description captures value - [ ] draft: false (to publish)\nMedia: - [ ] Hero image added - [ ] Images documented in README.md - [ ] All image paths relative (media/images/...) - [ ] Image alt text present - [ ] File sizes reasonable (&lt; 500KB each)\nStructure: - [ ] Symlinks verified (4 at root) - [ ] README.md customized - [ ] DESCRIPTION updated - [ ] .gitignore appropriate\nTesting: - [ ] Local render successful (if Quarto installed) - [ ] All images display - [ ] All links work - [ ] Code syntax highlighted - [ ] TOC generates correctly"
  },
  {
    "objectID": "posts/markdowntoblog/index.html#add-r-analysis",
    "href": "posts/markdowntoblog/index.html#add-r-analysis",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "11.1 Add R Analysis",
    "text": "11.1 Add R Analysis\nCreate analysis scripts:\n# analysis/scripts/01_prepare_data.R\nlibrary(tidyverse)\n# ... your analysis code ...\nwrite_csv(results, \"analysis/data/derived_data/results.csv\")\nUpdate Makefile:\npost-analysis:\n    Rscript analysis/scripts/01_prepare_data.R\n    Rscript analysis/scripts/02_generate_figures.R\n\npost-render: post-analysis\n    quarto render index.qmd"
  },
  {
    "objectID": "posts/markdowntoblog/index.html#add-interactive-content",
    "href": "posts/markdowntoblog/index.html#add-interactive-content",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "11.2 Add Interactive Content",
    "text": "11.2 Add Interactive Content\nQuarto supports multiple languages:\n\n// Observable JS for interactive visualizations\nPlot.plot({\n  // ... your D3/Observable code ...\n})\n\n\n\n\n\n\n\n# R code with Shiny for interactive elements\nlibrary(shiny)\nlibrary(plotly)"
  },
  {
    "objectID": "posts/markdowntoblog/index.html#version-updates",
    "href": "posts/markdowntoblog/index.html#version-updates",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "11.3 Version Updates",
    "text": "11.3 Version Updates\nWhen you update the post:\n# Edit content\nvim analysis/paper/index.qmd\n\n# Update date (if using last-modified)\n# Or change manually: date: \"2025-12-15\"\n\n# Render and test\nquarto render analysis/paper/index.qmd\n\n# Commit update\ngit add .\ngit commit -m \"Update post: [what changed]\"\ngit push"
  },
  {
    "objectID": "posts/markdowntoblog/index.html#symlink-issues",
    "href": "posts/markdowntoblog/index.html#symlink-issues",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "12.1 Symlink Issues",
    "text": "12.1 Symlink Issues\nSymlink appears broken after cloning:\n# Git may not have recreated symlinks correctly\n# Recreate manually:\ncd ~/prj/qblog/posts/my_blog_post\nrm index.qmd media figures data\nln -s analysis/paper/index.qmd index.qmd\nln -s analysis/media media\nln -s analysis/figures figures\nln -s analysis/data data"
  },
  {
    "objectID": "posts/markdowntoblog/index.html#yaml-errors",
    "href": "posts/markdowntoblog/index.html#yaml-errors",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "12.2 YAML Errors",
    "text": "12.2 YAML Errors\n“YAML parsing error”:\n# Check for tabs (they're not allowed)\ncat -A analysis/paper/index.qmd | head -30\n# Tabs show as ^I, spaces don't\n\n# Fix by using your editor in spaces-only mode\nvim analysis/paper/index.qmd\n:set expandtab"
  },
  {
    "objectID": "posts/markdowntoblog/index.html#quarto-issues",
    "href": "posts/markdowntoblog/index.html#quarto-issues",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "12.3 Quarto Issues",
    "text": "12.3 Quarto Issues\n“Quarto executable not found”:\n# Install Quarto\n# macOS:\nbrew install quarto\n\n# Or download from: https://quarto.org/docs/get-started/\nCode blocks not syntax-highlighted:\n# Wrong:\n```shell\ncode"
  },
  {
    "objectID": "posts/markdowntoblog/index.html#structure-used-here",
    "href": "posts/markdowntoblog/index.html#structure-used-here",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "14.1 Structure Used Here",
    "text": "14.1 Structure Used Here\nmarkdown_to_blog/\n├── index.qmd (symlink) → analysis/paper/index.qmd\n├── media/ (symlink) → analysis/media/\n├── analysis/\n│   ├── paper/\n│   │   └── index.qmd (THIS FILE, ~3000 words)\n│   └── media/\n│       └── images/\n│           ├── README.md (document image sources)\n│           └── [hero and concept images]"
  },
  {
    "objectID": "posts/markdowntoblog/index.html#what-was-converted",
    "href": "posts/markdowntoblog/index.html#what-was-converted",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "14.2 What Was Converted",
    "text": "14.2 What Was Converted\nSource: markdown_to_blogpost.md (process documentation, ~1300 lines)\nConverted to: This blog post (teaching audience how to do it)"
  },
  {
    "objectID": "posts/markdowntoblog/index.html#key-decisions-made",
    "href": "posts/markdowntoblog/index.html#key-decisions-made",
    "title": "From Markdown to Blog Post: Converting Documentation into a Reproducible ZZCOLLAB Workflow",
    "section": "14.3 Key Decisions Made",
    "text": "14.3 Key Decisions Made\n\n\n\nDecision\nOutcome\n\n\n\n\nSlug\nmarkdown_to_blog (kebab-case)\n\n\nProfile\nubuntu_standard_publishing\n\n\nContent\nEducational (teaching, not documenting)\n\n\nStructure\nProgressive (overview → details → checklist)\n\n\nExamples\nReference actual GitHub archive post"
  },
  {
    "objectID": "posts/palmerpenguinspart1/index.html",
    "href": "posts/palmerpenguinspart1/index.html",
    "title": "Palmer Penguins Data Analysis Series (Part 1): Exploratory Data Analysis and Simple Regression",
    "section": "",
    "text": "Curious Adelie penguins beginning their data science journey - because every great analysis starts with getting to know your data!\nPhoto: African penguins at Boulders Beach, South Africa. Licensed under CC BY 2.0 via Wikimedia Commons"
  },
  {
    "objectID": "posts/palmerpenguinspart1/index.html#species-and-morphometric-overview",
    "href": "posts/palmerpenguinspart1/index.html#species-and-morphometric-overview",
    "title": "Palmer Penguins Data Analysis Series (Part 1): Exploratory Data Analysis and Simple Regression",
    "section": "4.1 Species and Morphometric Overview",
    "text": "4.1 Species and Morphometric Overview\nLet’s understand our penguin community composition and key measurements:\n\n# Species summary with key statistics\nspecies_summary &lt;- penguins_clean %&gt;%\n  group_by(species) %&gt;%\n  summarise(\n    n = n(),\n    body_mass_mean = round(mean(body_mass_g), 0),\n    flipper_length_mean = round(mean(flipper_length_mm), 1),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(percentage = round(n / sum(n) * 100, 1))\n\nkable(species_summary, \n      caption = \"Species Distribution and Key Morphometrics\",\n      col.names = c(\"Species\", \"N\", \"Body Mass (g)\", \"Flipper Length (mm)\", \"% of Dataset\"))\n\n\nSpecies Distribution and Key Morphometrics\n\n\nSpecies\nN\nBody Mass (g)\nFlipper Length (mm)\n% of Dataset\n\n\n\n\nAdelie\n146\n3706\n190.1\n43.8\n\n\nChinstrap\n68\n3733\n195.8\n20.4\n\n\nGentoo\n119\n5092\n217.2\n35.7\n\n\n\n\n# Combined visualization: species distribution and key relationships\np_species &lt;- ggplot(species_summary, aes(x = species, y = n, fill = species)) +\n  geom_col(alpha = 0.8) +\n  geom_text(aes(label = paste0(n, \"\\n(\", percentage, \"%)\")), \n            vjust = -0.5, size = 3.5) +\n  scale_fill_manual(values = penguin_colors) +\n  labs(title = \"Species Distribution\", x = \"Species\", y = \"Count\") +\n  theme_minimal() + theme(legend.position = \"none\")\n\n# Flipper length vs body mass by species\np_relationship &lt;- ggplot(penguins_clean, aes(x = flipper_length_mm, y = body_mass_g, color = species)) +\n  geom_point(alpha = 0.7, size = 1.5) +\n  geom_smooth(method = \"lm\", se = FALSE, size = 0.8) +\n  scale_color_manual(values = penguin_colors) +\n  labs(title = \"Flipper Length vs Body Mass\", \n       x = \"Flipper Length (mm)\", y = \"Body Mass (g)\", color = \"Species\") +\n  theme_minimal()\n\n# Combine plots\neda_overview &lt;- p_species + p_relationship\nprint(eda_overview)\n\n\n\n\n\n\n\n# Save the plot\nggsave(\"eda-overview.png\", plot = eda_overview, width = 10, height = 5, dpi = 300)\n\n\n\n\nSpecies distribution and morphometric relationship overview showing sample sizes and the key flipper-body mass relationship across species"
  },
  {
    "objectID": "posts/palmerpenguinspart1/index.html#building-and-interpreting-the-model",
    "href": "posts/palmerpenguinspart1/index.html#building-and-interpreting-the-model",
    "title": "Palmer Penguins Data Analysis Series (Part 1): Exploratory Data Analysis and Simple Regression",
    "section": "7.1 Building and Interpreting the Model",
    "text": "7.1 Building and Interpreting the Model\n\n# Fit simple linear regression model\nsimple_model &lt;- lm(body_mass_g ~ flipper_length_mm, data = penguins_clean)\n\n# Extract coefficients with confidence intervals\nmodel_coefficients &lt;- tidy(simple_model, conf.int = TRUE)\nmodel_metrics &lt;- glance(simple_model)\n\n# Display key results\ncat(\"📊 Simple Linear Model Results:\\n\")\n\n📊 Simple Linear Model Results:\n\ncat(\"===============================\\n\")\n\n===============================\n\ncat(sprintf(\"R-squared: %.3f (%.1f%% of variance explained)\\n\", \n            model_metrics$r.squared, model_metrics$r.squared * 100))\n\nR-squared: 0.762 (76.2% of variance explained)\n\ncat(sprintf(\"RMSE: %.1f grams\\n\", sigma(simple_model)))\n\nRMSE: 393.3 grams\n\ncat(sprintf(\"F-statistic: %.1f (p &lt; 0.001)\\n\", model_metrics$statistic))\n\nF-statistic: 1060.3 (p &lt; 0.001)\n\n# Model equation with confidence intervals\nintercept &lt;- model_coefficients$estimate[1]\nslope &lt;- model_coefficients$estimate[2]\nslope_ci_lower &lt;- model_coefficients$conf.low[2]\nslope_ci_upper &lt;- model_coefficients$conf.high[2]\n\ncat(\"\\n🧮 Model Equation:\\n\")\n\n\n🧮 Model Equation:\n\ncat(sprintf(\"Body Mass = %.1f + %.1f × Flipper Length\\n\", intercept, slope))\n\nBody Mass = -5872.1 + 50.2 × Flipper Length\n\ncat(sprintf(\"Slope 95%% CI: [%.1f, %.1f] grams/mm\\n\", slope_ci_lower, slope_ci_upper))\n\nSlope 95% CI: [47.1, 53.2] grams/mm\n\n# Generate predictions with confidence intervals\nnew_data &lt;- tibble(flipper_length_mm = c(180, 200, 220))\npredictions &lt;- predict(simple_model, newdata = new_data, interval = \"confidence\")\n\ncat(\"\\n📝 Example Predictions (95% CI):\\n\")\n\n\n📝 Example Predictions (95% CI):\n\nfor(i in 1:nrow(new_data)) {\n  cat(sprintf(\"• %dmm flippers: %.0f g [%.0f, %.0f]\\n\", \n              new_data$flipper_length_mm[i], \n              predictions[i, \"fit\"], \n              predictions[i, \"lwr\"], \n              predictions[i, \"upr\"]))\n}\n\n• 180mm flippers: 3155 g [3079, 3232]\n• 200mm flippers: 4159 g [4116, 4201]\n• 220mm flippers: 5162 g [5090, 5233]\n\n# Visualize model with confidence bands\nmodel_plot &lt;- ggplot(penguins_clean, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(aes(color = species), alpha = 0.6) +\n  geom_smooth(method = \"lm\", color = \"black\", fill = \"gray80\") +\n  scale_color_manual(values = penguin_colors) +\n  labs(title = \"Simple Linear Regression: Body Mass ~ Flipper Length\",\n       subtitle = \"Gray band shows 95% confidence interval\",\n       x = \"Flipper Length (mm)\", y = \"Body Mass (g)\", color = \"Species\") +\n  theme_minimal()\n\nprint(model_plot)\n\n\n\n\n\n\n\nggsave(\"simple-regression-model.png\", plot = model_plot, width = 8, height = 5, dpi = 300)\n\n\n\n\nSimple linear regression model showing body mass predicted by flipper length with 95% confidence interval"
  },
  {
    "objectID": "posts/palmerpenguinspart1/index.html#statistical-limitations",
    "href": "posts/palmerpenguinspart1/index.html#statistical-limitations",
    "title": "Palmer Penguins Data Analysis Series (Part 1): Exploratory Data Analysis and Simple Regression",
    "section": "8.1 Statistical Limitations",
    "text": "8.1 Statistical Limitations\n\n# Model diagnostic checks\npenguins_with_predictions &lt;- penguins_clean %&gt;%\n  mutate(\n    predicted = predict(simple_model),\n    residuals = residuals(simple_model),\n    standardized_residuals = rstandard(simple_model)\n  )\n\n# Check for outliers and influential points\noutliers &lt;- which(abs(penguins_with_predictions$standardized_residuals) &gt; 2.5)\ncat(\"⚠️  Model Assumption Checks:\\n\")\n\n⚠️  Model Assumption Checks:\n\ncat(sprintf(\"• Potential outliers: %d observations (&gt;2.5 SD from mean)\\n\", length(outliers)))\n\n• Potential outliers: 5 observations (&gt;2.5 SD from mean)\n\ncat(sprintf(\"• Residual standard error: %.1f grams\\n\", sigma(simple_model)))\n\n• Residual standard error: 393.3 grams\n\n# Residuals diagnostic plot\ndiagnostic_plot &lt;- ggplot(penguins_with_predictions, aes(x = predicted, y = standardized_residuals)) +\n  geom_point(aes(color = species), alpha = 0.6) +\n  geom_hline(yintercept = c(-2, 0, 2), linetype = c(\"dashed\", \"solid\", \"dashed\"), \n             color = c(\"red\", \"black\", \"red\")) +\n  scale_color_manual(values = penguin_colors) +\n  labs(title = \"Model Residuals Diagnostic\",\n       subtitle = \"Species clustering suggests missing predictors\",\n       x = \"Predicted Body Mass (g)\", y = \"Standardized Residuals\", color = \"Species\") +\n  theme_minimal()\n\nprint(diagnostic_plot)\n\n\n\n\n\n\n\nggsave(\"model-diagnostics.png\", plot = diagnostic_plot, width = 8, height = 5, dpi = 300)\n\n\n\n\nModel diagnostic plot showing residuals clustered by species, indicating model limitations"
  },
  {
    "objectID": "posts/palmerpenguinspart1/index.html#key-limitations",
    "href": "posts/palmerpenguinspart1/index.html#key-limitations",
    "title": "Palmer Penguins Data Analysis Series (Part 1): Exploratory Data Analysis and Simple Regression",
    "section": "8.2 Key Limitations",
    "text": "8.2 Key Limitations\n\nSimpson’s Paradox Risk: The model ignores species differences, potentially masking important biological relationships\nModel Assumptions:\n\nLinear relationship assumption appears reasonable\nResidual clustering by species indicates missing predictors\nHomoscedasticity assumption may be violated across species\n\nTemporal Generalizability: Data spans 2007-2009; climate change may affect current relationships\nGeographic Scope: Limited to Palmer Station region; may not generalize to other penguin populations\nMeasurement Precision: Morphometric measurements have inherent measurement error not captured in model\nBiological Constraints: Model predictions outside observed flipper length range (172-231mm) should be interpreted cautiously"
  },
  {
    "objectID": "posts/palmerpenguinspart1/index.html#real-world-applications",
    "href": "posts/palmerpenguinspart1/index.html#real-world-applications",
    "title": "Palmer Penguins Data Analysis Series (Part 1): Exploratory Data Analysis and Simple Regression",
    "section": "9.1 Real-World Applications",
    "text": "9.1 Real-World Applications\nOur simple regression model has several practical applications in Antarctic research:\n\n# Calculate effect sizes and practical significance\neffect_size &lt;- slope / sd(penguins_clean$body_mass_g)\ncat(\"🌍 Practical Applications:\\n\")\n\n🌍 Practical Applications:\n\ncat(sprintf(\"• Field Assessment: Flipper measurements can estimate body condition (effect size: %.2f)\\n\", effect_size))\n\n• Field Assessment: Flipper measurements can estimate body condition (effect size: 0.06)\n\ncat(sprintf(\"• Population Monitoring: Track penguin health trends using flipper-mass relationships\\n\"))\n\n• Population Monitoring: Track penguin health trends using flipper-mass relationships\n\ncat(sprintf(\"• Climate Research: Changes in morphometric relationships may indicate environmental stress\\n\"))\n\n• Climate Research: Changes in morphometric relationships may indicate environmental stress\n\ncat(sprintf(\"• Conservation Planning: Identify underweight individuals for targeted intervention\\n\"))\n\n• Conservation Planning: Identify underweight individuals for targeted intervention\n\n# Practical thresholds based on model\nlow_threshold &lt;- quantile(penguins_clean$body_mass_g, 0.25)\nhigh_threshold &lt;- quantile(penguins_clean$body_mass_g, 0.75)\n\ncat(\"\\n📊 Clinical Thresholds:\\n\")\n\n\n📊 Clinical Thresholds:\n\ncat(sprintf(\"• Low body condition: &lt;%.0f g (based on 25th percentile)\\n\", low_threshold))\n\n• Low body condition: &lt;3550 g (based on 25th percentile)\n\ncat(sprintf(\"• Normal range: %.0f-%.0f g\\n\", low_threshold, high_threshold))\n\n• Normal range: 3550-4775 g\n\ncat(sprintf(\"• High body condition: &gt;%.0f g (based on 75th percentile)\\n\", high_threshold))\n\n• High body condition: &gt;4775 g (based on 75th percentile)"
  },
  {
    "objectID": "posts/palmerpenguinspart1/index.html#what-weve-learned-in-part-1",
    "href": "posts/palmerpenguinspart1/index.html#what-weve-learned-in-part-1",
    "title": "Palmer Penguins Data Analysis Series (Part 1): Exploratory Data Analysis and Simple Regression",
    "section": "10.1 What We’ve Learned in Part 1",
    "text": "10.1 What We’ve Learned in Part 1\n\nStrong Predictive Relationship: Flipper length explains 76% of body mass variance (R² = 0.762), providing a reliable field assessment tool\nSpecies-Specific Patterns: Residual clustering by species suggests important biological differences not captured by flipper length alone\nModel Performance: RMSE of 393g indicates reasonable prediction accuracy for most applications\nResearch Implications: Simple morphometric relationships can support field research and conservation efforts"
  },
  {
    "objectID": "posts/palmerpenguinspart1/index.html#looking-ahead-to-part-2",
    "href": "posts/palmerpenguinspart1/index.html#looking-ahead-to-part-2",
    "title": "Palmer Penguins Data Analysis Series (Part 1): Exploratory Data Analysis and Simple Regression",
    "section": "10.2 Looking Ahead to Part 2",
    "text": "10.2 Looking Ahead to Part 2\nOur residual analysis reveals clear opportunities for improvement through:\n\nSpecies Integration: Accounting for biological differences between penguin species\nMultiple Predictors: Incorporating bill measurements for enhanced accuracy\n\nInteraction Effects: Exploring how predictors work together\nModel Validation: Comparing simple vs. complex model performance\n\n\n\n\n\n\n\nTip🎯 Preview: Dramatic Model Improvement\n\n\n\nIn Part 2, adding species information will improve our model’s R² from 0.762 to over 0.860 - demonstrating why biological context matters in ecological modeling!"
  },
  {
    "objectID": "posts/palmerpenguinspart3/index.html",
    "href": "posts/palmerpenguinspart3/index.html",
    "title": "Palmer Penguins Data Analysis Series (Part 3): Advanced Models and Cross-Validation",
    "section": "",
    "text": "A tech-savvy penguin with a laptop, diving deep into advanced modeling techniques and cross-validation!\nPhoto: African penguins at Boulders Beach, South Africa. Licensed under CC BY 2.0 via Wikimedia Commons"
  },
  {
    "objectID": "posts/palmerpenguinspart3/index.html#setting-up-cross-validation",
    "href": "posts/palmerpenguinspart3/index.html#setting-up-cross-validation",
    "title": "Palmer Penguins Data Analysis Series (Part 3): Advanced Models and Cross-Validation",
    "section": "3.1 Setting Up Cross-Validation",
    "text": "3.1 Setting Up Cross-Validation\n\nset.seed(42)  # For reproducible results\n\n# Set up 10-fold cross-validation\ntrain_control &lt;- trainControl(\n  method = \"cv\",\n  number = 10,\n  savePredictions = \"final\",\n  verboseIter = FALSE\n)\n\ncat(\"🔄 Cross-Validation Setup:\\n\")\n\n🔄 Cross-Validation Setup:\n\ncat(\"==========================\\n\")\n\n==========================\n\ncat(\"Method: 10-fold cross-validation\\n\")\n\nMethod: 10-fold cross-validation\n\ncat(\"Folds: 10\\n\")\n\nFolds: 10\n\ncat(\"Seed: 42 (for reproducibility)\\n\")\n\nSeed: 42 (for reproducibility)\n\ncat(\"Predictions saved: Yes\\n\")\n\nPredictions saved: Yes"
  },
  {
    "objectID": "posts/palmerpenguinspart3/index.html#cross-validating-our-existing-models",
    "href": "posts/palmerpenguinspart3/index.html#cross-validating-our-existing-models",
    "title": "Palmer Penguins Data Analysis Series (Part 3): Advanced Models and Cross-Validation",
    "section": "3.2 Cross-Validating Our Existing Models",
    "text": "3.2 Cross-Validating Our Existing Models\n\n# Cross-validate all three models from Parts 1-2\ncv_simple &lt;- train(body_mass_g ~ flipper_length_mm, data = penguins_clean, method = \"lm\", trControl = train_control)\ncv_multiple &lt;- train(body_mass_g ~ bill_length_mm + bill_depth_mm + flipper_length_mm, data = penguins_clean, method = \"lm\", trControl = train_control)\ncv_species &lt;- train(body_mass_g ~ bill_length_mm + bill_depth_mm + flipper_length_mm + species, data = penguins_clean, method = \"lm\", trControl = train_control)\n\n# Cross-validation results with confidence intervals\ncat(\"Cross-Validation Results (Mean ± 95% CI):\\n\")\n\nCross-Validation Results (Mean ± 95% CI):\n\ncat(\"==========================================\\n\")\n\n==========================================\n\ncat(sprintf(\"Simple: RMSE %.1f ± %.1f, R² %.3f [%.3f-%.3f]\\n\",\n            cv_simple$results$RMSE, 1.96*sd(cv_simple$resample$RMSE),\n            cv_simple$results$Rsquared, \n            quantile(cv_simple$resample$Rsquared, 0.025),\n            quantile(cv_simple$resample$Rsquared, 0.975)))\n\nSimple: RMSE 390.9 ± 105.8, R² 0.775 [0.734-0.839]\n\ncat(sprintf(\"Species: RMSE %.1f ± %.1f, R² %.3f [%.3f-%.3f]\\n\",\n            cv_species$results$RMSE, 1.96*sd(cv_species$resample$RMSE),\n            cv_species$results$Rsquared,\n            quantile(cv_species$resample$Rsquared, 0.025),\n            quantile(cv_species$resample$Rsquared, 0.975)))\n\nSpecies: RMSE 315.7 ± 63.0, R² 0.856 [0.830-0.889]\n\n\nOur cross-validation reveals that the species model from Part 2 maintains excellent performance on unseen data, with minimal overfitting.\n “What if the relationships aren’t perfectly straight lines?”"
  },
  {
    "objectID": "posts/palmerpenguinspart5/index.html",
    "href": "posts/palmerpenguinspart5/index.html",
    "title": "Palmer Penguins Data Analysis Series (Part 5): Random Forest vs Linear Models - The Final Comparison",
    "section": "",
    "text": "Two penguins at a crossroads - one holding a linear regression equation, the other holding a decision tree, representing the classic interpretability vs performance tradeoff!\nPhoto: African penguins at Boulders Beach, South Africa. Licensed under CC BY 2.0 via Wikimedia Commons"
  },
  {
    "objectID": "posts/palmerpenguinspart5/index.html#key-findings",
    "href": "posts/palmerpenguinspart5/index.html#key-findings",
    "title": "Palmer Penguins Data Analysis Series (Part 5): Random Forest vs Linear Models - The Final Comparison",
    "section": "7.1 Key Findings",
    "text": "7.1 Key Findings\n\ncat(\"Palmer Penguins Series - Final Results:\\n\")\n\nPalmer Penguins Series - Final Results:\n\ncat(\"======================================\\n\")\n\n======================================\n\ncat(\"Part 1: Simple regression → R² = 0.759 (flipper length only)\\n\")\n\nPart 1: Simple regression → R² = 0.759 (flipper length only)\n\ncat(\"Part 2: Added species → R² = 0.863 (major improvement)\\n\")\n\nPart 2: Added species → R² = 0.863 (major improvement)\n\ncat(\"Part 3: Cross-validation → Confirmed robust generalization\\n\")\n\nPart 3: Cross-validation → Confirmed robust generalization\n\ncat(\"Part 4: Diagnostics → All assumptions satisfied\\n\")\n\nPart 4: Diagnostics → All assumptions satisfied\n\ncat(\"Part 5: RF comparison → Minimal gain (R² = 0.877) vs interpretability loss\\n\")\n\nPart 5: RF comparison → Minimal gain (R² = 0.877) vs interpretability loss\n\ncat(\"\\nBiological Insights:\\n\")\n\n\nBiological Insights:\n\ncat(\"• Flipper length strongest predictor across all models\\n\")\n\n• Flipper length strongest predictor across all models\n\ncat(\"• Species differences substantial (Gentoo ~1400g heavier than Adelie)\\n\")\n\n• Species differences substantial (Gentoo ~1400g heavier than Adelie)\n\ncat(\"• Morphometric relationships consistent across species\\n\")\n\n• Morphometric relationships consistent across species\n\ncat(\"• Linear assumptions well-satisfied for penguin morphometrics\\n\")\n\n• Linear assumptions well-satisfied for penguin morphometrics\n\ncat(\"\\nMethodological Insights:\\n\")\n\n\nMethodological Insights:\n\ncat(\"• Biological context (species) crucial for accurate predictions\\n\")\n\n• Biological context (species) crucial for accurate predictions\n\ncat(\"• Cross-validation essential for honest performance assessment\\n\")\n\n• Cross-validation essential for honest performance assessment\n\ncat(\"• Diagnostic checks confirm model appropriateness\\n\")\n\n• Diagnostic checks confirm model appropriateness\n\ncat(\"• Interpretability-performance tradeoff context-dependent\\n\")\n\n• Interpretability-performance tradeoff context-dependent\n\n\n\n\n\n\n\n\nImportant🎯 Final Recommendation\n\n\n\nFor Palmer penguins and similar ecological morphometric studies:\nPrimary Choice: Linear model with species information - Excellent performance (86% variance explained) - Full interpretability and statistical inference - Meets all statistical assumptions - Scientifically meaningful and communicable\nAlternative: Random forest when prediction accuracy is critical - Maximum performance (88% variance explained) - Handles complex interactions automatically - Robust to outliers and non-linearities"
  },
  {
    "objectID": "posts/penguins1zzcollab/analysis/paper/index.html",
    "href": "posts/penguins1zzcollab/analysis/paper/index.html",
    "title": "Palmer Penguins Data Analysis Series (Part 1): Exploratory Data Analysis and Simple Regression",
    "section": "",
    "text": "Curious Adelie penguins beginning their data science journey - because every great analysis starts with getting to know your data!\nPhoto: African penguins at Boulders Beach, South Africa. Licensed under CC BY 2.0 via Wikimedia Commons"
  },
  {
    "objectID": "posts/penguins1zzcollab/analysis/paper/index.html#species-and-morphometric-overview",
    "href": "posts/penguins1zzcollab/analysis/paper/index.html#species-and-morphometric-overview",
    "title": "Palmer Penguins Data Analysis Series (Part 1): Exploratory Data Analysis and Simple Regression",
    "section": "4.1 Species and Morphometric Overview",
    "text": "4.1 Species and Morphometric Overview\nLet’s understand our penguin community composition and key measurements:\n\n# Species summary with key statistics\nspecies_summary &lt;- penguins_clean %&gt;%\n  group_by(species) %&gt;%\n  summarise(\n    n = n(),\n    body_mass_mean = round(mean(body_mass_g), 0),\n    flipper_length_mean = round(mean(flipper_length_mm), 1),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(percentage = round(n / sum(n) * 100, 1))\n\nkable(species_summary, \n      caption = \"Species Distribution and Key Morphometrics\",\n      col.names = c(\"Species\", \"N\", \"Body Mass (g)\", \"Flipper Length (mm)\", \"% of Dataset\"))\n\n\nSpecies Distribution and Key Morphometrics\n\n\nSpecies\nN\nBody Mass (g)\nFlipper Length (mm)\n% of Dataset\n\n\n\n\nAdelie\n146\n3706\n190.1\n43.8\n\n\nChinstrap\n68\n3733\n195.8\n20.4\n\n\nGentoo\n119\n5092\n217.2\n35.7\n\n\n\n\n# Combined visualization: species distribution and key relationships\np_species &lt;- ggplot(species_summary, aes(x = species, y = n, fill = species)) +\n  geom_col(alpha = 0.8) +\n  geom_text(aes(label = paste0(n, \"\\n(\", percentage, \"%)\")), \n            vjust = -0.5, size = 3.5) +\n  scale_fill_manual(values = penguin_colors) +\n  labs(title = \"Species Distribution\", x = \"Species\", y = \"Count\") +\n  theme_minimal() + theme(legend.position = \"none\")\n\n# Flipper length vs body mass by species\np_relationship &lt;- ggplot(penguins_clean, aes(x = flipper_length_mm, y = body_mass_g, color = species)) +\n  geom_point(alpha = 0.7, size = 1.5) +\n  geom_smooth(method = \"lm\", se = FALSE, size = 0.8) +\n  scale_color_manual(values = penguin_colors) +\n  labs(title = \"Flipper Length vs Body Mass\", \n       x = \"Flipper Length (mm)\", y = \"Body Mass (g)\", color = \"Species\") +\n  theme_minimal()\n\n# Combine plots\neda_overview &lt;- p_species + p_relationship\nprint(eda_overview)\n\n\n\n\n\n\n\n# Save the plot\nggsave(\"eda-overview.png\", plot = eda_overview, width = 10, height = 5, dpi = 300)\n\n\n\n\nSpecies distribution and morphometric relationship overview showing sample sizes and the key flipper-body mass relationship across species"
  },
  {
    "objectID": "posts/penguins1zzcollab/analysis/paper/index.html#building-and-interpreting-the-model",
    "href": "posts/penguins1zzcollab/analysis/paper/index.html#building-and-interpreting-the-model",
    "title": "Palmer Penguins Data Analysis Series (Part 1): Exploratory Data Analysis and Simple Regression",
    "section": "7.1 Building and Interpreting the Model",
    "text": "7.1 Building and Interpreting the Model\n\n# Fit simple linear regression model\nsimple_model &lt;- lm(body_mass_g ~ flipper_length_mm, data = penguins_clean)\n\n# Extract coefficients with confidence intervals\nmodel_coefficients &lt;- tidy(simple_model, conf.int = TRUE)\nmodel_metrics &lt;- glance(simple_model)\n\n# Display key results\ncat(\"📊 Simple Linear Model Results:\\n\")\n\n📊 Simple Linear Model Results:\n\ncat(\"===============================\\n\")\n\n===============================\n\ncat(sprintf(\"R-squared: %.3f (%.1f%% of variance explained)\\n\", \n            model_metrics$r.squared, model_metrics$r.squared * 100))\n\nR-squared: 0.762 (76.2% of variance explained)\n\ncat(sprintf(\"RMSE: %.1f grams\\n\", sigma(simple_model)))\n\nRMSE: 393.3 grams\n\ncat(sprintf(\"F-statistic: %.1f (p &lt; 0.001)\\n\", model_metrics$statistic))\n\nF-statistic: 1060.3 (p &lt; 0.001)\n\n# Model equation with confidence intervals\nintercept &lt;- model_coefficients$estimate[1]\nslope &lt;- model_coefficients$estimate[2]\nslope_ci_lower &lt;- model_coefficients$conf.low[2]\nslope_ci_upper &lt;- model_coefficients$conf.high[2]\n\ncat(\"\\n🧮 Model Equation:\\n\")\n\n\n🧮 Model Equation:\n\ncat(sprintf(\"Body Mass = %.1f + %.1f × Flipper Length\\n\", intercept, slope))\n\nBody Mass = -5872.1 + 50.2 × Flipper Length\n\ncat(sprintf(\"Slope 95%% CI: [%.1f, %.1f] grams/mm\\n\", slope_ci_lower, slope_ci_upper))\n\nSlope 95% CI: [47.1, 53.2] grams/mm\n\n# Generate predictions with confidence intervals\nnew_data &lt;- tibble(flipper_length_mm = c(180, 200, 220))\npredictions &lt;- predict(simple_model, newdata = new_data, interval = \"confidence\")\n\ncat(\"\\n📝 Example Predictions (95% CI):\\n\")\n\n\n📝 Example Predictions (95% CI):\n\nfor(i in 1:nrow(new_data)) {\n  cat(sprintf(\"• %dmm flippers: %.0f g [%.0f, %.0f]\\n\", \n              new_data$flipper_length_mm[i], \n              predictions[i, \"fit\"], \n              predictions[i, \"lwr\"], \n              predictions[i, \"upr\"]))\n}\n\n• 180mm flippers: 3155 g [3079, 3232]\n• 200mm flippers: 4159 g [4116, 4201]\n• 220mm flippers: 5162 g [5090, 5233]\n\n# Visualize model with confidence bands\nmodel_plot &lt;- ggplot(penguins_clean, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(aes(color = species), alpha = 0.6) +\n  geom_smooth(method = \"lm\", color = \"black\", fill = \"gray80\") +\n  scale_color_manual(values = penguin_colors) +\n  labs(title = \"Simple Linear Regression: Body Mass ~ Flipper Length\",\n       subtitle = \"Gray band shows 95% confidence interval\",\n       x = \"Flipper Length (mm)\", y = \"Body Mass (g)\", color = \"Species\") +\n  theme_minimal()\n\nprint(model_plot)\n\n\n\n\n\n\n\nggsave(\"simple-regression-model.png\", plot = model_plot, width = 8, height = 5, dpi = 300)\n\n\n\n\nSimple linear regression model showing body mass predicted by flipper length with 95% confidence interval"
  },
  {
    "objectID": "posts/penguins1zzcollab/analysis/paper/index.html#statistical-limitations",
    "href": "posts/penguins1zzcollab/analysis/paper/index.html#statistical-limitations",
    "title": "Palmer Penguins Data Analysis Series (Part 1): Exploratory Data Analysis and Simple Regression",
    "section": "8.1 Statistical Limitations",
    "text": "8.1 Statistical Limitations\n\n# Model diagnostic checks\npenguins_with_predictions &lt;- penguins_clean %&gt;%\n  mutate(\n    predicted = predict(simple_model),\n    residuals = residuals(simple_model),\n    standardized_residuals = rstandard(simple_model)\n  )\n\n# Check for outliers and influential points\noutliers &lt;- which(abs(penguins_with_predictions$standardized_residuals) &gt; 2.5)\ncat(\"⚠️  Model Assumption Checks:\\n\")\n\n⚠️  Model Assumption Checks:\n\ncat(sprintf(\"• Potential outliers: %d observations (&gt;2.5 SD from mean)\\n\", length(outliers)))\n\n• Potential outliers: 5 observations (&gt;2.5 SD from mean)\n\ncat(sprintf(\"• Residual standard error: %.1f grams\\n\", sigma(simple_model)))\n\n• Residual standard error: 393.3 grams\n\n# Residuals diagnostic plot\ndiagnostic_plot &lt;- ggplot(penguins_with_predictions, aes(x = predicted, y = standardized_residuals)) +\n  geom_point(aes(color = species), alpha = 0.6) +\n  geom_hline(yintercept = c(-2, 0, 2), linetype = c(\"dashed\", \"solid\", \"dashed\"), \n             color = c(\"red\", \"black\", \"red\")) +\n  scale_color_manual(values = penguin_colors) +\n  labs(title = \"Model Residuals Diagnostic\",\n       subtitle = \"Species clustering suggests missing predictors\",\n       x = \"Predicted Body Mass (g)\", y = \"Standardized Residuals\", color = \"Species\") +\n  theme_minimal()\n\nprint(diagnostic_plot)\n\n\n\n\n\n\n\nggsave(\"model-diagnostics.png\", plot = diagnostic_plot, width = 8, height = 5, dpi = 300)\n\n\n\n\nModel diagnostic plot showing residuals clustered by species, indicating model limitations"
  },
  {
    "objectID": "posts/penguins1zzcollab/analysis/paper/index.html#key-limitations",
    "href": "posts/penguins1zzcollab/analysis/paper/index.html#key-limitations",
    "title": "Palmer Penguins Data Analysis Series (Part 1): Exploratory Data Analysis and Simple Regression",
    "section": "8.2 Key Limitations",
    "text": "8.2 Key Limitations\n\nSimpson’s Paradox Risk: The model ignores species differences, potentially masking important biological relationships\nModel Assumptions:\n\nLinear relationship assumption appears reasonable\nResidual clustering by species indicates missing predictors\nHomoscedasticity assumption may be violated across species\n\nTemporal Generalizability: Data spans 2007-2009; climate change may affect current relationships\nGeographic Scope: Limited to Palmer Station region; may not generalize to other penguin populations\nMeasurement Precision: Morphometric measurements have inherent measurement error not captured in model\nBiological Constraints: Model predictions outside observed flipper length range (172-231mm) should be interpreted cautiously"
  },
  {
    "objectID": "posts/penguins1zzcollab/analysis/paper/index.html#real-world-applications",
    "href": "posts/penguins1zzcollab/analysis/paper/index.html#real-world-applications",
    "title": "Palmer Penguins Data Analysis Series (Part 1): Exploratory Data Analysis and Simple Regression",
    "section": "9.1 Real-World Applications",
    "text": "9.1 Real-World Applications\nOur simple regression model has several practical applications in Antarctic research:\n\n# Calculate effect sizes and practical significance\neffect_size &lt;- slope / sd(penguins_clean$body_mass_g)\ncat(\"🌍 Practical Applications:\\n\")\n\n🌍 Practical Applications:\n\ncat(sprintf(\"• Field Assessment: Flipper measurements can estimate body condition (effect size: %.2f)\\n\", effect_size))\n\n• Field Assessment: Flipper measurements can estimate body condition (effect size: 0.06)\n\ncat(sprintf(\"• Population Monitoring: Track penguin health trends using flipper-mass relationships\\n\"))\n\n• Population Monitoring: Track penguin health trends using flipper-mass relationships\n\ncat(sprintf(\"• Climate Research: Changes in morphometric relationships may indicate environmental stress\\n\"))\n\n• Climate Research: Changes in morphometric relationships may indicate environmental stress\n\ncat(sprintf(\"• Conservation Planning: Identify underweight individuals for targeted intervention\\n\"))\n\n• Conservation Planning: Identify underweight individuals for targeted intervention\n\n# Practical thresholds based on model\nlow_threshold &lt;- quantile(penguins_clean$body_mass_g, 0.25)\nhigh_threshold &lt;- quantile(penguins_clean$body_mass_g, 0.75)\n\ncat(\"\\n📊 Clinical Thresholds:\\n\")\n\n\n📊 Clinical Thresholds:\n\ncat(sprintf(\"• Low body condition: &lt;%.0f g (based on 25th percentile)\\n\", low_threshold))\n\n• Low body condition: &lt;3550 g (based on 25th percentile)\n\ncat(sprintf(\"• Normal range: %.0f-%.0f g\\n\", low_threshold, high_threshold))\n\n• Normal range: 3550-4775 g\n\ncat(sprintf(\"• High body condition: &gt;%.0f g (based on 75th percentile)\\n\", high_threshold))\n\n• High body condition: &gt;4775 g (based on 75th percentile)"
  },
  {
    "objectID": "posts/penguins1zzcollab/analysis/paper/index.html#what-weve-learned-in-part-1",
    "href": "posts/penguins1zzcollab/analysis/paper/index.html#what-weve-learned-in-part-1",
    "title": "Palmer Penguins Data Analysis Series (Part 1): Exploratory Data Analysis and Simple Regression",
    "section": "10.1 What We’ve Learned in Part 1",
    "text": "10.1 What We’ve Learned in Part 1\n\nStrong Predictive Relationship: Flipper length explains 76% of body mass variance (R² = 0.762), providing a reliable field assessment tool\nSpecies-Specific Patterns: Residual clustering by species suggests important biological differences not captured by flipper length alone\nModel Performance: RMSE of 393g indicates reasonable prediction accuracy for most applications\nResearch Implications: Simple morphometric relationships can support field research and conservation efforts"
  },
  {
    "objectID": "posts/penguins1zzcollab/analysis/paper/index.html#looking-ahead-to-part-2",
    "href": "posts/penguins1zzcollab/analysis/paper/index.html#looking-ahead-to-part-2",
    "title": "Palmer Penguins Data Analysis Series (Part 1): Exploratory Data Analysis and Simple Regression",
    "section": "10.2 Looking Ahead to Part 2",
    "text": "10.2 Looking Ahead to Part 2\nOur residual analysis reveals clear opportunities for improvement through:\n\nSpecies Integration: Accounting for biological differences between penguin species\nMultiple Predictors: Incorporating bill measurements for enhanced accuracy\n\nInteraction Effects: Exploring how predictors work together\nModel Validation: Comparing simple vs. complex model performance\n\n\n\n\n\n\n\nTip🎯 Preview: Dramatic Model Improvement\n\n\n\nIn Part 2, adding species information will improve our model’s R² from 0.762 to over 0.860 - demonstrating why biological context matters in ecological modeling!"
  },
  {
    "objectID": "posts/rapidconversionRtoRmdp35/index.html",
    "href": "posts/rapidconversionRtoRmdp35/index.html",
    "title": "Rapid conversion of draft R scripts to formal Rmd reports",
    "section": "",
    "text": "Caption for your hero image - either conceptual or a preview of main results"
  },
  {
    "objectID": "posts/rapidconversionRtoRmdp35/index.html#subsection-1.1-more-specific-topic",
    "href": "posts/rapidconversionRtoRmdp35/index.html#subsection-1.1-more-specific-topic",
    "title": "Rapid conversion of draft R scripts to formal Rmd reports",
    "section": "3.1 Subsection 1.1: [More Specific Topic]",
    "text": "3.1 Subsection 1.1: [More Specific Topic]\n\n[More detailed explanation or variation]\n\n\n\nOptional supporting visualization"
  },
  {
    "objectID": "posts/rapidconversionRtoRmdp35/index.html#subsection-2.1-handling-edge-cases",
    "href": "posts/rapidconversionRtoRmdp35/index.html#subsection-2.1-handling-edge-cases",
    "title": "Rapid conversion of draft R scripts to formal Rmd reports",
    "section": "4.1 Subsection 2.1: [Handling Edge Cases]",
    "text": "4.1 Subsection 2.1: [Handling Edge Cases]\n\n[Discussion of potential issues and solutions]\n\n# Replace with your actual error handling code\n# tryCatch({\n#   risky_operation(data)\n# }, error = function(e) {\n#   message(\"Error handled: \", e$message)\n# })"
  },
  {
    "objectID": "posts/rapidconversionRtoRmdp35/index.html#appendix-a-complete-code",
    "href": "posts/rapidconversionRtoRmdp35/index.html#appendix-a-complete-code",
    "title": "Rapid conversion of draft R scripts to formal Rmd reports",
    "section": "13.1 Appendix A: Complete Code",
    "text": "13.1 Appendix A: Complete Code\n\n\n# Complete code for easy reproduction - replace with your actual code\n# library(your_packages)\n# data &lt;- load_your_data()\n# results &lt;- your_analysis(data)\n# plot(results)"
  },
  {
    "objectID": "posts/rapidconversionRtoRmdp35/index.html#appendix-b-mathematical-details",
    "href": "posts/rapidconversionRtoRmdp35/index.html#appendix-b-mathematical-details",
    "title": "Rapid conversion of draft R scripts to formal Rmd reports",
    "section": "13.2 Appendix B: Mathematical Details",
    "text": "13.2 Appendix B: Mathematical Details\n\n[Detailed mathematical explanations or derivations]"
  },
  {
    "objectID": "posts/rapidconversionRtoRmdp35/index.html#appendix-c-additional-data",
    "href": "posts/rapidconversionRtoRmdp35/index.html#appendix-c-additional-data",
    "title": "Rapid conversion of draft R scripts to formal Rmd reports",
    "section": "13.3 Appendix C: Additional Data",
    "text": "13.3 Appendix C: Additional Data\n\n[Additional tables, charts, or data summaries]\n\nHave questions or suggestions? Feel free to reach out on Twitter or LinkedIn. You can also find the complete code for this analysis on GitHub.\n\nAbout the Author: [Your name] is a [your role] specializing in [your expertise]. [Brief background and interests.]"
  },
  {
    "objectID": "posts/setupdotfilesongithub/index.html",
    "href": "posts/setupdotfilesongithub/index.html",
    "title": "Creating a GitHub Dotfiles Repository for Configuration Management",
    "section": "",
    "text": "dotfiles galore"
  },
  {
    "objectID": "posts/setupdotfilesongithub/index.html#creating-the-initial-repository",
    "href": "posts/setupdotfilesongithub/index.html#creating-the-initial-repository",
    "title": "Creating a GitHub Dotfiles Repository for Configuration Management",
    "section": "3.1 Creating the Initial Repository",
    "text": "3.1 Creating the Initial Repository\nStart by creating a new GitHub repository specifically for your dotfiles:\n# Create local directory\nmkdir ~/dotfiles && cd ~/dotfiles\n\n# Initialize git repository\ngit init\n\n# Create basic structure\nmkdir -p shell git editors system packages\n\n# Add README with basic documentation\necho \"# My Dotfiles\" &gt; README.md\necho \"Personal configuration files for development environment\" &gt;&gt; README.md\n\n\n\nGit workflow foundation - version control for configuration management\n\n\n3"
  },
  {
    "objectID": "posts/setupdotfilesongithub/index.html#handling-cross-platform-compatibility",
    "href": "posts/setupdotfilesongithub/index.html#handling-cross-platform-compatibility",
    "title": "Creating a GitHub Dotfiles Repository for Configuration Management",
    "section": "4.1 Handling Cross-Platform Compatibility",
    "text": "4.1 Handling Cross-Platform Compatibility\nDifferent operating systems require platform-specific configurations (Limoncelli, Hogan, and Chalup 2016). Use conditional logic to handle these differences gracefully:\n# Platform detection in shell scripts\ncase \"$OSTYPE\" in\n  darwin*)  # macOS\n    export BREW_PREFIX=\"/opt/homebrew\"\n    alias ls=\"ls -G\"\n    ;;\n  linux*)   # Linux\n    export BREW_PREFIX=\"/home/linuxbrew/.linuxbrew\"\n    alias ls=\"ls --color=auto\"\n    ;;\nesac\n\n# Conditional sourcing based on file existence\n[ -f \"$BREW_PREFIX/bin/brew\" ] && eval \"$(\"$BREW_PREFIX/bin/brew\" shellenv)\""
  },
  {
    "objectID": "posts/setupdotfilesongithub/index.html#basic-script-structure",
    "href": "posts/setupdotfilesongithub/index.html#basic-script-structure",
    "title": "Creating a GitHub Dotfiles Repository for Configuration Management",
    "section": "5.1 Basic Script Structure",
    "text": "5.1 Basic Script Structure\nFirst, establish error handling and logging functions:\n#!/bin/bash\n# install.sh - Main dotfiles installation script\n\nset -e  # Exit on any error\n\nlog() {\n    echo \"[INFO] $1\"\n}\n\nwarn() {\n    echo \"[WARN] $1\"\n}\n\nerror() {\n    echo \"[ERROR] $1\"\n}"
  },
  {
    "objectID": "posts/setupdotfilesongithub/index.html#safe-file-linking-function",
    "href": "posts/setupdotfilesongithub/index.html#safe-file-linking-function",
    "title": "Creating a GitHub Dotfiles Repository for Configuration Management",
    "section": "5.2 Safe File Linking Function",
    "text": "5.2 Safe File Linking Function\nThe core functionality creates symbolic links while preserving existing files:\n# Create symbolic links for configuration files\nlink_file() {\n    local src=\"$1\"\n    local dest=\"$2\"\n    \n    if [ -e \"$dest\" ]; then\n        warn \"$dest already exists, creating backup\"\n        mv \"$dest\" \"${dest}.backup.$(date +%Y%m%d_%H%M%S)\"\n    fi\n    \n    ln -sf \"$src\" \"$dest\"\n    log \"Linked $src -&gt; $dest\"\n}\nThis function automatically backs up any existing configuration files with a timestamp, ensuring you never lose your current setup."
  },
  {
    "objectID": "posts/setupdotfilesongithub/index.html#installation-logic",
    "href": "posts/setupdotfilesongithub/index.html#installation-logic",
    "title": "Creating a GitHub Dotfiles Repository for Configuration Management",
    "section": "5.3 Installation Logic",
    "text": "5.3 Installation Logic\nFinally, the main installation function ties everything together:\n# Main installation function\ninstall_dotfiles() {\n    local dotfiles_dir=\"$(cd \"$(dirname \"$0\")\" && pwd)\"\n    \n    log \"Installing dotfiles from $dotfiles_dir\"\n    \n    # Link shell configurations\n    link_file \"$dotfiles_dir/shell/zshrc\" \"$HOME/.zshrc\"\n    link_file \"$dotfiles_dir/shell/aliases\" \"$HOME/.aliases\"\n    link_file \"$dotfiles_dir/shell/functions\" \"$HOME/.functions\"\n    \n    # Link git configuration\n    link_file \"$dotfiles_dir/git/gitconfig\" \"$HOME/.gitconfig\"\n    \n    # Link editor configurations\n    link_file \"$dotfiles_dir/editors/vimrc\" \"$HOME/.vimrc\"\n    \n    log \"Dotfiles installation complete!\"\n    log \"Please restart your shell or run: source ~/.zshrc\"\n}\n\n# Run installation\ninstall_dotfiles\n\n\n\nDocumentation and automation workflow - streamlined setup processes\n\n\nJust as Quarto automates document generation, a well-crafted installation script automates environment setup, transforming complex manual processes into simple, repeatable workflows.\n4"
  },
  {
    "objectID": "posts/setupdotfilesongithub/index.html#package-management-integration",
    "href": "posts/setupdotfilesongithub/index.html#package-management-integration",
    "title": "Creating a GitHub Dotfiles Repository for Configuration Management",
    "section": "6.1 Package Management Integration",
    "text": "6.1 Package Management Integration\nAutomating software installation alongside configuration files creates a complete environment setup:\nPackage Management Integration:\n# Brewfile for macOS package management\nbrew \"git\"\nbrew \"vim\"\nbrew \"tmux\"\nbrew \"node\"\nbrew \"python@3.11\"\n\n# Cask applications\ncask \"visual-studio-code\"\ncask \"iterm2\"\ncask \"docker\"\n\n# Install packages automatically\nbrew bundle --file=packages/Brewfile"
  },
  {
    "objectID": "posts/setupdotfilesongithub/index.html#secure-configuration-management",
    "href": "posts/setupdotfilesongithub/index.html#secure-configuration-management",
    "title": "Creating a GitHub Dotfiles Repository for Configuration Management",
    "section": "6.2 Secure Configuration Management",
    "text": "6.2 Secure Configuration Management\nProtecting sensitive data while maintaining functionality requires careful planning:\nSecure Handling of Sensitive Configuration:\n# Use environment variables for sensitive data\n# In .zshrc:\nif [ -f ~/.env.local ]; then\n    export $(grep -v '^#' ~/.env.local | xargs)\nfi\n\n# .env.local (NOT in git repository):\n# GITHUB_TOKEN=your_token_here\n# AWS_ACCESS_KEY_ID=your_key_here\n\n# Alternative: Use git-crypt for encrypted files\ngit-crypt init\ngit-crypt add-gpg-user your-gpg-key-id\necho \"secrets/* filter=git-crypt diff=git-crypt\" &gt;&gt; .gitattributes"
  },
  {
    "objectID": "posts/setupdotfilesongithub/index.html#team-collaboration-features",
    "href": "posts/setupdotfilesongithub/index.html#team-collaboration-features",
    "title": "Creating a GitHub Dotfiles Repository for Configuration Management",
    "section": "6.3 Team Collaboration Features",
    "text": "6.3 Team Collaboration Features\nSharing dotfiles across teams requires additional automation and standardization:\nTeam Collaboration Features:\n# Makefile for common operations\n.PHONY: install update backup test\n\ninstall:\n    @echo \"Installing dotfiles...\"\n    ./install.sh\n\nupdate:\n    @echo \"Updating dotfiles repository...\"\n    git pull origin main\n    ./install.sh\n\nbackup:\n    @echo \"Creating backup of current configurations...\"\n    ./scripts/backup.sh\n\ntest:\n    @echo \"Testing dotfiles configuration...\"\n    ./scripts/test.sh"
  },
  {
    "objectID": "posts/setupdotfilesongithub/index.html#understanding-the-risks",
    "href": "posts/setupdotfilesongithub/index.html#understanding-the-risks",
    "title": "Creating a GitHub Dotfiles Repository for Configuration Management",
    "section": "8.1 Understanding the Risks",
    "text": "8.1 Understanding the Risks\n\n8.1.1 Security Limitations\n\nPublic Repository Risk: Any configuration data in public repos is visible to everyone - never include credentials, API keys, or personal information\nSSH Key Management: Private SSH keys should never be in dotfiles; use SSH agent forwarding or regenerate keys per machine\nCross-Platform Compatibility: Scripts may require platform-specific modifications for macOS and Linux\n\n\n\n8.1.2 Maintenance Considerations\n\nDependency Management: External tools and packages may change, breaking installation scripts\nBackup Conflicts: Automated backups of existing configs can accumulate over time, requiring periodic cleanup\nTesting Requirements: Configuration changes should be tested on both macOS and Linux before deployment\n\n\n\n8.1.3 Organizational Limitations\n\nPersonal vs. Team Configs: Individual preferences may conflict with team standards, requiring separate personal and shared repositories\nCompany Policies: Some organizations restrict public code repositories or require specific security measures\nScalability: Large teams may need more sophisticated configuration management tools beyond simple dotfiles repositories"
  },
  {
    "objectID": "posts/setupdotfilesongithub/index.html#foundational-resources",
    "href": "posts/setupdotfilesongithub/index.html#foundational-resources",
    "title": "Creating a GitHub Dotfiles Repository for Configuration Management",
    "section": "11.1 Foundational Resources",
    "text": "11.1 Foundational Resources\n\nEssential Reading:\n\nZach Holman (2014). “Dotfiles Are Meant to Be Forked”. GitHub Blog. https://zachholman.com/2010/08/dotfiles-are-meant-to-be-forked/\nAnish Athalye (2016). “Managing Your Dotfiles”. MIT CSAIL. https://www.anishathalye.com/2014/02/15/managing-your-dotfiles/\nMathias Bynens (2021). “macOS Defaults: Sensible Hacker Defaults for macOS”. https://mths.be/macos\n\nConfiguration Management Theory:\n\nKamp, P. H. (2011). “Configuration Management for System Administrators”. ACM Queue, 9(7), 20-26. https://doi.org/10.1145/2002268.2002271\nMorris, K. (2016). Infrastructure as Code: Managing Servers in the Cloud. O’Reilly Media.\nLimoncelli, T. A., Hogan, C. J., & Chalup, S. R. (2016). The Practice of System and Network Administration (3rd ed.). Addison-Wesley.\n\nVersion Control Best Practices:\n\nChacon, S., & Straub, B. (2014). Pro Git (2nd ed.). Apress. https://git-scm.com/book\nLoeliger, J., & McCullough, M. (2012). Version Control with Git (2nd ed.). O’Reilly Media."
  },
  {
    "objectID": "posts/setupdotfilesongithub/index.html#blog-posts-and-tutorials",
    "href": "posts/setupdotfilesongithub/index.html#blog-posts-and-tutorials",
    "title": "Creating a GitHub Dotfiles Repository for Configuration Management",
    "section": "11.2 Blog Posts and Tutorials",
    "text": "11.2 Blog Posts and Tutorials\n\nComprehensive Dotfiles Guides:\n\nAtlassian: “The best way to store your dotfiles: A bare Git repository” - Alternative storage approach using bare repositories\nGitHub Docs: “Creating a personal access token” - Secure GitHub authentication\nOh My Zsh: “Installing ZSH” - Popular Zsh framework with extensive plugin ecosystem\n\nPlatform-Specific Tutorials:\n\nHomebrew: “Installation and Usage” - macOS package manager integration\nArch Wiki: “Dotfiles” - Comprehensive Linux dotfiles documentation\n\nSecurity and Best Practices:\n\nGitHub: “Removing sensitive data from a repository” - Security remediation\nOWASP: “Secrets Management Cheat Sheet” - Security best practices\n1Password: “SSH & Git” - Secure SSH key management"
  },
  {
    "objectID": "posts/setupdotfilesongithub/index.html#technical-documentation",
    "href": "posts/setupdotfilesongithub/index.html#technical-documentation",
    "title": "Creating a GitHub Dotfiles Repository for Configuration Management",
    "section": "11.3 Technical Documentation",
    "text": "11.3 Technical Documentation\n\nShell and Terminal Documentation:\n\nBash Reference Manual - Complete Bash shell documentation\nZsh Documentation - Zsh shell manual and configuration guide\nGNU Make Manual - Makefile automation documentation\n\nGit and Version Control:\n\nGit Documentation - Official Git command reference\nGitHub CLI Manual - GitHub command-line tool documentation\nGitLab CI/CD Documentation - Continuous integration for dotfiles testing\n\nConfiguration Management Tools:\n\nAnsible Documentation - Infrastructure automation and configuration management\nTerraform Documentation - Infrastructure as code for cloud environments\nDocker Documentation - Containerized development environments"
  },
  {
    "objectID": "posts/setupdotfilesongithub/index.html#community-resources",
    "href": "posts/setupdotfilesongithub/index.html#community-resources",
    "title": "Creating a GitHub Dotfiles Repository for Configuration Management",
    "section": "11.4 Community Resources",
    "text": "11.4 Community Resources\n\nDotfiles Communities:\n\nr/dotfiles - Reddit community for sharing and discussing dotfiles\nGitHub Topics: Dotfiles - Curated collection of popular dotfiles repositories\nDotfiles.github.io - Community showcase and inspiration gallery\n\nDeveloper Forums:\n\nStack Overflow: Dotfiles - Technical troubleshooting and implementation questions\nUnix & Linux Stack Exchange - System configuration and shell scripting help\nServer Fault - System administration and configuration management\n\nPlatform-Specific Communities:\n\nHomebrew Discussions - macOS package management community\nOh My Zsh Community - Zsh configuration and plugin discussions\nr/vim - Editor configuration communities"
  },
  {
    "objectID": "posts/setupdotfilesongithub/index.html#popular-dotfiles-repositories",
    "href": "posts/setupdotfilesongithub/index.html#popular-dotfiles-repositories",
    "title": "Creating a GitHub Dotfiles Repository for Configuration Management",
    "section": "11.5 Popular Dotfiles Repositories",
    "text": "11.5 Popular Dotfiles Repositories\n\nExemplary Community Repositories:\n\nMathias Bynens’ dotfiles - Comprehensive macOS dotfiles with extensive documentation\nZach Holman’s dotfiles - Topic-based organization approach with automated setup\nPaul Irish’s dotfiles - Well-documented configurations for web development\n\nFramework-Based Approaches:\n\nOh My Zsh - Community-driven Zsh configuration framework\nPrezto - Configuration framework for Zsh with modules\nDotbot - Tool for bootstrapping dotfiles with declarative configuration"
  },
  {
    "objectID": "posts/setupdotfilesongithub/index.html#advanced-configuration-management",
    "href": "posts/setupdotfilesongithub/index.html#advanced-configuration-management",
    "title": "Creating a GitHub Dotfiles Repository for Configuration Management",
    "section": "11.6 Advanced Configuration Management",
    "text": "11.6 Advanced Configuration Management\n\nEnterprise-Level Solutions:\n\nPuppet Labs (2017). “Configuration Management Best Practices”. Puppet Documentation. https://puppet.com/docs/\nChef Software (2019). “Infrastructure Automation and DevOps”. Chef Documentation. https://docs.chef.io/\nRed Hat (2021). “Ansible Automation Platform”. Red Hat Documentation. https://docs.ansible.com/\n\nContainer-Based Development:\n\nDocker Inc. (2021). “Development Environments with Docker Compose”. Docker Documentation. https://docs.docker.com/compose/\nMicrosoft (2021). “Developing inside a Container”. Visual Studio Code Documentation. https://code.visualstudio.com/docs/remote/containers\nGitHub (2021). “GitHub Codespaces”. GitHub Documentation. https://docs.github.com/en/codespaces\n\n\n\nCitation Note: When using configurations or scripts from these resources, please provide appropriate attribution. For public dotfiles repositories, follow the repository’s license terms (typically MIT or Apache 2.0). Always review and understand configurations before implementing them in your environment."
  },
  {
    "objectID": "posts/setupdotfilesongithub/index.html#example-repository",
    "href": "posts/setupdotfilesongithub/index.html#example-repository",
    "title": "Creating a GitHub Dotfiles Repository for Configuration Management",
    "section": "12.1 Example Repository",
    "text": "12.1 Example Repository\n\nGitHub: Example dotfiles repository structure (Note: Replace with your actual repository)\nLicense: MIT License - Feel free to fork and modify\nCompatibility: Tested on macOS 12+, Ubuntu 20.04+"
  },
  {
    "objectID": "posts/setupdotfilesongithub/index.html#system-requirements",
    "href": "posts/setupdotfilesongithub/index.html#system-requirements",
    "title": "Creating a GitHub Dotfiles Repository for Configuration Management",
    "section": "12.2 System Requirements",
    "text": "12.2 System Requirements\n\nGit: Version 2.0 or higher\nShell: Bash 4.0+ or Zsh 5.0+\nGitHub Account: For repository hosting and collaboration\nCommand Line Tools: Platform-specific (Xcode Command Line Tools for macOS, build-essential for Ubuntu)"
  },
  {
    "objectID": "posts/setupdotfilesongithub/index.html#environment-information",
    "href": "posts/setupdotfilesongithub/index.html#environment-information",
    "title": "Creating a GitHub Dotfiles Repository for Configuration Management",
    "section": "12.3 Environment Information",
    "text": "12.3 Environment Information\n# Check your system compatibility\ngit --version\n$SHELL --version\nuname -a\necho $HOME"
  },
  {
    "objectID": "posts/setupdotfilesongithub/index.html#cross-platform-compatibility-patterns",
    "href": "posts/setupdotfilesongithub/index.html#cross-platform-compatibility-patterns",
    "title": "Creating a GitHub Dotfiles Repository for Configuration Management",
    "section": "13.1 Cross-Platform Compatibility Patterns",
    "text": "13.1 Cross-Platform Compatibility Patterns\nFor readers implementing cross-platform solutions, these detection patterns provide robust OS and package manager identification:\nPlatform Detection Patterns:\n# Detect operating system\ncase \"$OSTYPE\" in\n    darwin*)  OS=\"macos\" ;;\n    linux*)   OS=\"linux\" ;;\n    *)        OS=\"unknown\" ;;\nesac\n\n# Detect package manager\nif command -v brew &gt;/dev/null 2&gt;&1; then\n    PKG_MANAGER=\"brew\"\nelif command -v apt &gt;/dev/null 2&gt;&1; then\n    PKG_MANAGER=\"apt\"\nelif command -v yum &gt;/dev/null 2&gt;&1; then\n    PKG_MANAGER=\"yum\"\nelif command -v pacman &gt;/dev/null 2&gt;&1; then\n    PKG_MANAGER=\"pacman\"\nfi"
  },
  {
    "objectID": "posts/setupdotfilesongithub/index.html#security-review-checklist",
    "href": "posts/setupdotfilesongithub/index.html#security-review-checklist",
    "title": "Creating a GitHub Dotfiles Repository for Configuration Management",
    "section": "13.2 Security Review Checklist",
    "text": "13.2 Security Review Checklist\nBefore publishing your dotfiles repository, ensure you’ve addressed these security considerations:\nPre-Publication Security Review: - [ ] No SSH private keys (id_rsa, id_ed25519) - [ ] No API tokens or credentials - [ ] No hardcoded passwords - [ ] No personal information (real names, addresses) - [ ] .gitignore includes sensitive file patterns - [ ] Environment variables externalized to .env.local - [ ] SSH config excludes private key paths - [ ] Git config excludes email addresses"
  },
  {
    "objectID": "posts/setupdotfilesongithub/index.html#share-this-post",
    "href": "posts/setupdotfilesongithub/index.html#share-this-post",
    "title": "Creating a GitHub Dotfiles Repository for Configuration Management",
    "section": "13.3 Share This Post",
    "text": "13.3 Share This Post\nFound this helpful? Share it with your network:\n\nTwitter\nLinkedIn\nReddit"
  },
  {
    "objectID": "posts/setupdotfilesongithub/index.html#connect-and-discuss",
    "href": "posts/setupdotfilesongithub/index.html#connect-and-discuss",
    "title": "Creating a GitHub Dotfiles Repository for Configuration Management",
    "section": "13.4 Connect and Discuss",
    "text": "13.4 Connect and Discuss\nHave questions or suggestions? I’d love to hear from you:\n\nTwitter: @rgt47 - Quick questions and discussions\nLinkedIn: Ronald Glenn Thomas - Professional networking\nGitHub: rgt47 - Code, issues, and contributions\nEmail: Contact through website - Detailed inquiries\n\nComments are enabled below via Utterances - join the discussion!"
  },
  {
    "objectID": "posts/setupdotfilesongithub/index.html#about-the-author",
    "href": "posts/setupdotfilesongithub/index.html#about-the-author",
    "title": "Creating a GitHub Dotfiles Repository for Configuration Management",
    "section": "13.5 About the Author",
    "text": "13.5 About the Author\nRonald (Ryy) Glenn Thomas is a biostatistician and data scientist at UC San Diego, specializing in statistical computing, machine learning applications in healthcare, and reproducible research methods. He develops R packages and conducts research at the intersection of statistics, data science, and clinical research.\nConnect: Website | ORCID | Google Scholar"
  },
  {
    "objectID": "posts/setupdotfilesongithub/index.html#footnotes",
    "href": "posts/setupdotfilesongithub/index.html#footnotes",
    "title": "Creating a GitHub Dotfiles Repository for Configuration Management",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n“Your dotfiles are how you personalize your system; these are the files that make your shell yours.” —Zach Holman, GitHub (2014)↩︎\nStorage Best Practice: Notice that configuration files are stored without leading dots (e.g., vimrc instead of .vimrc). This makes them visible in file browsers, improves cross-platform compatibility, and creates cleaner GitHub repository displays. The installation script handles adding the dots when creating symlinks in your home directory.↩︎\nPro Tip: “Start simple. A few files in a git repo is all you need to get started.” —Anish Athalye, MIT CSAIL↩︎\nSecurity Alert: “Never commit secrets to version control. Use environment variables or external secret management.” —OWASP Security Guidelines↩︎\nInfrastructure Wisdom: “Configuration management is not about tools—it’s about discipline and consistency.” —Tom Limoncelli, The Practice of System Administration↩︎\nCommunity Insight: “The best part about dotfiles isn’t the files themselves—it’s learning from other people’s workflows.” —Paul Irish, Web Developer↩︎"
  },
  {
    "objectID": "posts/setupquartop01/index.html",
    "href": "posts/setupquartop01/index.html",
    "title": "Setting Up a Minimal Quarto Blog: A Learning Journey",
    "section": "",
    "text": "Getting started with Quarto blogging\nQuarto makes technical blogging surprisingly accessible"
  },
  {
    "objectID": "posts/setupquartop01/index.html#motivations",
    "href": "posts/setupquartop01/index.html#motivations",
    "title": "Setting Up a Minimal Quarto Blog: A Learning Journey",
    "section": "1.1 Motivations",
    "text": "1.1 Motivations\nWhy explore Quarto blogging? - Needed a platform for sharing lab research and tutorials - Wanted something that integrates seamlessly with R and RStudio - Required literate programming capabilities (mix code and narrative) - Preferred static site generation (fast, secure, easy hosting)"
  },
  {
    "objectID": "posts/setupquartop01/index.html#objectives",
    "href": "posts/setupquartop01/index.html#objectives",
    "title": "Setting Up a Minimal Quarto Blog: A Learning Journey",
    "section": "1.2 Objectives",
    "text": "1.2 Objectives\nWhat I wanted to accomplish: 1. Understand the minimal file structure needed for a Quarto blog 2. Learn the essential configuration options in _quarto.yml 3. Create a working blog with at least one post 4. Document the process for future reference\nDisclaimer: This is my learning journey with Quarto blogging. If you spot errors or have better approaches, please let me know! 💙"
  },
  {
    "objectID": "posts/setupquartop01/index.html#the-site-configuration",
    "href": "posts/setupquartop01/index.html#the-site-configuration",
    "title": "Setting Up a Minimal Quarto Blog: A Learning Journey",
    "section": "4.1 The Site Configuration",
    "text": "4.1 The Site Configuration\nFile: _quarto.yml\nproject:\n  type: website\n\nwebsite:\n  title: \"Thomas Lab\"\n  navbar:\n    left:\n      - href: posts/index.qmd\n        text: Blog\n\nformat:\n  html:\n    theme: cosmo\nThis is your main configuration file. It defines: - Project type (website) - Site title - Navigation structure - Default theme (cosmo is clean and professional)"
  },
  {
    "objectID": "posts/setupquartop01/index.html#the-home-page",
    "href": "posts/setupquartop01/index.html#the-home-page",
    "title": "Setting Up a Minimal Quarto Blog: A Learning Journey",
    "section": "4.2 The Home Page",
    "text": "4.2 The Home Page\nFile: index.qmd\n---\ntitle: \"Thomas Lab\"\n---\n\n**Director: Professor Ronald G. Thomas**\nSchool of Public Health\nUC San Diego\nLa Jolla, California\n\nFocused on new and useful data science technologies.\nSuper simple! Just basic markdown for your landing page."
  },
  {
    "objectID": "posts/setupquartop01/index.html#the-blog-listing-page",
    "href": "posts/setupquartop01/index.html#the-blog-listing-page",
    "title": "Setting Up a Minimal Quarto Blog: A Learning Journey",
    "section": "4.3 The Blog Listing Page",
    "text": "4.3 The Blog Listing Page\nFile: posts/index.qmd\n---\ntitle: \"Blog\"\nlisting: default\n---\nThe magic happens with listing: default - this tells Quarto to automatically create a list of all posts in the posts/ directory ✅"
  },
  {
    "objectID": "posts/setupquartop01/index.html#your-first-post",
    "href": "posts/setupquartop01/index.html#your-first-post",
    "title": "Setting Up a Minimal Quarto Blog: A Learning Journey",
    "section": "4.4 Your First Post",
    "text": "4.4 Your First Post\nFile: posts/post1.qmd\n---\ntitle: \"First Post\"\ndate: \"2025-11-17\"\n---\n\n# Introduction\n\nMinimal text for first post. This is all you need to get started!\nThat’s it! Just a title, date, and some content."
  },
  {
    "objectID": "posts/setupquartop01/index.html#limitations",
    "href": "posts/setupquartop01/index.html#limitations",
    "title": "Setting Up a Minimal Quarto Blog: A Learning Journey",
    "section": "8.1 Limitations",
    "text": "8.1 Limitations\nThis minimal approach has some constraints:\n\nNo advanced features: Missing tags, categories, search, RSS feeds\nBasic styling: Using default theme without customization\nLimited navigation: Only one blog link in navbar\nNo about page: Stripped out biographical content\nManual organization: No date-based or category-based post organization\n\nFor a personal blog or lab website, these limitations might not matter. For a professional publication, you’ll want more features."
  },
  {
    "objectID": "posts/setupquartop01/index.html#opportunities-for-improvement",
    "href": "posts/setupquartop01/index.html#opportunities-for-improvement",
    "title": "Setting Up a Minimal Quarto Blog: A Learning Journey",
    "section": "8.2 Opportunities for Improvement",
    "text": "8.2 Opportunities for Improvement\nIf I had more time, here’s what I’d add:\n\nCustom styling - Override theme CSS for lab branding\nCategories and tags - Better post organization and discovery\nSearch functionality - Built-in search for larger blogs\nRSS feed - Auto-generated feed for subscribers\nAbout page - Team bios and lab information\nSocial sharing - Make posts easy to share on Twitter/LinkedIn"
  },
  {
    "objectID": "posts/shareRcodeviadockerp25/analysis/claude_synth.html",
    "href": "posts/shareRcodeviadockerp25/analysis/claude_synth.html",
    "title": "Achieving Full Reproducibility in R: A Docker and renv Strategy",
    "section": "",
    "text": "Reproducibility stands as a cornerstone of professional data analysis, yet in practice, achieving it consistently with R workflows remains challenging. R projects frequently break when transferred between computers due to mismatched R versions or package dependencies, leaving developers in what is colloquially known as “dependency hell.” This white paper presents a comprehensive approach to solving this problem by combining two powerful tools: renv for R package management and Docker for containerizing the computing environment. Together, these tools ensure that an R workflow runs identically across different systems with the same packages, R version, and system libraries as the original setup."
  },
  {
    "objectID": "posts/shareRcodeviadockerp25/analysis/claude_synth.html#executive-summary",
    "href": "posts/shareRcodeviadockerp25/analysis/claude_synth.html#executive-summary",
    "title": "Achieving Full Reproducibility in R: A Docker and renv Strategy",
    "section": "",
    "text": "Reproducibility stands as a cornerstone of professional data analysis, yet in practice, achieving it consistently with R workflows remains challenging. R projects frequently break when transferred between computers due to mismatched R versions or package dependencies, leaving developers in what is colloquially known as “dependency hell.” This white paper presents a comprehensive approach to solving this problem by combining two powerful tools: renv for R package management and Docker for containerizing the computing environment. Together, these tools ensure that an R workflow runs identically across different systems with the same packages, R version, and system libraries as the original setup."
  },
  {
    "objectID": "posts/shareRcodeviadockerp25/analysis/claude_synth.html#introduction",
    "href": "posts/shareRcodeviadockerp25/analysis/claude_synth.html#introduction",
    "title": "Achieving Full Reproducibility in R: A Docker and renv Strategy",
    "section": "2 Introduction",
    "text": "2 Introduction\n\n2.1 The Challenge of Reproducibility in R\nR has become a standard tool for data science and statistical analysis across numerous disciplines. However, as R projects grow in complexity, they often develop intricate webs of dependencies that can make sharing and reproducing analyses difficult. Some common challenges include:\n\nDifferent R versions across machines\nIncompatible package versions\nMissing system-level dependencies\nOperating system differences\nConflicts with other installed packages\n\nThese challenges often manifest as the frustrating “it works on my machine” problem, where analysis code runs perfectly for the original author but fails when others attempt to use it. This undermines the scientific and collaborative potential of R-based analyses.\n\n\n2.2 A Two-Level Solution\nTo address these challenges comprehensively, we need to tackle reproducibility at two distinct levels:\n\nPackage-level reproducibility: Ensuring exact package versions and dependencies are maintained\nSystem-level reproducibility: Guaranteeing consistent R versions, operating system, and system libraries\n\nThe strategy presented in this white paper leverages renv for package-level consistency and Docker for system-level consistency. When combined, they provide a robust framework for end-to-end reproducible R workflows."
  },
  {
    "objectID": "posts/shareRcodeviadockerp25/analysis/claude_synth.html#renv-package-level-reproducibility",
    "href": "posts/shareRcodeviadockerp25/analysis/claude_synth.html#renv-package-level-reproducibility",
    "title": "Achieving Full Reproducibility in R: A Docker and renv Strategy",
    "section": "3 renv: Package-Level Reproducibility",
    "text": "3 renv: Package-Level Reproducibility\n\n3.1 What is renv?\nrenv (Reproducible Environment) is an R package designed to create isolated, project-specific library environments. Instead of relying on a shared system-wide R library that might change over time, renv gives each project its own separate collection of packages with specific versions.\n\n\n3.2 Key Features of renv\n\nIsolated project library: renv creates a project-specific library (typically in renv/library) containing only the packages used by that project. This isolation ensures that updates or changes to packages in one project won’t affect others.\nLockfile for dependencies: When you finish installing or updating packages, renv::snapshot() produces a renv.lock file - a JSON document listing each package and its exact version and source. This lockfile is designed to be committed to version control and shared with collaborators.\nEnvironment restoration: On a new machine (or when reproducing past results), renv::restore() installs the exact versions of packages specified in the lockfile. This creates an R package environment identical to the one that created the lockfile, provided the same R version is available.\n\n\n\n3.3 Basic renv Workflow\nThe typical workflow with renv involves:\n# One-time installation of renv\ninstall.packages(\"renv\")\n\n# Initialize renv for the project\nrenv::init()  # Creates renv infrastructure\n\n# Install project-specific packages\n# ...\n\n# Save the package state to renv.lock\nrenv::snapshot()\n\n# Later or on another system...\nrenv::restore()  # Restore packages from renv.lock\nWhile renv effectively handles package dependencies, it does not address differences in R versions or system libraries. This limitation is where Docker becomes essential."
  },
  {
    "objectID": "posts/shareRcodeviadockerp25/analysis/claude_synth.html#docker-system-level-reproducibility",
    "href": "posts/shareRcodeviadockerp25/analysis/claude_synth.html#docker-system-level-reproducibility",
    "title": "Achieving Full Reproducibility in R: A Docker and renv Strategy",
    "section": "4 Docker: System-Level Reproducibility",
    "text": "4 Docker: System-Level Reproducibility\n\n4.1 What is Docker?\nDocker is a platform that allows you to package software into standardized units called containers. A Docker container is like a lightweight virtual machine that includes everything needed to run an application: the code, runtime, system tools, libraries, and settings.\n\n\n4.2 Docker’s Role in Reproducibility\nWhile renv handles R packages, Docker ensures consistency for:\n\nOperating system: The specific Linux distribution or OS version\nR interpreter: The exact R version\nSystem libraries: Required C/C++ libraries and other dependencies\nComputational environment: Memory limits, CPU configuration, etc.\n\nBy running an R Markdown project in Docker, you eliminate differences in OS or R installation as potential sources of irreproducibility. Any machine running Docker will execute the container in an identical environment.\n\n\n4.3 Docker Components for R Workflows\nFor R-based projects, a typical Docker approach involves:\n\nBase image: Starting from a pre-configured R image (e.g., from the Rocker project)\nDependencies: Adding system and R package dependencies\nConfiguration: Setting working directories and environment variables\nContent: Adding project files\nExecution: Defining how the project should run\n\nA simple Dockerfile for an R Markdown project might look like:\n# Use R 4.1.0 on Linux as base image\nFROM rocker/r-ver:4.1.0\n\n# Set the working directory inside the container\nWORKDIR /workspace\n\n# Install renv and restore dependencies\nRUN R -e \"install.packages('renv', repos='https://cloud.r-project.org')\"\n\n# Copy renv lockfile and infrastructure\nCOPY renv.lock renv/activate.R /workspace/\n\n# Restore the R package environment\nRUN R -e \"renv::restore()\"\n\n# Default command when container runs\nCMD [\"/bin/bash\"]\nThis Dockerfile creates a consistent environment with a specific R version and packages, regardless of the host system."
  },
  {
    "objectID": "posts/shareRcodeviadockerp25/analysis/claude_synth.html#combining-renv-and-docker-a-comprehensive-approach",
    "href": "posts/shareRcodeviadockerp25/analysis/claude_synth.html#combining-renv-and-docker-a-comprehensive-approach",
    "title": "Achieving Full Reproducibility in R: A Docker and renv Strategy",
    "section": "5 Combining renv and Docker: A Comprehensive Approach",
    "text": "5 Combining renv and Docker: A Comprehensive Approach\n\n5.1 Why Use Both?\nUsing renv or Docker alone improves reproducibility, but combining them provides the most comprehensive solution:\n\nDocker guarantees the OS and R version\nrenv guarantees the R packages and their versions\nTogether they achieve end-to-end reproducibility from operating system to package dependencies\n\nThis combined approach creates a fully portable analytical environment that can be shared and will produce identical results across different computers.\n\n\n5.2 Integration Strategy\nThe recommended workflow integrates renv and Docker in the following manner:\n\nDevelop locally with renv: Create your R project with renv to manage package dependencies.\nSnapshot dependencies: Use renv::snapshot() to create a lockfile.\nContainerize with Docker: Create a Dockerfile that uses a specific R version and incorporates the renv lockfile.\nShare both: Distribute both the code (with lockfile) and the Docker configuration.\nExecute consistently: Run analyses in the Docker container for guaranteed reproducibility.\n\nThis strategy ensures that your R Markdown documents and analyses will run identically for anyone who has access to your Docker container, regardless of their local setup."
  },
  {
    "objectID": "posts/shareRcodeviadockerp25/analysis/claude_synth.html#practical-example-collaborative-r-markdown-development",
    "href": "posts/shareRcodeviadockerp25/analysis/claude_synth.html#practical-example-collaborative-r-markdown-development",
    "title": "Achieving Full Reproducibility in R: A Docker and renv Strategy",
    "section": "6 Practical Example: Collaborative R Markdown Development",
    "text": "6 Practical Example: Collaborative R Markdown Development\nThe following case study demonstrates how two developers can collaborate on an R Markdown project using renv and Docker to ensure reproducibility.\n\n6.1 Project Scenario\nTwo data scientists are collaborating on an analysis of the Palmer Penguins dataset. Developer 1 will set up the initial project structure and create a basic analysis. Developer 2 will extend the analysis with additional visualizations. They’ll use GitHub for version control and DockerHub to share the containerized environment.\n\n\n6.2 Step-by-Step Implementation\n\n6.2.1 Developer 1: Project Setup and Initial Analysis\nStep 1: Create and Initialize the GitHub Repository\nDeveloper 1 creates a new GitHub repository called “penguins-analysis” and clones it locally:\ngit clone https://github.com/username/penguins-analysis.git\ncd penguins-analysis\nStep 2: Initialize renv for Dependency Management\ninstall.packages(\"renv\")  # If not already installed\nrenv::init()  # Initialize renv for the project\nThis creates the necessary renv infrastructure, including an initial renv.lock file.\nStep 3: Install Required R Packages\ninstall.packages(\"ggplot2\")\ninstall.packages(\"palmerpenguins\")\nrenv::snapshot()  # Save package versions to renv.lock\nStep 4: Create Initial R Markdown Analysis\nDeveloper 1 creates a file named peng1.Rmd with the following content:\n\n\npeng1.Rmd\n\n---\ntitle: \"Palmer Penguins Analysis\"\nauthor: \"Developer 1\"\ndate: \"`r Sys.Date()`\"\noutput: html_document\n---\n\n```r\n#| label: setup\n#| include: false\nlibrary(ggplot2)\nlibrary(palmerpenguins)"
  },
  {
    "objectID": "posts/shareRcodeviadockerp25/analysis/claude_synth.html#flipper-length-vs.-bill-length",
    "href": "posts/shareRcodeviadockerp25/analysis/claude_synth.html#flipper-length-vs.-bill-length",
    "title": "Achieving Full Reproducibility in R: A Docker and renv Strategy",
    "section": "7 Flipper Length vs. Bill Length",
    "text": "7 Flipper Length vs. Bill Length\n#| label: flipper-bill-plot\nggplot(penguins, aes(x = flipper_length_mm, y = bill_length_mm)) +\n  geom_point() + \n  theme_minimal() + \n  ggtitle(\"Flipper Length vs. Bill Length\")\n\n**Step 5: Create a Dockerfile**\n\nDeveloper 1 creates a Dockerfile that deliberately excludes the R Markdown file to ensure that Developer 2's local files are used when running the container:\n\n```{.dockerfile filename=\"Dockerfile\"}\n# Use R 4.1.0 as base image\nFROM rocker/r-ver:4.1.0\n\n# Set the working directory inside the container\nWORKDIR /workspace\n\n# Install renv and restore dependencies\nRUN R -e \"install.packages('renv', repos='https://cloud.r-project.org')\"\n\n# Copy only the renv.lock and renv infrastructure\nCOPY renv.lock renv/activate.R /workspace/\n\n# Restore the R package environment\nRUN R -e \"renv::restore()\"\n\nCMD [\"/bin/bash\"]\nStep 6: Build and Push the Docker Image\ndocker build -t username/penguins-analysis:v1 .\ndocker login\ndocker push username/penguins-analysis:v1\nStep 7: Commit and Push to GitHub\nDeveloper 1 commits the project files (excluding the R Markdown document from the Docker image):\ngit add .\ngit commit -m \"Initial renv setup and Docker environment (without Rmd)\"\ngit push origin main\nStep 8: Communicate with Developer 2\nDeveloper 1 provides these instructions to Developer 2:\n\nClone the GitHub repository\nPull the prebuilt Docker image from DockerHub\nRun the container interactively, mounting the local repository\nExtend the analysis in the peng1.Rmd file\nPush changes back to GitHub\n\n\n7.0.1 Developer 2: Extending the Analysis\nStep 1: Clone the Repository and Pull the Docker Image\ngit clone https://github.com/username/penguins-analysis.git\ncd penguins-analysis\ndocker pull username/penguins-analysis:v1\nStep 2: Run Docker Interactively\nDeveloper 2 runs the container with the local repository mounted:\ndocker run --rm -it -v \"$(pwd):/workspace\" -w /workspace username/penguins-analysis:v1 /bin/bash\nThis approach: - Uses the renv-restored environment from the container - Allows Developer 2 to access and modify files directly from their local machine\nStep 3: Extend the Analysis\nDeveloper 2 modifies peng1.Rmd to add a second plot for body mass vs. bill length:\n\n\npeng1.Rmd (modified)\n\n---\ntitle: \"Palmer Penguins Analysis\"\nauthor: \"Developer 2\"\ndate: \"`r Sys.Date()`\"\noutput: html_document\n---\n\n```r\n#| label: setup\n#| include: false\nlibrary(ggplot2)\nlibrary(palmerpenguins)"
  },
  {
    "objectID": "posts/shareRcodeviadockerp25/analysis/claude_synth.html#flipper-length-vs.-bill-length-1",
    "href": "posts/shareRcodeviadockerp25/analysis/claude_synth.html#flipper-length-vs.-bill-length-1",
    "title": "Achieving Full Reproducibility in R: A Docker and renv Strategy",
    "section": "8 Flipper Length vs. Bill Length",
    "text": "8 Flipper Length vs. Bill Length\n#| label: flipper-bill-plot\nggplot(penguins, aes(x = flipper_length_mm, y = bill_length_mm)) +\n  geom_point() + \n  theme_minimal() + \n  ggtitle(\"Flipper Length vs. Bill Length\")"
  },
  {
    "objectID": "posts/shareRcodeviadockerp25/analysis/claude_synth.html#body-mass-vs.-bill-length",
    "href": "posts/shareRcodeviadockerp25/analysis/claude_synth.html#body-mass-vs.-bill-length",
    "title": "Achieving Full Reproducibility in R: A Docker and renv Strategy",
    "section": "9 Body Mass vs. Bill Length",
    "text": "9 Body Mass vs. Bill Length\n#| label: mass-bill-plot\nggplot(penguins, aes(x = body_mass_g, y = bill_length_mm)) +\n  geom_point() + \n  theme_minimal() + \n  ggtitle(\"Body Mass vs. Bill Length\")\n\n**Step 4: Commit and Push Changes Back to GitHub**\n\n```bash\ngit add peng1.Rmd\ngit commit -m \"Added second plot: Body Mass vs. Bill Length\"\ngit push origin main\n\n9.1 Key Benefits Demonstrated in This Example\nThis collaborative workflow demonstrates several advantages of the renv + Docker approach:\n\nDependency consistency: Both developers work with identical R package versions thanks to renv.\nEnvironment consistency: The Docker container ensures the same R version and system libraries.\nSeparation of concerns: The R Markdown document remains outside the Docker image, allowing for easier collaboration.\nWorkflow flexibility: Developer 2 can work in the container while editing files locally.\nFull reproducibility: The entire analysis environment is captured and shareable."
  },
  {
    "objectID": "posts/shareRcodeviadockerp25/analysis/claude_synth.html#best-practices-and-considerations",
    "href": "posts/shareRcodeviadockerp25/analysis/claude_synth.html#best-practices-and-considerations",
    "title": "Achieving Full Reproducibility in R: A Docker and renv Strategy",
    "section": "10 Best Practices and Considerations",
    "text": "10 Best Practices and Considerations\n\n10.1 When to Use This Approach\nThe renv + Docker approach is particularly valuable for:\n\nLong-term research projects where reproducibility over time is crucial\nCollaborative analyses with multiple contributors on different systems\nProduction analytical pipelines that need to run consistently\nAcademic publications where methods must be reproducible\nTeaching and education to ensure consistent student experiences\n\n\n\n10.2 Tips for Efficient Implementation\n\nKeep Docker images minimal: Include only what’s necessary for reproducibility.\nUse specific version tags: For both R packages and Docker base images, specify exact versions.\nDocument system requirements: Include notes on RAM and storage requirements.\nLeverage bind mounts: Mount local directories to containers for easier development.\nConsider computational requirements: Particularly for resource-intensive analyses.\n\n\n\n10.3 Potential Challenges\nSome challenges to be aware of:\n\nDocker image size: Images with many packages can become large\nLearning curve: Both Docker and renv require some initial learning\nSystem-specific features: Some analyses may rely on hardware features\nPerformance considerations: Containers may have different performance characteristics"
  },
  {
    "objectID": "posts/shareRcodeviadockerp25/analysis/claude_synth.html#conclusion",
    "href": "posts/shareRcodeviadockerp25/analysis/claude_synth.html#conclusion",
    "title": "Achieving Full Reproducibility in R: A Docker and renv Strategy",
    "section": "11 Conclusion",
    "text": "11 Conclusion\nAchieving full reproducibility in R requires addressing both package dependencies and system-level consistency. By combining renv for R package management and Docker for environment containerization, data scientists and researchers can create truly portable and reproducible workflows.\nThis approach ensures that the common frustration of “it works on my machine” becomes a thing of the past. Instead, R Markdown projects become easy to share and fully reproducible. A collaborator or reviewer can launch the Docker container and get identical results, without worrying about package versions or system setup.\nThe case study presented demonstrates how two developers can effectively collaborate on an analysis while maintaining reproducibility throughout the project lifecycle. This strategy represents a best practice for long-term reproducibility in R, meeting the high standards required for professional data science and research documentation.\nBy adopting this two-tool approach, the R community can make significant strides toward the goal of fully reproducible research and analysis."
  },
  {
    "objectID": "posts/shareRcodeviadockerp25/analysis/claude_synth.html#references",
    "href": "posts/shareRcodeviadockerp25/analysis/claude_synth.html#references",
    "title": "Achieving Full Reproducibility in R: A Docker and renv Strategy",
    "section": "12 References",
    "text": "12 References\n\nThomas, R.G. “Docker and renv strategy.”\n“Palmer Penguins Analysis.” Developer 2.\nThe Rocker Project. https://www.rocker-project.org/\nrenv documentation. https://rstudio.github.io/renv/"
  },
  {
    "objectID": "posts/shareRcodeviadockerp25/analysis/docker_renv_slides_updated.html",
    "href": "posts/shareRcodeviadockerp25/analysis/docker_renv_slides_updated.html",
    "title": "Reproducible R Development with Docker and renv",
    "section": "",
    "text": "Ensuring reproducibility in R Markdown can be challenging. Package version mismatches and OS differences cause issues. The solution: renv (package management) + Docker (containerization)."
  },
  {
    "objectID": "posts/shareRcodeviadockerp25/analysis/docker_renv_slides_updated.html#step-1-setup-github-repository",
    "href": "posts/shareRcodeviadockerp25/analysis/docker_renv_slides_updated.html#step-1-setup-github-repository",
    "title": "Reproducible R Development with Docker and renv",
    "section": "3.1 Step 1: Setup GitHub Repository",
    "text": "3.1 Step 1: Setup GitHub Repository\nStart by creating a new repository for your project:\ngit clone https://github.com/username/penguins-analysis.git\ncd penguins-analysis"
  },
  {
    "objectID": "posts/shareRcodeviadockerp25/analysis/docker_renv_slides_updated.html#step-2-initialize-renv",
    "href": "posts/shareRcodeviadockerp25/analysis/docker_renv_slides_updated.html#step-2-initialize-renv",
    "title": "Reproducible R Development with Docker and renv",
    "section": "3.2 Step 2: Initialize renv",
    "text": "3.2 Step 2: Initialize renv\nInitialize renv to manage package versions:\ninstall.packages(\"renv\")\nrenv::init()\nThis creates renv.lock to track package versions consistently across all collaborators."
  },
  {
    "objectID": "posts/shareRcodeviadockerp25/analysis/docker_renv_slides_updated.html#step-3-install-required-packages",
    "href": "posts/shareRcodeviadockerp25/analysis/docker_renv_slides_updated.html#step-3-install-required-packages",
    "title": "Reproducible R Development with Docker and renv",
    "section": "3.3 Step 3: Install Required Packages",
    "text": "3.3 Step 3: Install Required Packages\nInstall your project dependencies and snapshot them:\ninstall.packages(\"ggplot2\")\ninstall.packages(\"palmerpenguins\")\nrenv::snapshot()\nThe renv::snapshot() command saves exact package versions to renv.lock."
  },
  {
    "objectID": "posts/shareRcodeviadockerp25/analysis/docker_renv_slides_updated.html#step-4-create-dockerfile",
    "href": "posts/shareRcodeviadockerp25/analysis/docker_renv_slides_updated.html#step-4-create-dockerfile",
    "title": "Reproducible R Development with Docker and renv",
    "section": "3.4 Step 4: Create Dockerfile",
    "text": "3.4 Step 4: Create Dockerfile\nCreate a reproducible Docker environment:\nFROM rocker/r-ver:4.1.0\nWORKDIR /workspace\nRUN R -e \"install.packages('renv', repos='https://cloud.r-project.org')\"\nCOPY renv.lock renv/activate.R /workspace/\nRUN R -e \"renv::restore()\"\nCMD [\"/bin/bash\"]\nNote: This Dockerfile does NOT include analysis files like peng1.Rmd - those are handled locally by collaborators using bind mounting."
  },
  {
    "objectID": "posts/shareRcodeviadockerp25/analysis/docker_renv_slides_updated.html#step-5-build-push-docker-image",
    "href": "posts/shareRcodeviadockerp25/analysis/docker_renv_slides_updated.html#step-5-build-push-docker-image",
    "title": "Reproducible R Development with Docker and renv",
    "section": "3.5 Step 5: Build & Push Docker Image",
    "text": "3.5 Step 5: Build & Push Docker Image\nBuild and publish your Docker image:\ndocker build -t username/penguins-analysis:v1 .\ndocker login\ndocker push username/penguins-analysis:v1\nYour image is now available on DockerHub for team members."
  },
  {
    "objectID": "posts/shareRcodeviadockerp25/analysis/docker_renv_slides_updated.html#step-6-push-to-github",
    "href": "posts/shareRcodeviadockerp25/analysis/docker_renv_slides_updated.html#step-6-push-to-github",
    "title": "Reproducible R Development with Docker and renv",
    "section": "3.6 Step 6: Push to GitHub",
    "text": "3.6 Step 6: Push to GitHub\nCommit and push your configuration:\ngit add .\ngit commit -m \"Initial renv setup and Docker environment (without Rmd)\"\ngit push origin main"
  },
  {
    "objectID": "posts/shareRcodeviadockerp25/analysis/docker_renv_slides_updated.html#step-1-clone-repository-pull-docker-image",
    "href": "posts/shareRcodeviadockerp25/analysis/docker_renv_slides_updated.html#step-1-clone-repository-pull-docker-image",
    "title": "Reproducible R Development with Docker and renv",
    "section": "4.1 Step 1: Clone Repository & Pull Docker Image",
    "text": "4.1 Step 1: Clone Repository & Pull Docker Image\nGet the repository and Docker image locally:\ngit clone https://github.com/username/penguins-analysis.git\ncd penguins-analysis\ndocker pull username/penguins-analysis:v1"
  },
  {
    "objectID": "posts/shareRcodeviadockerp25/analysis/docker_renv_slides_updated.html#step-2-run-docker-with-bind-mounting",
    "href": "posts/shareRcodeviadockerp25/analysis/docker_renv_slides_updated.html#step-2-run-docker-with-bind-mounting",
    "title": "Reproducible R Development with Docker and renv",
    "section": "4.2 Step 2: Run Docker with Bind Mounting",
    "text": "4.2 Step 2: Run Docker with Bind Mounting\nRun the Docker container with your local directory mounted:\ndocker run --rm -it -v \"$(pwd):/workspace\" -w /workspace username/penguins-analysis:v1 /bin/bash\nThis command: - -v \"$(pwd):/workspace\": Mounts your local repository to /workspace in the container - Allows you to edit files locally while running code in the reproducible environment"
  },
  {
    "objectID": "posts/shareRcodeviadockerp25/analysis/docker_renv_slides_updated.html#step-3-create-analysis-file",
    "href": "posts/shareRcodeviadockerp25/analysis/docker_renv_slides_updated.html#step-3-create-analysis-file",
    "title": "Reproducible R Development with Docker and renv",
    "section": "4.3 Step 3: Create Analysis File",
    "text": "4.3 Step 3: Create Analysis File\nWrite your analysis as peng1.Rmd inside the container:\n---\ntitle: \"Palmer Penguins Analysis\"\nauthor: \"Developer 2\"\ndate: \"2025-12-08\"\noutput: html_document\n---\n\n\\`\\`\\`{r}\nlibrary(ggplot2)\nlibrary(palmerpenguins)\n\n# Explicitly use palmerpenguins::penguins to avoid conflicts\nggplot(palmerpenguins::penguins, aes(x = flipper_length_mm, y = bill_length_mm)) +\n  geom_point() + theme_minimal()\n\\`\\`\\`"
  },
  {
    "objectID": "posts/shareRcodeviadockerp25/analysis/docker_renv_slides_updated.html#step-4-render-your-analysis",
    "href": "posts/shareRcodeviadockerp25/analysis/docker_renv_slides_updated.html#step-4-render-your-analysis",
    "title": "Reproducible R Development with Docker and renv",
    "section": "4.4 Step 4: Render Your Analysis",
    "text": "4.4 Step 4: Render Your Analysis\nGenerate the HTML output:\nrmarkdown::render(\"peng1.Rmd\")\nThe output peng1.html is saved locally thanks to the bind mount, accessible outside the container."
  },
  {
    "objectID": "posts/shareRcodeviadockerp25/analysis/docker_renv_slides_updated.html#step-5-extend-share",
    "href": "posts/shareRcodeviadockerp25/analysis/docker_renv_slides_updated.html#step-5-extend-share",
    "title": "Reproducible R Development with Docker and renv",
    "section": "4.5 Step 5: Extend & Share",
    "text": "4.5 Step 5: Extend & Share\nAdd more analysis and push back to GitHub:\n## Body Mass vs. Bill Length\n\n\\`\\`\\`{r}\nggplot(palmerpenguins::penguins, aes(x = body_mass_g, y = bill_length_mm)) +\n  geom_point() + theme_minimal()\n\\`\\`\\`\nThen commit and push:\ngit add peng1.Rmd\ngit commit -m \"Added second plot: Body Mass vs. Bill Length\"\ngit push origin main"
  },
  {
    "objectID": "posts/shinyvsobservable/analysis/paper/blog_ojs_shiny_sidebyside.html",
    "href": "posts/shinyvsobservable/analysis/paper/blog_ojs_shiny_sidebyside.html",
    "title": "Combining Observable JS and Shiny in One Quarto Document",
    "section": "",
    "text": "If you’re a statistician who’s heard about Observable JS and wants to compare it side-by-side with Shiny in a single document, you might think: “Quarto supports both—how hard can it be?”\nVery hard, as it turns out.\nThis post documents our journey trying to create a simple side-by-side comparison of Observable JS and Shiny showing the same interactive visualization. What should have taken 30 minutes took several hours of debugging. We’re sharing our experience so you don’t have to suffer the same fate."
  },
  {
    "objectID": "posts/shinyvsobservable/analysis/paper/blog_ojs_shiny_sidebyside.html#introduction",
    "href": "posts/shinyvsobservable/analysis/paper/blog_ojs_shiny_sidebyside.html#introduction",
    "title": "Combining Observable JS and Shiny in One Quarto Document",
    "section": "",
    "text": "If you’re a statistician who’s heard about Observable JS and wants to compare it side-by-side with Shiny in a single document, you might think: “Quarto supports both—how hard can it be?”\nVery hard, as it turns out.\nThis post documents our journey trying to create a simple side-by-side comparison of Observable JS and Shiny showing the same interactive visualization. What should have taken 30 minutes took several hours of debugging. We’re sharing our experience so you don’t have to suffer the same fate."
  },
  {
    "objectID": "posts/shinyvsobservable/analysis/paper/blog_ojs_shiny_sidebyside.html#the-goal",
    "href": "posts/shinyvsobservable/analysis/paper/blog_ojs_shiny_sidebyside.html#the-goal",
    "title": "Combining Observable JS and Shiny in One Quarto Document",
    "section": "2 The Goal",
    "text": "2 The Goal\nWe wanted to create a single Quarto document that shows:\n\nLeft column: An Observable JS histogram with a slider filter\nRight column: The identical visualization in Shiny\n\nBoth would filter the Palmer Penguins dataset by bill length and display a histogram of body mass. Simple, right?"
  },
  {
    "objectID": "posts/shinyvsobservable/analysis/paper/blog_ojs_shiny_sidebyside.html#the-naive-approach-what-we-tried-first",
    "href": "posts/shinyvsobservable/analysis/paper/blog_ojs_shiny_sidebyside.html#the-naive-approach-what-we-tried-first",
    "title": "Combining Observable JS and Shiny in One Quarto Document",
    "section": "3 The Naive Approach (What We Tried First)",
    "text": "3 The Naive Approach (What We Tried First)\n\n3.1 Attempt 1: Just Put Both in One File\nOur first attempt looked like this:\n---\ntitle: \"Side by Side\"\nformat: html\nserver: shiny\n---\nWith Observable JS code:\ndata = FileAttachment(\"palmer-penguins.csv\").csv({typed: true})\nAnd Shiny code:\n#| context: server\ndata &lt;- read.csv(\"palmer-penguins.csv\")\nResult: The Shiny side worked. The Observable side showed:\nOJS Error: Unable to load file: palmer-penguins.csv\n\n\n3.2 Why This Fails\nWhen you add server: shiny to your YAML header, the document runs as a Shiny application. This fundamentally changes how files are served:\n\nShiny serves the HTML dynamically from an R process\nFileAttachment paths don’t resolve because the Shiny server doesn’t know about Observable’s file attachment system\nThe CSV file isn’t accessible at the URL that Observable expects\n\n\n\n\n\n\n\nWarningKey Insight #1\n\n\n\nFileAttachment() does NOT work in Shiny documents. This is the single biggest “gotcha” when combining OJS and Shiny."
  },
  {
    "objectID": "posts/shinyvsobservable/analysis/paper/blog_ojs_shiny_sidebyside.html#attempt-2-use-ojs_define-to-share-data",
    "href": "posts/shinyvsobservable/analysis/paper/blog_ojs_shiny_sidebyside.html#attempt-2-use-ojs_define-to-share-data",
    "title": "Combining Observable JS and Shiny in One Quarto Document",
    "section": "4 Attempt 2: Use ojs_define() to Share Data",
    "text": "4 Attempt 2: Use ojs_define() to Share Data\nThe Quarto documentation mentions ojs_define() as a way to pass data from R to Observable. Perfect! We’ll load the data in R and share it:\n#| context: server\ndata &lt;- read.csv(\"palmer-penguins.csv\")\nojs_define(data_from_r = data)\ndata_ojs = transpose(data_from_r)\nResult:\nError in .subset2(x, \"impl\")$defineOutput:\n  Unexpected data.frame object for output$data_from_r\nℹ Did you forget to use a render function?\n\n4.1 Why This Fails\nIn a Shiny document, ojs_define() behaves differently than in a regular Quarto document:\n\nIn regular Quarto: ojs_define() runs at render time and embeds data in the HTML\nIn Shiny Quarto: ojs_define() tries to create a Shiny output, which expects a reactive expression\n\n\n\n\n\n\n\nWarningKey Insight #2\n\n\n\nojs_define() in a server: shiny document requires passing a reactive expression, not a static data frame. And even then, it may not work as expected.\n\n\n\n\n4.2 What About context: setup?\nWe tried putting ojs_define() in different contexts:\n#| context: setup\nojs_define(data = penguins)\nResult: Same error. In a Shiny document, even “setup” code runs within the Shiny server context."
  },
  {
    "objectID": "posts/shinyvsobservable/analysis/paper/blog_ojs_shiny_sidebyside.html#attempt-3-use-d3.csv-instead",
    "href": "posts/shinyvsobservable/analysis/paper/blog_ojs_shiny_sidebyside.html#attempt-3-use-d3.csv-instead",
    "title": "Combining Observable JS and Shiny in One Quarto Document",
    "section": "5 Attempt 3: Use d3.csv() Instead",
    "text": "5 Attempt 3: Use d3.csv() Instead\nSince FileAttachment doesn’t work, maybe we can use D3’s CSV loader:\ndata_ojs = d3.csv(\"palmer-penguins.csv\", d3.autoType)\nResult:\nOJS Error: 404 Not Found\n\n5.1 Why This Fails\nThe Shiny server doesn’t serve static files from the working directory by default. When Observable tries to fetch palmer-penguins.csv, the Shiny server returns a 404.\nWe tried adding resources to the YAML:\nformat:\n  html:\n    resources:\n      - palmer-penguins.csv\nResult: Still 404. The resources option works for static HTML output, but the Shiny server has its own file-serving logic."
  },
  {
    "objectID": "posts/shinyvsobservable/analysis/paper/blog_ojs_shiny_sidebyside.html#the-solution-that-actually-works",
    "href": "posts/shinyvsobservable/analysis/paper/blog_ojs_shiny_sidebyside.html#the-solution-that-actually-works",
    "title": "Combining Observable JS and Shiny in One Quarto Document",
    "section": "6 The Solution That Actually Works",
    "text": "6 The Solution That Actually Works\nAfter much trial and error, here’s what finally worked:\n\n6.1 Fetch Data from a Public URL\npenguins_raw = {\n  const response = await fetch(\n    \"https://raw.githubusercontent.com/allisonhorst/palmerpenguins/main/inst/extdata/penguins.csv\"\n  );\n  const text = await response.text();\n  return d3.csvParse(text, d3.autoType);\n}\n\ndata_ojs = penguins_raw.filter(d =&gt; d.bill_length_mm != null)\nThis works because:\n\nfetch() makes an HTTP request to an external URL\nGitHub’s raw content URLs are publicly accessible\nThe request bypasses the Shiny server entirely\nD3’s csvParse() converts the text to typed data\n\n\n\n\n\n\n\nTipThe Working Solution\n\n\n\nIn a Shiny+OJS document, load OJS data from a public URL using fetch(), not from local files."
  },
  {
    "objectID": "posts/shinyvsobservable/analysis/paper/blog_ojs_shiny_sidebyside.html#complete-working-example",
    "href": "posts/shinyvsobservable/analysis/paper/blog_ojs_shiny_sidebyside.html#complete-working-example",
    "title": "Combining Observable JS and Shiny in One Quarto Document",
    "section": "7 Complete Working Example",
    "text": "7 Complete Working Example\nHere’s the minimal working code:\n---\ntitle: \"Observable vs Shiny: Side-by-Side\"\nformat: html\nserver: shiny\n---\n\n::: {.grid}\n\n::: {.g-col-6}\n## Observable JS\n\n```{ojs}\nviewof bill_min = Inputs.range([32, 50], {value: 35, label: \"Min bill length:\"})\n\n// Load from public URL - this is the key!\npenguins = {\n  const resp = await fetch(\"https://raw.githubusercontent.com/allisonhorst/palmerpenguins/main/inst/extdata/penguins.csv\");\n  return d3.csvParse(await resp.text(), d3.autoType);\n}\n\nfiltered = penguins.filter(d =&gt; d.bill_length_mm &gt; bill_min && d.bill_length_mm != null)\n\nPlot.plot({\n  marks: [Plot.rectY(filtered, Plot.binX({y: \"count\"}, {x: \"body_mass_g\", fill: \"species\"}))]\n})\n```\n:::\n\n::: {.g-col-6}\n## Shiny\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsliderInput(\"bill_min\", \"Min bill length:\", min = 32, max = 50, value = 35)\nplotOutput(\"plot\")\n```\n:::\n\n\n\n::: {.cell context='server'}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(dplyr)\n\ndata &lt;- read.csv(\"palmer-penguins.csv\")\n\noutput$plot &lt;- renderPlot({\n  data |&gt;\n    filter(bill_length_mm &gt; input$bill_min, !is.na(body_mass_g)) |&gt;\n    ggplot(aes(x = body_mass_g, fill = species)) +\n    geom_histogram(bins = 20)\n})\n```\n:::\n\n:::\n\n:::"
  },
  {
    "objectID": "posts/shinyvsobservable/analysis/paper/blog_ojs_shiny_sidebyside.html#dos-and-donts",
    "href": "posts/shinyvsobservable/analysis/paper/blog_ojs_shiny_sidebyside.html#dos-and-donts",
    "title": "Combining Observable JS and Shiny in One Quarto Document",
    "section": "8 Dos and Don’ts",
    "text": "8 Dos and Don’ts\n\n8.1 ✅ DO\n\nUse fetch() with public URLs for loading data in OJS within Shiny documents\nKeep OJS and Shiny data loading separate - don’t try to share data between them\nTest incrementally - get Shiny working first, then add OJS\nUse await properly when fetching data in OJS code blocks\nHard refresh your browser (Cmd+Shift+R) when debugging - caching causes confusion\nRestart the preview server when making major changes - Quarto’s hot reload doesn’t always catch everything\n\n\n\n8.2 ❌ DON’T\n\nDon’t use FileAttachment() in Shiny documents - it won’t work\nDon’t use ojs_define() with static data in Shiny documents - it expects reactives\nDon’t assume d3.csv(\"local-file.csv\") will work - Shiny doesn’t serve local files\nDon’t rely on resources: in YAML to make files accessible to OJS in Shiny mode\nDon’t mix context: setup with ojs_define() - it still runs in Shiny context\nDon’t trust the displayed code in the browser - it may be cached from a previous version"
  },
  {
    "objectID": "posts/shinyvsobservable/analysis/paper/blog_ojs_shiny_sidebyside.html#gotchas-and-edge-cases",
    "href": "posts/shinyvsobservable/analysis/paper/blog_ojs_shiny_sidebyside.html#gotchas-and-edge-cases",
    "title": "Combining Observable JS and Shiny in One Quarto Document",
    "section": "9 Gotchas and Edge Cases",
    "text": "9 Gotchas and Edge Cases\n\n9.1 Gotcha 1: Browser Caching\nWhen debugging, the browser often shows old code even after you’ve updated the file. The displayed code (with echo: true) might show FileAttachment(...) even though your file now uses fetch().\nSolution: Kill the preview server, delete generated files, restart, and hard-refresh:\nrm -rf yourfile_files yourfile.html\nquarto preview yourfile.qmd\n# Then Cmd+Shift+R in browser\n\n\n9.2 Gotcha 2: Server Context Confusion\nIn a Shiny document, all R code runs in the server context by default. Even chunks without #| context: server are affected by the Shiny runtime.\n\n\n9.3 Gotcha 3: Error Messages Are Misleading\nThe error “Did you forget to use a render function?” when using ojs_define() suggests you need renderSomething(), but that’s not the real issue. The real issue is that ojs_define() doesn’t work well with static data in Shiny documents.\n\n\n9.4 Gotcha 4: The Preview Port Changes\nEvery time you restart quarto preview, you might get a different port (5155, 6532, 7665, etc.). Make sure you’re looking at the right URL.\n\n\n9.5 Gotcha 5: OJS Inputs Don’t Automatically Connect to Shiny\nUnlike what some documentation suggests, viewof variables in OJS don’t automatically become Shiny inputs unless you configure ojs-import in the YAML. For a simple side-by-side comparison, keep the two systems completely separate."
  },
  {
    "objectID": "posts/shinyvsobservable/analysis/paper/blog_ojs_shiny_sidebyside.html#alternative-approaches",
    "href": "posts/shinyvsobservable/analysis/paper/blog_ojs_shiny_sidebyside.html#alternative-approaches",
    "title": "Combining Observable JS and Shiny in One Quarto Document",
    "section": "10 Alternative Approaches",
    "text": "10 Alternative Approaches\nIf this complexity is too much, consider these alternatives:\n\n10.1 Option 1: Two Separate Documents\nCreate penguins_observable.qmd and penguins_shiny.qmd separately, then link between them or embed them in iframes.\n\n\n10.2 Option 2: Static OJS Only\nIf you don’t need Shiny’s server-side features, remove server: shiny and use pure OJS with FileAttachment():\n---\ntitle: \"Observable Only\"\nformat: html\n# No server: shiny!\n---\n\n\n10.3 Option 3: Pre-embed the Data as JSON\nConvert your CSV to JSON and embed it directly in the OJS code:\ndata = [\n  {\"species\": \"Adelie\", \"bill_length_mm\": 39.1, ...},\n  ...\n]\nThis makes the file larger but avoids all loading issues."
  },
  {
    "objectID": "posts/shinyvsobservable/analysis/paper/blog_ojs_shiny_sidebyside.html#when-to-use-each-technology",
    "href": "posts/shinyvsobservable/analysis/paper/blog_ojs_shiny_sidebyside.html#when-to-use-each-technology",
    "title": "Combining Observable JS and Shiny in One Quarto Document",
    "section": "11 When to Use Each Technology",
    "text": "11 When to Use Each Technology\nBased on our experience:\n\n\n\nUse Case\nRecommendation\n\n\n\n\nSelf-contained report\nObservable JS only\n\n\nComplex statistics\nShiny only\n\n\nTeaching comparison\nSeparate documents\n\n\nProduction dashboard\nPick one, not both"
  },
  {
    "objectID": "posts/shinyvsobservable/analysis/paper/blog_ojs_shiny_sidebyside.html#conclusion",
    "href": "posts/shinyvsobservable/analysis/paper/blog_ojs_shiny_sidebyside.html#conclusion",
    "title": "Combining Observable JS and Shiny in One Quarto Document",
    "section": "12 Conclusion",
    "text": "12 Conclusion\nCombining Observable JS and Shiny in a single Quarto document is possible, but it’s not straightforward. The key challenges are:\n\nFile loading works completely differently in Shiny vs. static Quarto\nojs_define() has different behavior depending on context\nDocumentation doesn’t cover this edge case well\nDebugging is frustrating due to caching and misleading errors\n\nThe working solution—fetching data from a public URL—is a workaround rather than an elegant integration. If you’re building production applications, we recommend choosing one technology rather than combining both.\nHowever, for educational purposes (showing students or colleagues how both technologies work), the side-by-side approach can be valuable once you get past the setup hurdles."
  },
  {
    "objectID": "posts/shinyvsobservable/analysis/paper/blog_ojs_shiny_sidebyside.html#resources",
    "href": "posts/shinyvsobservable/analysis/paper/blog_ojs_shiny_sidebyside.html#resources",
    "title": "Combining Observable JS and Shiny in One Quarto Document",
    "section": "13 Resources",
    "text": "13 Resources\n\nQuarto OJS Documentation\nQuarto Shiny Documentation\nQuarto Shiny Reactives (ojs_define)\nObservable Plot\nPalmer Penguins Dataset\n\n\nThis post was written after several hours of debugging. We hope it saves you time."
  },
  {
    "objectID": "posts/shinyvsobservable/analysis/paper/index.html",
    "href": "posts/shinyvsobservable/analysis/paper/index.html",
    "title": "Combining Observable JS and Shiny in One Quarto Document",
    "section": "",
    "text": "Penguins make great companions for learning new technologies\nCombining two reactive frameworks in one document: harder than it looks!"
  },
  {
    "objectID": "posts/shinyvsobservable/analysis/paper/index.html#motivations",
    "href": "posts/shinyvsobservable/analysis/paper/index.html#motivations",
    "title": "Combining Observable JS and Shiny in One Quarto Document",
    "section": "1.1 Motivations",
    "text": "1.1 Motivations\nWhy explore this combination?\n\nWanted to compare client-side (OJS) vs server-side (Shiny) reactive frameworks\nNeeded to demonstrate both technologies to colleagues in a single document\nCurious whether Quarto’s multi-language support extends to mixing reactive systems\nCouldn’t find good documentation on combining these two approaches"
  },
  {
    "objectID": "posts/shinyvsobservable/analysis/paper/index.html#objectives",
    "href": "posts/shinyvsobservable/analysis/paper/index.html#objectives",
    "title": "Combining Observable JS and Shiny in One Quarto Document",
    "section": "1.2 Objectives",
    "text": "1.2 Objectives\nWhat I wanted to accomplish:\n\nCreate identical interactive visualizations using both Observable JS and Shiny\nDisplay them side-by-side in a single Quarto document\nDocument the gotchas and workarounds for others\nUnderstand the fundamental differences between client-side and server-side reactivity\n\nDisclaimer: I’m documenting my learning process here. If you spot errors or have better approaches, please let me know!"
  },
  {
    "objectID": "posts/shinyvsobservable/analysis/paper/index.html#challenge-1-fileattachment-doesnt-work",
    "href": "posts/shinyvsobservable/analysis/paper/index.html#challenge-1-fileattachment-doesnt-work",
    "title": "Combining Observable JS and Shiny in One Quarto Document",
    "section": "4.1 Challenge 1: FileAttachment Doesn’t Work",
    "text": "4.1 Challenge 1: FileAttachment Doesn’t Work\nOur first attempt used Observable’s standard data loading:\n// THIS DOESN'T WORK IN SHINY DOCUMENTS!\ndata = FileAttachment(\"palmer-penguins.csv\").csv({typed: true})\nResult: OJS Error: Unable to load file\nWhy: When you add server: shiny, the document runs as a Shiny app. The Shiny server doesn’t know about Observable’s file attachment system.\n\n\n\n\n\n\nWarningGotcha #1\n\n\n\nFileAttachment() does NOT work in Shiny documents. This is the biggest “gotcha” when combining OJS and Shiny."
  },
  {
    "objectID": "posts/shinyvsobservable/analysis/paper/index.html#challenge-2-ojs_define-fails-too",
    "href": "posts/shinyvsobservable/analysis/paper/index.html#challenge-2-ojs_define-fails-too",
    "title": "Combining Observable JS and Shiny in One Quarto Document",
    "section": "4.2 Challenge 2: ojs_define() Fails Too",
    "text": "4.2 Challenge 2: ojs_define() Fails Too\nThe Quarto docs mention ojs_define() for passing data from R to OJS:\n#| context: server\ndata &lt;- read.csv(\"penguins.csv\")\nojs_define(my_data = data)  # THIS FAILS!\nResult: Error: Unexpected data.frame object... Did you forget to use a render function?\nWhy: In Shiny context, ojs_define() expects reactive expressions, not static data frames.\n\n\n\n\n\n\nWarningGotcha #2\n\n\n\nojs_define() behaves differently in Shiny documents. Don’t expect it to work like the docs show for regular Quarto."
  },
  {
    "objectID": "posts/shinyvsobservable/analysis/paper/index.html#challenge-3-local-d3.csv-returns-404",
    "href": "posts/shinyvsobservable/analysis/paper/index.html#challenge-3-local-d3.csv-returns-404",
    "title": "Combining Observable JS and Shiny in One Quarto Document",
    "section": "4.3 Challenge 3: Local d3.csv() Returns 404",
    "text": "4.3 Challenge 3: Local d3.csv() Returns 404\nWe tried D3’s CSV loader:\n// THIS ALSO DOESN'T WORK!\ndata = d3.csv(\"palmer-penguins.csv\", d3.autoType)\nResult: OJS Error: 404 Not Found\nWhy: The Shiny server doesn’t serve local static files the way Observable expects."
  },
  {
    "objectID": "posts/shinyvsobservable/analysis/paper/index.html#the-solution-fetch-from-public-url",
    "href": "posts/shinyvsobservable/analysis/paper/index.html#the-solution-fetch-from-public-url",
    "title": "Combining Observable JS and Shiny in One Quarto Document",
    "section": "4.4 The Solution: Fetch from Public URL",
    "text": "4.4 The Solution: Fetch from Public URL\nThe only reliable approach is fetching data from a public URL:\n// THIS WORKS!\npenguins_raw = {\n  const response = await fetch(\n    \"https://raw.githubusercontent.com/.../penguins.csv\"\n  );\n  const text = await response.text();\n  return d3.csvParse(text, d3.autoType);\n}\nThis bypasses the Shiny server entirely by making an HTTP request to an external source."
  },
  {
    "objectID": "posts/shinyvsobservable/analysis/paper/index.html#do",
    "href": "posts/shinyvsobservable/analysis/paper/index.html#do",
    "title": "Combining Observable JS and Shiny in One Quarto Document",
    "section": "5.1 DO",
    "text": "5.1 DO\n\nUse fetch() with public URLs for OJS data in Shiny documents\nKeep OJS and Shiny data loading separate—don’t try to share data\nTest incrementally—get Shiny working first, then add OJS\nHard refresh your browser (Cmd+Shift+R) when debugging\nRestart the preview server after major changes"
  },
  {
    "objectID": "posts/shinyvsobservable/analysis/paper/index.html#dont",
    "href": "posts/shinyvsobservable/analysis/paper/index.html#dont",
    "title": "Combining Observable JS and Shiny in One Quarto Document",
    "section": "5.2 DON’T",
    "text": "5.2 DON’T\n\nDon’t use FileAttachment() in Shiny documents\nDon’t use ojs_define() with static data in Shiny context\nDon’t assume d3.csv(\"local.csv\") will work\nDon’t trust displayed code—browser caching causes confusion\nDon’t mix contexts—keep OJS and Shiny logic separate"
  },
  {
    "objectID": "posts/shinyvsobservable/analysis/paper/index.html#browser-caching",
    "href": "posts/shinyvsobservable/analysis/paper/index.html#browser-caching",
    "title": "Combining Observable JS and Shiny in One Quarto Document",
    "section": "6.1 Browser Caching",
    "text": "6.1 Browser Caching\nWhen debugging, the browser often shows old code. The displayed code might show FileAttachment(...) even though your file now uses fetch().\nSolution: Kill server, delete generated files, restart, hard-refresh:\nrm -rf yourfile_files yourfile.html\nquarto preview yourfile.qmd\n# Then Cmd+Shift+R in browser"
  },
  {
    "objectID": "posts/shinyvsobservable/analysis/paper/index.html#port-changes",
    "href": "posts/shinyvsobservable/analysis/paper/index.html#port-changes",
    "title": "Combining Observable JS and Shiny in One Quarto Document",
    "section": "6.2 Port Changes",
    "text": "6.2 Port Changes\nEvery restart may give a different port (5155, 6532, 7665…). Check the terminal output for the correct URL."
  },
  {
    "objectID": "posts/shinyvsobservable/analysis/paper/index.html#misleading-error-messages",
    "href": "posts/shinyvsobservable/analysis/paper/index.html#misleading-error-messages",
    "title": "Combining Observable JS and Shiny in One Quarto Document",
    "section": "6.3 Misleading Error Messages",
    "text": "6.3 Misleading Error Messages\nThe error “Did you forget to use a render function?” doesn’t mean you need renderSomething(). It means ojs_define() doesn’t work with static data in Shiny context."
  },
  {
    "objectID": "posts/shinyvsobservable/analysis/paper/index.html#limitations",
    "href": "posts/shinyvsobservable/analysis/paper/index.html#limitations",
    "title": "Combining Observable JS and Shiny in One Quarto Document",
    "section": "8.1 Limitations",
    "text": "8.1 Limitations\n\nThis approach requires external data hosting for OJS\nNo data sharing between OJS and Shiny contexts\nDebugging is significantly harder than single-framework documents\nDocumentation doesn’t cover this edge case well"
  },
  {
    "objectID": "posts/shinyvsobservable/analysis/paper/index.html#opportunities-for-improvement",
    "href": "posts/shinyvsobservable/analysis/paper/index.html#opportunities-for-improvement",
    "title": "Combining Observable JS and Shiny in One Quarto Document",
    "section": "8.2 Opportunities for Improvement",
    "text": "8.2 Opportunities for Improvement\nIf I had more time, here’s what I’d explore next:\n\nTest ojs_define() with reactive expressions - might work with reactive() wrapper\nCreate a helper function to bridge OJS and Shiny data\nExplore Shiny’s resource serving - maybe there’s a way to make local files available to OJS\nDocument a clean workflow for teams who need both frameworks"
  },
  {
    "objectID": "posts/shinyvsobservable/analysis/paper/penguins_shiny.html",
    "href": "posts/shinyvsobservable/analysis/paper/penguins_shiny.html",
    "title": "Penguins",
    "section": "",
    "text": "A simple example based on Allison Horst’s Palmer Penguins dataset. Here we look at how penguin body mass varies across both sex and species (use the provided inputs to filter the dataset by bill length and island):\n\n\n\nCode\n# -----------------------------------------------------------------------------\n# SLIDER INPUT\n# -----------------------------------------------------------------------------\n# sliderInput() creates a slider control\n#\n# Arguments:\n#   inputId: \"bill_length_min\" → Unique ID, accessed as input$bill_length_min\n#   label: \"...\"               → Text label displayed above the slider\n#   min: 32                    → Minimum allowed value\n#   max: 50                    → Maximum allowed value\n#   value: 35                  → Initial/default value\n#   step: 1                    → Increment amount\n#\n# Compare to Observable:\n#   viewof bill_length_min = Inputs.range([32, 50], {value: 35, step: 1, label: \"...\"})\n\nsliderInput(\n  inputId = \"bill_length_min\",\n  label = \"Bill length (min):\",\n  min = 32,\n  max = 50,\n  value = 35,\n  step = 1\n)\n\n# -----------------------------------------------------------------------------\n# CHECKBOX GROUP INPUT\n# -----------------------------------------------------------------------------\n# checkboxGroupInput() creates a group of checkboxes\n#\n# Arguments:\n#   inputId: \"islands\"                        → Unique ID, accessed as input$islands\n#   label: \"Islands:\"                         → Text label for the group\n#   choices: c(\"Torgersen\", \"Biscoe\", \"Dream\") → Available options\n#   selected: c(\"Torgersen\", \"Biscoe\")        → Initially checked values\n#\n# The input$islands will contain a character vector of selected values\n# e.g., c(\"Torgersen\", \"Biscoe\") or \"Dream\" or NULL (if none selected)\n#\n# Compare to Observable:\n#   viewof islands = Inputs.checkbox([\"Torgersen\", \"Biscoe\", \"Dream\"],\n#                                    {value: [\"Torgersen\", \"Biscoe\"], label: \"Islands:\"})\n\ncheckboxGroupInput(\n  inputId = \"islands\",\n  label = \"Islands:\",\n  choices = c(\"Torgersen\", \"Biscoe\", \"Dream\"),\n  selected = c(\"Torgersen\", \"Biscoe\")\n)\n\n\n\n\nPlotData\n\n\n\n\n\nCode\n# -----------------------------------------------------------------------------\n# REACTIVE PLOT OUTPUT\n# -----------------------------------------------------------------------------\n# plotOutput() creates a placeholder in the UI for a plot\n# The actual plot is rendered by renderPlot() in the server context block below\n#\n# Compare to Observable:\n#   In OJS, you just write the Plot code directly and it renders automatically\n\nplotOutput(\"penguin_plot\")\n\n\n\n\n\n\n\nCode\n# -----------------------------------------------------------------------------\n# REACTIVE TABLE OUTPUT\n# -----------------------------------------------------------------------------\n# tableOutput() creates a placeholder for a table\n# The actual table is rendered by renderTable() in the server context block\n#\n# For more interactive tables, you could use:\n#   DT::dataTableOutput(\"penguin_table\") with DT::renderDataTable()\n#\n# Compare to Observable:\n#   Inputs.table(filtered)\n\ntableOutput(\"penguin_table\")\n\n\n\n\n\n\n\n\nCode\n# -----------------------------------------------------------------------------\n# LOAD REQUIRED LIBRARIES\n# -----------------------------------------------------------------------------\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# -----------------------------------------------------------------------------\n# LOAD DATA\n# -----------------------------------------------------------------------------\n# Read the CSV file once when the app starts\n# In Shiny, data loading typically happens outside reactive contexts\n# for better performance (load once, use many times)\n#\n# Compare to Observable:\n#   data = FileAttachment(\"palmer-penguins.csv\").csv({ typed: true })\n\ndata &lt;- read.csv(\"palmer-penguins.csv\")\n\n# -----------------------------------------------------------------------------\n# REACTIVE FILTERED DATA\n# -----------------------------------------------------------------------------\n# reactive() creates a reactive expression that automatically updates\n# when its dependencies (input$bill_length_min, input$islands) change\n#\n# Call it as filtered() - note the parentheses! It's a function.\n#\n# Compare to Observable:\n#   filtered = data.filter(function(penguin) {\n#     return bill_length_min &lt; penguin.bill_length_mm &&\n#            islands.includes(penguin.island);\n#   })\n#\n# KEY DIFFERENCE: In Shiny, you explicitly wrap reactive code in reactive({})\n# In Observable, reactivity is automatic based on variable references\n\nfiltered &lt;- reactive({\n  # Handle case when no islands are selected\n  req(input$islands)  # Don't run if islands is NULL/empty\n\n  data |&gt;\n    filter(\n      bill_length_mm &gt; input$bill_length_min,\n      island %in% input$islands\n    ) |&gt;\n    # Remove rows with NA values for cleaner plotting\n    filter(!is.na(body_mass_g), !is.na(sex), !is.na(species))\n})\n\n# -----------------------------------------------------------------------------\n# RENDER PLOT\n# -----------------------------------------------------------------------------\n# renderPlot() creates a reactive plot that re-renders when dependencies change\n# The output$penguin_plot connects to plotOutput(\"penguin_plot\") in the UI\n#\n# Compare to Observable Plot:\n#   Plot.rectY(filtered, Plot.binX({y: \"count\"}, {x: \"body_mass_g\", fill: \"species\"}))\n#     .plot({facet: {data: filtered, x: \"sex\", y: \"species\"}})\n#\n# Here we use ggplot2 which has similar concepts:\n#   - geom_histogram() ≈ Plot.rectY() + Plot.binX()\n#   - facet_grid()     ≈ facet: {x: \"sex\", y: \"species\"}\n#   - fill = species   ≈ fill: \"species\"\n\noutput$penguin_plot &lt;- renderPlot({\n  ggplot(filtered(), aes(x = body_mass_g, fill = species)) +\n    geom_histogram(bins = 20, color = \"white\", linewidth = 0.2) +\n    facet_grid(species ~ sex) +\n    labs(\n      x = \"Body Mass (g)\",\n      y = \"Count\",\n      fill = \"Species\"\n    ) +\n    theme_minimal() +\n    theme(\n      strip.text = element_text(size = 12, face = \"bold\"),\n      legend.position = \"bottom\"\n    )\n})\n\n# -----------------------------------------------------------------------------\n# RENDER TABLE\n# -----------------------------------------------------------------------------\n# renderTable() creates a reactive table that updates when filtered() changes\n# The output$penguin_table connects to tableOutput(\"penguin_table\") in the UI\n#\n# Compare to Observable:\n#   Inputs.table(filtered)\n\noutput$penguin_table &lt;- renderTable({\n  filtered()\n})\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{(ryy)_glenn_thomas,\n  author = {(Ryy) Glenn Thomas, Ronald},\n  title = {Penguins},\n  url = {https://focusonr.org/posts/shinyvsobservable/analysis/paper/penguins_shiny.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\n(Ryy) Glenn Thomas, Ronald. n.d. “Penguins.” https://focusonr.org/posts/shinyvsobservable/analysis/paper/penguins_shiny.html."
  },
  {
    "objectID": "posts/templatepost/analysis/paper/index.html",
    "href": "posts/templatepost/analysis/paper/index.html",
    "title": "Your Engaging Title Here: A Learning Journey",
    "section": "",
    "text": "Engaging hero image that introduces your topic visually\nPhoto caption with attribution if needed. This image sets the visual tone for your entire post."
  },
  {
    "objectID": "posts/templatepost/analysis/paper/index.html#motivations",
    "href": "posts/templatepost/analysis/paper/index.html#motivations",
    "title": "Your Engaging Title Here: A Learning Journey",
    "section": "1.1 Motivations",
    "text": "1.1 Motivations\n\nWhy explore [topic]? - [Personal reason 1: specific problem you faced] - [Practical need 2: gap in your workflow] - [Learning goal 3: skill you wanted to develop] - [Curiosity 4: interesting question you had]"
  },
  {
    "objectID": "posts/templatepost/analysis/paper/index.html#objectives",
    "href": "posts/templatepost/analysis/paper/index.html#objectives",
    "title": "Your Engaging Title Here: A Learning Journey",
    "section": "1.2 Objectives",
    "text": "1.2 Objectives\n\nWhat I wanted to accomplish: 1. [Specific, measurable objective 1] 2. [Specific, measurable objective 2] 3. [Specific, measurable objective 3] 4. [Stretch goal or advanced concept]\nDisclaimer: I’m documenting my learning process here. If you spot errors or have better approaches, please let me know! 💙\n\n\n\n\nAtmospheric image to maintain visual engagement - replace with relevant scene"
  },
  {
    "objectID": "posts/templatepost/analysis/paper/index.html#looking-for-relationships",
    "href": "posts/templatepost/analysis/paper/index.html#looking-for-relationships",
    "title": "Your Engaging Title Here: A Learning Journey",
    "section": "5.1 Looking for Relationships",
    "text": "5.1 Looking for Relationships\n\n# Find strongest correlations with MPG\ncorrelations &lt;- cor(mtcars_clean %&gt;% select(where(is.numeric))) %&gt;%\n  as.data.frame() %&gt;%\n  rownames_to_column(\"var1\") %&gt;%\n  pivot_longer(-var1, names_to = \"var2\", values_to = \"correlation\") %&gt;%\n  filter(var1 == \"mpg\", var2 != \"mpg\") %&gt;%\n  arrange(desc(abs(correlation)))\n\n# Display top 5\nkable(correlations %&gt;% head(5),\n      caption = \"Top 5 Correlations with MPG (fuel efficiency)\")\n\n🔍 Weight has the strongest correlation with MPG (r = -0.87). Let’s visualize that relationship:\n\nknitr::include_graphics(\"figures/correlation-plot.png\")\n\n\n\n\nStrong negative relationship between vehicle weight and fuel efficiency. Heavier cars consistently get worse mileage, regardless of cylinder count. The fitted regression line (dashed) shows the overall trend.\n\n\n\n\nInteresting! Heavier cars consistently get worse mileage. Makes sense when you think about it 🚗"
  },
  {
    "objectID": "posts/templatepost/analysis/paper/index.html#making-predictions",
    "href": "posts/templatepost/analysis/paper/index.html#making-predictions",
    "title": "Your Engaging Title Here: A Learning Journey",
    "section": "6.1 Making Predictions",
    "text": "6.1 Making Predictions\n\nLet me make some predictions to see how this works in practice:\n\n# Predict MPG for different weights\nnew_data &lt;- tibble(wt = c(2, 3, 4))\nmodel &lt;- readRDS(\"data/derived_data/simple_model.rds\")\npredictions &lt;- predict(model, newdata = new_data, interval = \"confidence\")\n\ncbind(new_data, predictions) %&gt;%\n  kable(digits = 2,\n        caption = \"Predicted MPG for Vehicles of Different Weights\")\n\n📝 So a 2,000 lb car gets ~30 MPG, while a 4,000 lb car only gets ~15 MPG. That’s quite a difference!"
  },
  {
    "objectID": "posts/templatepost/analysis/paper/index.html#things-to-watch-out-for",
    "href": "posts/templatepost/analysis/paper/index.html#things-to-watch-out-for",
    "title": "Your Engaging Title Here: A Learning Journey",
    "section": "7.1 Things to Watch Out For",
    "text": "7.1 Things to Watch Out For\n\nA few gotchas I encountered while working on this:\n\nDon’t extrapolate too far - This model works for weights between 1.5-5.5 thousand lbs. Predicting outside that range? Risky!\nCorrelation ≠ Causation - Weight correlates with MPG, but there are confounding variables (engine size, aerodynamics, etc.)\nCheck your assumptions - Always plot residuals! A good R² doesn’t guarantee your model is appropriate.\nSmall sample size - We only have 32 cars. Take the confidence intervals seriously!\n\n\n\n\n\nConcluding visual - tie back to topic theme"
  },
  {
    "objectID": "posts/templatepost/analysis/paper/index.html#lessons-learnt",
    "href": "posts/templatepost/analysis/paper/index.html#lessons-learnt",
    "title": "Your Engaging Title Here: A Learning Journey",
    "section": "8.1 Lessons Learnt",
    "text": "8.1 Lessons Learnt\nHere’s what I took away from this exploration:\nConceptual Understanding: - Vehicle weight is a strong predictor of fuel efficiency (R² = 0.75) - Each 1,000 lbs reduces MPG by ~5.3 miles (95% CI: [-6.5, -4.1]) - Cylinder count effects are partially mediated through weight - Simple models can be surprisingly effective with the right predictor\nTechnical Skills: - Using broom::tidy() for clean model output formatting ✅ - Calculating and interpreting confidence intervals for predictions - Creating diagnostic plots to validate regression assumptions - Combining multiple ggplot visualizations with patchwork\nGotchas and Pitfalls: - Always check residual plots - R² alone isn’t enough! - Extrapolation beyond data range is dangerous - Small sample sizes (n=32) require cautious interpretation - Correlation doesn’t prove causation (confounding variables matter)"
  },
  {
    "objectID": "posts/templatepost/analysis/paper/index.html#limitations",
    "href": "posts/templatepost/analysis/paper/index.html#limitations",
    "title": "Your Engaging Title Here: A Learning Journey",
    "section": "8.2 Limitations",
    "text": "8.2 Limitations\n\nThis analysis has several limitations to keep in mind:\n\nOld data: mtcars is from 1974 - modern vehicles (hybrids, EVs) behave differently\nSmall sample: Only 32 observations limits statistical power\nMissing variables: Doesn’t account for aerodynamics, transmission type, engine tech\nSimple model: Single predictor ignores important confounders\nLimited scope: Only passenger cars; may not generalize to trucks/SUVs"
  },
  {
    "objectID": "posts/templatepost/analysis/paper/index.html#opportunities-for-improvement",
    "href": "posts/templatepost/analysis/paper/index.html#opportunities-for-improvement",
    "title": "Your Engaging Title Here: A Learning Journey",
    "section": "8.3 Opportunities for Improvement",
    "text": "8.3 Opportunities for Improvement\n\nIf I had more time, here’s what I’d explore next:\n\nMultiple regression - Add cylinder count, horsepower, transmission type\nInteraction effects - Does weight impact differ by number of cylinders?\nModern data - Replicate with 2020+ vehicle data to see how relationships changed\nNon-linear models - Try polynomial regression or splines for better fit\nMachine learning comparison - How does linear regression compare to random forest?\nCausal inference - Use techniques to establish causality, not just correlation"
  },
  {
    "objectID": "posts/testingfordataanalysisworkflow/index.html",
    "href": "posts/testingfordataanalysisworkflow/index.html",
    "title": "Constructing tests for data analysis workflows",
    "section": "",
    "text": "UCSD Geisel Library - A hub for research and academic discovery\nThe Geisel Library at UC San Diego, where research and innovation converge. TEMPLATE NOTE: Replace this hero image with one relevant to your specific topic while maintaining visual impact and professional appearance. Consider using high-quality images from your field, data visualizations, or compelling stock photos that set the tone for your content."
  },
  {
    "objectID": "posts/testingfordataanalysisworkflow/index.html#subsection-1.1-more-specific-topic",
    "href": "posts/testingfordataanalysisworkflow/index.html#subsection-1.1-more-specific-topic",
    "title": "Constructing tests for data analysis workflows",
    "section": "3.1 Subsection 1.1: [More Specific Topic]",
    "text": "3.1 Subsection 1.1: [More Specific Topic]\n\n[More detailed explanation or variation]\n\n\n\nOptional supporting visualization with descriptive caption"
  },
  {
    "objectID": "posts/testingfordataanalysisworkflow/index.html#subsection-2.1-handling-edge-cases",
    "href": "posts/testingfordataanalysisworkflow/index.html#subsection-2.1-handling-edge-cases",
    "title": "Constructing tests for data analysis workflows",
    "section": "4.1 Subsection 2.1: [Handling Edge Cases]",
    "text": "4.1 Subsection 2.1: [Handling Edge Cases]\n\n[Discussion of potential issues and solutions]\n\n# Replace with your actual error handling code\n# tryCatch({\n#   risky_operation(data)\n# }, error = function(e) {\n#   message(\"Error handled: \", e$message)\n# })"
  },
  {
    "objectID": "posts/testingfordataanalysisworkflow/index.html#model-assumptions",
    "href": "posts/testingfordataanalysisworkflow/index.html#model-assumptions",
    "title": "Constructing tests for data analysis workflows",
    "section": "8.1 Model Assumptions",
    "text": "8.1 Model Assumptions\n\n[Assumption 1]: [e.g., Linearity assumption - check with residual plots]\n[Assumption 2]: [e.g., Independence of observations]\n[Assumption 3]: [e.g., Homoscedasticity - constant variance]"
  },
  {
    "objectID": "posts/testingfordataanalysisworkflow/index.html#data-limitations",
    "href": "posts/testingfordataanalysisworkflow/index.html#data-limitations",
    "title": "Constructing tests for data analysis workflows",
    "section": "8.2 Data Limitations",
    "text": "8.2 Data Limitations\n\nSample size: [Discussion of adequacy for conclusions]\nGeneralizability: [Population this applies to vs. broader populations]\nMissing data: [How missing values were handled and potential bias]"
  },
  {
    "objectID": "posts/testingfordataanalysisworkflow/index.html#method-limitations",
    "href": "posts/testingfordataanalysisworkflow/index.html#method-limitations",
    "title": "Constructing tests for data analysis workflows",
    "section": "8.3 Method Limitations",
    "text": "8.3 Method Limitations\n\n[Limitation 1]: [Explanation and potential workarounds]\n[Limitation 2]: [When this approach may not be appropriate]\nPerformance considerations: [Computational requirements, scalability]"
  },
  {
    "objectID": "posts/testingfordataanalysisworkflow/index.html#academic-literature",
    "href": "posts/testingfordataanalysisworkflow/index.html#academic-literature",
    "title": "Constructing tests for data analysis workflows",
    "section": "11.1 Academic Literature",
    "text": "11.1 Academic Literature\n\n\nPrimary Research Papers:\n\nWickham, H. (2014). “Tidy Data”. Journal of Statistical Software, 59(10), 1-23. https://doi.org/10.18637/jss.v059.i10\nBreiman, L. (2001). “Random Forests”. Machine Learning, 45(1), 5-32. https://doi.org/10.1023/A:1010933404324\n[Your domain-specific paper]. Author, A. (Year). “Relevant Paper Title”. Journal Name, Volume(Issue), pages. DOI\n\nFoundational Books:\n\nWickham, H., & Grolemund, G. (2017). R for Data Science. O’Reilly Media. https://r4ds.had.co.nz/\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). An Introduction to Statistical Learning with Applications in R (2nd ed.). Springer.\n[Your domain book]. Author, B. (Year). Book Title. Publisher.\n\nStatistical Methods:\n\nBox, G. E. P., Jenkins, G. M., Reinsel, G. C., & Ljung, G. M. (2015). Time Series Analysis: Forecasting and Control (5th ed.). Wiley.\nGelman, A., & Hill, J. (2006). Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge University Press."
  },
  {
    "objectID": "posts/testingfordataanalysisworkflow/index.html#blog-posts-and-tutorials",
    "href": "posts/testingfordataanalysisworkflow/index.html#blog-posts-and-tutorials",
    "title": "Constructing tests for data analysis workflows",
    "section": "11.2 Blog Posts and Tutorials",
    "text": "11.2 Blog Posts and Tutorials\n\n\nTechnical Blog Posts:\n\nR-bloggers: “Advanced ggplot2 Techniques” - Comprehensive visualization strategies\nSimply Statistics: “The Role of Statistics in Data Science” - Foundational concepts\nTowards Data Science: “Machine Learning Best Practices” - Practical implementation guidance\n\nPackage-Specific Tutorials:\n\nPackage creator’s blog: “Introduction to [PackageName]” - Official guidance from package authors\nRStudio Blog: “New Features in [Package]” - Updates and best practices\nStack Overflow: “Common [Package] Issues and Solutions” - Community troubleshooting\n\nDomain-Specific Applications:\n\nIndustry blog: “Real-world Application of [Method]” - Practical case studies\nAcademic blog: “Methodological Considerations for [Technique]” - Research perspectives\nPractitioner blog: “Lessons Learned from [Project]” - Implementation insights"
  },
  {
    "objectID": "posts/testingfordataanalysisworkflow/index.html#technical-documentation",
    "href": "posts/testingfordataanalysisworkflow/index.html#technical-documentation",
    "title": "Constructing tests for data analysis workflows",
    "section": "11.3 Technical Documentation",
    "text": "11.3 Technical Documentation\n\n\nPackage Documentation:\n\nPackage Reference Manual - Complete function documentation\nPackage Vignettes - Detailed usage examples\nGitHub Repository - Source code and development issues\n\nLanguage and Framework Guides:\n\nR Language Definition - Official R documentation\nQuarto Documentation - Publishing framework reference\nRMarkdown Cookbook - Advanced document preparation\n\nStandards and Best Practices:\n\nGoogle’s R Style Guide - Code formatting standards\nrOpenSci Packages - Peer-reviewed R packages for research\nCRAN Task Views - Domain-specific package collections"
  },
  {
    "objectID": "posts/testingfordataanalysisworkflow/index.html#community-resources",
    "href": "posts/testingfordataanalysisworkflow/index.html#community-resources",
    "title": "Constructing tests for data analysis workflows",
    "section": "11.4 Community Resources",
    "text": "11.4 Community Resources\n\n\nQ&A and Discussion:\n\nCross Validated - Statistical methodology discussions\nStack Overflow R Tag - Programming troubleshooting\nRStudio Community - User support and discussions\n\nSocial Learning:\n\n#rstats Twitter - Community updates and tips\nR Weekly Newsletter - Curated R news and resources\nR-Ladies Global - Inclusive R community and events\n\nProfessional Networks:\n\nLinkedIn R Groups - Professional networking and job opportunities\nMeetup R Groups - Local community events\nUseR! Conference - Annual R user conference"
  },
  {
    "objectID": "posts/testingfordataanalysisworkflow/index.html#data-sources-and-repositories",
    "href": "posts/testingfordataanalysisworkflow/index.html#data-sources-and-repositories",
    "title": "Constructing tests for data analysis workflows",
    "section": "11.5 Data Sources and Repositories",
    "text": "11.5 Data Sources and Repositories\n\n\nPublic Datasets:\n\nUCI Machine Learning Repository - Benchmark datasets\nKaggle Datasets - Community-contributed data\n[government data portal] - Domain-specific public data\n\nR Built-in Data:\n\ndatasets package - Standard R datasets for examples\n[Your specific dataset source] - Domain-relevant data repositories"
  },
  {
    "objectID": "posts/testingfordataanalysisworkflow/index.html#related-work-and-extensions",
    "href": "posts/testingfordataanalysisworkflow/index.html#related-work-and-extensions",
    "title": "Constructing tests for data analysis workflows",
    "section": "11.6 Related Work and Extensions",
    "text": "11.6 Related Work and Extensions\n\n\nMethodological Extensions:\n\nAuthor, C. (Year). “Extension of [Your Method]”. Journal, Volume(Issue), pages.\nAuthor, D. (Year). “Comparative Analysis of [Related Methods]”. Conference Proceedings.\n\nApplications in Other Domains:\n\nAuthor, E. (Year). “Application to [Different Field]”. Domain Journal, Volume(Issue), pages.\nAuthor, F. (Year). “Cross-disciplinary Perspectives on [Topic]”. Interdisciplinary Journal.\n\n\n\nCitation Note: When using ideas or code from these resources, please cite appropriately. For academic work, use standard citation formats. For blog posts and online resources, include the author, title, publication date, and URL."
  },
  {
    "objectID": "posts/testingfordataanalysisworkflow/index.html#data-availability",
    "href": "posts/testingfordataanalysisworkflow/index.html#data-availability",
    "title": "Constructing tests for data analysis workflows",
    "section": "12.1 Data Availability",
    "text": "12.1 Data Availability\n\nDataset: [Name and source of dataset used]\nAccess: [How others can access the data - URL, package, etc.]\nLicense: [Data usage license and restrictions]"
  },
  {
    "objectID": "posts/testingfordataanalysisworkflow/index.html#code-repository",
    "href": "posts/testingfordataanalysisworkflow/index.html#code-repository",
    "title": "Constructing tests for data analysis workflows",
    "section": "12.2 Code Repository",
    "text": "12.2 Code Repository\n\nGitHub: [Link to repository with complete analysis code]\nCommit: [Specific commit hash for reproducibility]\nEnvironment: [Docker image, renv lockfile, or environment specs]"
  },
  {
    "objectID": "posts/testingfordataanalysisworkflow/index.html#session-information",
    "href": "posts/testingfordataanalysisworkflow/index.html#session-information",
    "title": "Constructing tests for data analysis workflows",
    "section": "12.3 Session Information",
    "text": "12.3 Session Information\n\n\nR version 4.5.2 (2025-10-31)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sequoia 15.6.1\n\nMatrix products: default\nBLAS:   /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/Los_Angeles\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.4 compiler_4.5.2    fastmap_1.2.0     cli_3.6.5        \n [5] tools_4.5.2       htmltools_0.5.9   parallel_4.5.2    yaml_2.3.11      \n [9] rmarkdown_2.30    knitr_1.50        jsonlite_2.0.0    xfun_0.54        \n[13] digest_0.6.39     rlang_1.1.6       evaluate_1.0.5"
  },
  {
    "objectID": "posts/testingfordataanalysisworkflow/index.html#appendix-a-complete-code",
    "href": "posts/testingfordataanalysisworkflow/index.html#appendix-a-complete-code",
    "title": "Constructing tests for data analysis workflows",
    "section": "13.1 Appendix A: Complete Code",
    "text": "13.1 Appendix A: Complete Code\n\n\n# Complete code for easy reproduction - replace with your actual code\n# library(your_packages)\n# data &lt;- load_your_data()\n# results &lt;- your_analysis(data)\n# plot(results)"
  },
  {
    "objectID": "posts/testingfordataanalysisworkflow/index.html#appendix-b-mathematical-details",
    "href": "posts/testingfordataanalysisworkflow/index.html#appendix-b-mathematical-details",
    "title": "Constructing tests for data analysis workflows",
    "section": "13.2 Appendix B: Mathematical Details",
    "text": "13.2 Appendix B: Mathematical Details\n\nFor statistical posts, include relevant formulas using LaTeX notation:\nLinear Regression Model: y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + ... + \\beta_p x_{ip} + \\epsilon_i\nModel Evaluation Metrics: - RMSE: RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y_i})^2} - R-squared: R^2 = 1 - \\frac{SS_{res}}{SS_{tot}}\n[Additional mathematical explanations or derivations as needed]"
  },
  {
    "objectID": "posts/testingfordataanalysisworkflow/index.html#appendix-c-additional-data",
    "href": "posts/testingfordataanalysisworkflow/index.html#appendix-c-additional-data",
    "title": "Constructing tests for data analysis workflows",
    "section": "13.3 Appendix C: Additional Data",
    "text": "13.3 Appendix C: Additional Data\n\n[Additional tables, charts, or data summaries]"
  },
  {
    "objectID": "posts/testingfordataanalysisworkflow/index.html#share-this-post",
    "href": "posts/testingfordataanalysisworkflow/index.html#share-this-post",
    "title": "Constructing tests for data analysis workflows",
    "section": "13.4 Share This Post",
    "text": "13.4 Share This Post\nFound this helpful? Share it with your network:\n\nTwitter\nLinkedIn\nReddit"
  },
  {
    "objectID": "posts/testingfordataanalysisworkflow/index.html#connect-and-discuss",
    "href": "posts/testingfordataanalysisworkflow/index.html#connect-and-discuss",
    "title": "Constructing tests for data analysis workflows",
    "section": "13.5 Connect and Discuss",
    "text": "13.5 Connect and Discuss\nHave questions or suggestions? I’d love to hear from you:\n\nTwitter: @rgt47 - Quick questions and discussions\nLinkedIn: Ronald Glenn Thomas - Professional networking\nGitHub: rgt47 - Code, issues, and contributions\nEmail: Contact through website - Detailed inquiries\n\nComments are enabled below via Utterances - join the discussion!"
  },
  {
    "objectID": "posts/testingfordataanalysisworkflow/index.html#about-the-author",
    "href": "posts/testingfordataanalysisworkflow/index.html#about-the-author",
    "title": "Constructing tests for data analysis workflows",
    "section": "13.6 About the Author",
    "text": "13.6 About the Author\nRonald (Ryy) Glenn Thomas is a biostatistician and data scientist at UC San Diego, specializing in statistical computing, machine learning applications in healthcare, and reproducible research methods. He develops R packages and conducts research at the intersection of statistics, data science, and clinical research.\nConnect: Website | ORCID | Google Scholar"
  },
  {
    "objectID": "posts/zzedcindependence/analysis/paper/index.html",
    "href": "posts/zzedcindependence/analysis/paper/index.html",
    "title": "Taking Control of Your Clinical Trial: Running ZZedc Independently",
    "section": "",
    "text": "The landscape of clinical research data management is changing. Investigators increasingly want full control over their data, systems, and operational decisions. Yet many commonly used electronic data capture (EDC) systems create vendor dependency: expensive licenses, proprietary data formats, and the uncomfortable reality that your system operates at the pleasure of your vendor.\nThis post documents ZZedc, an open-source, investigator-owned EDC platform designed with a different philosophy. ZZedc runs on infrastructure you control, uses standard databases and formats, costs a fraction of commercial EDC systems, and—most importantly—can be deployed and managed by your research team without ongoing dependency on any biostatistics lab or commercial vendor.\n\n\n\nThe independence problem: Why control matters in clinical research\nZZedc overview: What it is and how it differs from commercial EDC systems\nDeployment paths: From a single investigator’s laptop to multi-site trials on AWS\nTechnical architecture: How ZZedc achieves simplicity and security\nGetting started: Step-by-step guides for different deployment scenarios\nLong-term sustainability: Ongoing maintenance, migration, and data ownership"
  },
  {
    "objectID": "posts/zzedcindependence/analysis/paper/index.html#what-this-post-covers",
    "href": "posts/zzedcindependence/analysis/paper/index.html#what-this-post-covers",
    "title": "Taking Control of Your Clinical Trial: Running ZZedc Independently",
    "section": "",
    "text": "The independence problem: Why control matters in clinical research\nZZedc overview: What it is and how it differs from commercial EDC systems\nDeployment paths: From a single investigator’s laptop to multi-site trials on AWS\nTechnical architecture: How ZZedc achieves simplicity and security\nGetting started: Step-by-step guides for different deployment scenarios\nLong-term sustainability: Ongoing maintenance, migration, and data ownership"
  },
  {
    "objectID": "posts/zzedcindependence/analysis/paper/index.html#why-investigator-control-matters",
    "href": "posts/zzedcindependence/analysis/paper/index.html#why-investigator-control-matters",
    "title": "Taking Control of Your Clinical Trial: Running ZZedc Independently",
    "section": "2.1 Why Investigator Control Matters",
    "text": "2.1 Why Investigator Control Matters\nClinical researchers routinely face uncomfortable situations with EDC systems:\nVendor dependency: Your study relies on continued vendor support. If the company changes pricing, goes out of business, or decides to discontinue support, you’re stuck.\nData ownership ambiguity: Proprietary data formats mean your data isn’t truly “yours.” Exporting data can be expensive, slow, or impossible without vendor cooperation.\nOperational constraints: You can’t customize validation rules, reports, or workflows without vendor professional services (and associated costs).\nSecurity concerns: Your patient data sits on vendor infrastructure. You don’t control security, backup locations, or data residency.\nCost escalation: EDC licensing grows with patient volume. A system that costs $20K/year at baseline often costs $50K+ by study completion."
  },
  {
    "objectID": "posts/zzedcindependence/analysis/paper/index.html#common-scenarios-where-investigators-break-free",
    "href": "posts/zzedcindependence/analysis/paper/index.html#common-scenarios-where-investigators-break-free",
    "title": "Taking Control of Your Clinical Trial: Running ZZedc Independently",
    "section": "2.2 Common Scenarios Where Investigators Break Free",
    "text": "2.2 Common Scenarios Where Investigators Break Free\nIn practice, these situations drive investigators to seek alternatives:\nCollaboration breakup: A productive collaboration with a biostatistics lab ends due to cost disagreements or service level mismatches. The investigator is left with data trapped in a vendor system.\nStudy expansion: A pilot study is successful and scales to multi-site. Existing EDC licensing becomes prohibitively expensive. The investigator needs a more scalable approach.\nRegulatory concerns: The investigator’s institution requires data to be stored in a specific location or under specific security controls that the vendor can’t accommodate.\nLong-term stewardship: The study becomes long-term follow-up. Vendor relationship doesn’t last 5+ years, but the investigator must maintain the system and data indefinitely.\nMethodological evolution: As analysis evolves, the investigator needs flexible validation rules, custom reports, and integration with analysis tools. The vendor system feels restrictive."
  },
  {
    "objectID": "posts/zzedcindependence/analysis/paper/index.html#existing-solutions-have-limitations",
    "href": "posts/zzedcindependence/analysis/paper/index.html#existing-solutions-have-limitations",
    "title": "Taking Control of Your Clinical Trial: Running ZZedc Independently",
    "section": "2.3 Existing Solutions Have Limitations",
    "text": "2.3 Existing Solutions Have Limitations\n\n\n\n\n\n\n\n\nSolution\nAdvantage\nLimitation\n\n\n\n\nCommercial EDC (REDCap, Medidata)\nPolished interface, vendor support\nHigh cost, vendor lock-in, data ownership questions\n\n\nSpreadsheets (Excel, Google Sheets)\nFamiliar, free\nNo validation, poor audit trail, compliance issues\n\n\nHomebrew databases (Access, FileMaker)\nCustomizable\nNo security, poor scalability, compliance nightmare\n\n\nCustom R/Shiny apps\nCompletely flexible\nRequires skilled programmer, no pre-built features\n\n\nZZedc\nOpen source, low cost, independent\nRequires basic technical setup"
  },
  {
    "objectID": "posts/zzedcindependence/analysis/paper/index.html#core-design-philosophy",
    "href": "posts/zzedcindependence/analysis/paper/index.html#core-design-philosophy",
    "title": "Taking Control of Your Clinical Trial: Running ZZedc Independently",
    "section": "3.1 Core Design Philosophy",
    "text": "3.1 Core Design Philosophy\nInvestigator-centered: Every design decision prioritizes your control. You own the data, the system, and the infrastructure.\nCloud-native but independent: Deploy on AWS, Azure, Google Cloud, or local servers. You choose the infrastructure provider based on cost and compliance needs.\nSecurity and compliance by default: GDPR and 21 CFR Part 11 compliance frameworks built in. Data encryption, audit trails, and electronic signatures are standard features.\nOpen source, not proprietary: Source code is available on GitHub. If you need customization, you can do it yourself or hire any consultant—you’re not locked into the vendor.\nStandards-based: SQLite databases, YAML configuration, standard web technologies. If you need to migrate away, your data is in standard formats."
  },
  {
    "objectID": "posts/zzedcindependence/analysis/paper/index.html#what-zzedc-includes",
    "href": "posts/zzedcindependence/analysis/paper/index.html#what-zzedc-includes",
    "title": "Taking Control of Your Clinical Trial: Running ZZedc Independently",
    "section": "3.2 What ZZedc Includes",
    "text": "3.2 What ZZedc Includes\nElectronic data capture with real-time validation: Enter study data with immediate field-level validation. No waiting for data manager review.\nRole-based access control: Five built-in roles (Admin, PI, Coordinator, Data Manager, Monitor) with configurable permissions.\nComprehensive reporting: Basic enrollment reports, quality control summaries, and statistical overviews—all built-in, no custom programming.\nData quality framework: Automated checks for missing data, outliers, and consistency across visits. Nightly QC runs identify issues early.\nAudit trail and compliance: Every action is logged with user, timestamp, and change history. Electronic signatures supported for regulatory studies.\nData export and analysis: Export to CSV, Excel, SPSS, or R. Integrate with your preferred analysis tool directly.\nUser-friendly admin interface: Create users, manage backups, view audit logs—all from the web interface. No command-line expertise required."
  },
  {
    "objectID": "posts/zzedcindependence/analysis/paper/index.html#architecture-simplicity-and-transparency",
    "href": "posts/zzedcindependence/analysis/paper/index.html#architecture-simplicity-and-transparency",
    "title": "Taking Control of Your Clinical Trial: Running ZZedc Independently",
    "section": "4.1 Architecture: Simplicity and Transparency",
    "text": "4.1 Architecture: Simplicity and Transparency\n┌─────────────────────────────────────────────────┐\n│  Your Infrastructure (AWS, Local, Hybrid)      │\n│                                                  │\n│  ┌──────────────────────────────────────────┐  │\n│  │  Web Browser (Any Location)              │  │\n│  │  https://trial.example.org               │  │\n│  └──────────────────────────────────────────┘  │\n│            ↓ HTTPS (Automatic)                  │\n│  ┌──────────────────────────────────────────┐  │\n│  │  Caddy Reverse Proxy                     │  │\n│  │  (Automatic HTTPS, Let's Encrypt)        │  │\n│  └──────────────────────────────────────────┘  │\n│            ↓ Reverse Proxy                      │\n│  ┌──────────────────────────────────────────┐  │\n│  │  ZZedc R/Shiny Application               │  │\n│  │  (Authentication, Forms, Reporting)      │  │\n│  └──────────────────────────────────────────┘  │\n│            ↓ Database                           │\n│  ┌──────────────────────────────────────────┐  │\n│  │  SQLite Database (Standard Format)       │  │\n│  │  Your Data, Your Control                 │  │\n│  └──────────────────────────────────────────┘  │\n│                                                  │\n└─────────────────────────────────────────────────┘\nKey design points:\n\nAll infrastructure is standard: Docker containers, standard web server (Caddy), open-source database (SQLite)\nNo proprietary components: You’re not dependent on any vendor-specific technology\nTransparent processes: You can see, audit, and modify every part of the system\nPortable data: Your data is in SQLite—you can access it with any SQL tool, analyze it with any tool, migrate it anywhere"
  },
  {
    "objectID": "posts/zzedcindependence/analysis/paper/index.html#deployment-paths-flexibility-for-different-needs",
    "href": "posts/zzedcindependence/analysis/paper/index.html#deployment-paths-flexibility-for-different-needs",
    "title": "Taking Control of Your Clinical Trial: Running ZZedc Independently",
    "section": "4.2 Deployment Paths: Flexibility for Different Needs",
    "text": "4.2 Deployment Paths: Flexibility for Different Needs\nZZedc supports multiple deployment approaches, depending on your scale and technical resources:\n\n4.2.1 Path 1: Solo Researcher (Local Laptop)\nBest for: Individual investigators, small pilot studies, initial prototyping\nInstall on your personal laptop in 10 minutes:\n# 1. Install ZZedc package from CRAN\ninstall.packages(\"zzedc\")\n\n# 2. Initialize project with interactive setup\nlibrary(zzedc)\nzzedc::init()  # Answers 15 simple questions\n\n# 3. Launch\nlaunch_zzedc()\n# Application opens in browser at http://localhost:3838\nData lives in a local SQLite file (data/zzedc.db). You’re the only user. Backup by copying the file to cloud storage.\nCost: Free (only your laptop electricity) Maintenance: Minimal (you run it when needed)\n\n\n4.2.2 Path 2: Team Research (Single AWS Server)\nBest for: Research team at single institution, collaborative multi-site trial\nDeploy on AWS EC2 in ~15 minutes:\n# 1. Install AWS CLI, configure credentials\n\n# 2. Run deployment script\n./aws_setup.sh \\\n  --region us-west-2 \\\n  --study-name \"Depression Treatment Trial\" \\\n  --study-id \"DEPR-2025-001\" \\\n  --admin-password \"SecurePass123!\" \\\n  --domain trial.example.org \\\n  --instance-type t3.medium\nApplication runs on AWS infrastructure. Multiple team members access via HTTPS. Database backed up automatically.\nCost: ~$30-50/month for EC2 instance (or less, depending on size) Maintenance: Basic (Docker handles updates, Caddy handles HTTPS renewal)\n\n\n4.2.3 Path 3: Enterprise/Multi-Site\nBest for: Large NIH-funded studies, pharmaceutical trials, production deployments\nDeploy across multiple AWS availability zones with load balancing, RDS database, S3 backup, CloudWatch monitoring.\nCost: $200-500/month depending on data volume Maintenance: Automated (infrastructure-as-code, CI/CD pipeline)"
  },
  {
    "objectID": "posts/zzedcindependence/analysis/paper/index.html#security-and-compliance",
    "href": "posts/zzedcindependence/analysis/paper/index.html#security-and-compliance",
    "title": "Taking Control of Your Clinical Trial: Running ZZedc Independently",
    "section": "4.3 Security and Compliance",
    "text": "4.3 Security and Compliance\nZZedc includes security and compliance frameworks that commercial EDC systems charge extra for:\nGDPR Compliance: - Data subject rights portal (access your data, request deletion) - Purpose limitation (users only see data they need) - Audit trail of all access\n21 CFR Part 11 (FDA): - Electronic signatures with role-based authorization - Immutable audit trail with hash chaining - System validation framework\nSecurity Baseline: - Password encryption with configurable salt - HTTPS with automatic Let’s Encrypt certificates - Role-based access control - Session timeout and concurrent login limits"
  },
  {
    "objectID": "posts/zzedcindependence/analysis/paper/index.html#scenario-1-solo-researcher-prototype",
    "href": "posts/zzedcindependence/analysis/paper/index.html#scenario-1-solo-researcher-prototype",
    "title": "Taking Control of Your Clinical Trial: Running ZZedc Independently",
    "section": "5.1 Scenario 1: Solo Researcher Prototype",
    "text": "5.1 Scenario 1: Solo Researcher Prototype\nDr. Jane is a clinical psychologist planning a small depression treatment study. She wants to test intervention feasibility before seeking NIH funding.\nHer approach:\n# Install on her laptop\ninstall.packages(\"zzedc\")\nlibrary(zzedc)\n\n# Quick setup (5 minutes of questions)\nzzedc::init()\n# - Study name: \"Depression CBT Pilot\"\n# - Target enrollment: 20\n# - Admin username: jane_smith\n# - Password: (secure password)\n\n# Launch\nlaunch_zzedc()\n# App opens at http://localhost:3838\nResult: Jane has a secure, validated EDC system running locally. She can: - Create forms for baseline, weekly, and endpoint visits - Enroll patients and enter data - Generate enrollment reports - Export data to Excel for analysis - Back up by copying a single file to Dropbox\nCost: $0 Timeline: 15 minutes from zero to collecting data"
  },
  {
    "objectID": "posts/zzedcindependence/analysis/paper/index.html#scenario-2-multi-site-trial-migration",
    "href": "posts/zzedcindependence/analysis/paper/index.html#scenario-2-multi-site-trial-migration",
    "title": "Taking Control of Your Clinical Trial: Running ZZedc Independently",
    "section": "5.2 Scenario 2: Multi-Site Trial Migration",
    "text": "5.2 Scenario 2: Multi-Site Trial Migration\nThe ADHD research consortium at 5 universities currently uses an expensive commercial EDC that costs $40K/year and is inflexible. They want to migrate to something more affordable and customizable.\nTheir approach:\n# IT staff deploys to AWS\n./aws_setup.sh \\\n  --region us-west-2 \\\n  --study-name \"Multisite ADHD Trial\" \\\n  --study-id \"ADHD-MULTI-2025\" \\\n  --admin-password \"SecurePassword123!\" \\\n  --domain adhd-trial.org \\\n  --instance-type t3.large  # Larger instance for multi-site\nResult: - Single centralized instance accessible from all 5 sites - HTTPS with automatic security certificates - Role-based access: 2 administrators, 5 principal investigators, 20 coordinators, 5 data managers - Data shared securely across institutions - Monthly cost: $40 (95% cheaper than commercial EDC)\nCost: $40/month infrastructure + staff time for administration Timeline: 1 week from decision to enrollment opened"
  },
  {
    "objectID": "posts/zzedcindependence/analysis/paper/index.html#scenario-3-individual-investigator-independence",
    "href": "posts/zzedcindependence/analysis/paper/index.html#scenario-3-individual-investigator-independence",
    "title": "Taking Control of Your Clinical Trial: Running ZZedc Independently",
    "section": "5.3 Scenario 3: Individual Investigator Independence",
    "text": "5.3 Scenario 3: Individual Investigator Independence\nDr. Robert was running a study with a local Biostatistics Lab that managed his EDC. The lab relationship deteriorated due to cost escalation and support issues. He wants to take control.\nThe situation: - His data is in a commercial EDC (vendor owns the data format) - Migration would cost $20K to export and reformat - He needs a system that’s independent from any vendor - He has basic IT skills but doesn’t want to manage Linux servers\nHis solution: 1. Deploy ZZedc independently on AWS (not through the Biostatistics Lab) bash    ./aws_setup.sh --region us-west-1 --study-name \"Robert's Study\" \\      --study-id \"ROBERT-2025\" --domain robert-study.org\n\nMigrate his data from the old system to ZZedc\n\nOld system exports to CSV\nCSV imported into ZZedc\nData is now in standard SQLite format\n\nComplete his study independently\n\nData entry continues in ZZedc\nAnalysis done with R/Python (direct database access)\nFinal data archived as standard database file\n\nEnd of study\n\nData archived in standard SQLite format to institutional repository\nSystem shut down (delete EC2 instance)\nOngoing data access requires only free SQLite tools\n\n\nResult: Dr. Robert owns his data and system. If he wants to work with another Biostatistics Lab in the future, he can—without vendor lock-in."
  },
  {
    "objectID": "posts/zzedcindependence/analysis/paper/index.html#for-solo-researchers",
    "href": "posts/zzedcindependence/analysis/paper/index.html#for-solo-researchers",
    "title": "Taking Control of Your Clinical Trial: Running ZZedc Independently",
    "section": "6.1 For Solo Researchers",
    "text": "6.1 For Solo Researchers\nRequirements: R installed, ~5 minutes\n# Step 1: Install\ninstall.packages(\"zzedc\")\n\n# Step 2: Load and initialize\nlibrary(zzedc)\nzzedc::init()\n\n# Step 3: Answer interactive questions\n# The system guides you through setup\n\n# Step 4: Launch\nSys.setenv(ZZEDC_SALT = \"...\")  # (from setup output)\nlaunch_zzedc()\n\n# Step 5: Open browser to http://localhost:3838\n# Step 6: Login with admin credentials you chose\nDetailed instructions: See vignettes/quick-start-solo-researcher.Rmd"
  },
  {
    "objectID": "posts/zzedcindependence/analysis/paper/index.html#for-aws-deployment",
    "href": "posts/zzedcindependence/analysis/paper/index.html#for-aws-deployment",
    "title": "Taking Control of Your Clinical Trial: Running ZZedc Independently",
    "section": "6.2 For AWS Deployment",
    "text": "6.2 For AWS Deployment\nRequirements: AWS account, AWS CLI, ~15 minutes plus setup time\nStep 1: Prepare\n# Ensure AWS credentials configured\naws sts get-caller-identity\n\n# Gather information\n# - Study name\n# - Study protocol ID\n# - Domain name (e.g., trial.example.org)\n# - Admin password (8+ characters)\nStep 2: Deploy\ncd deployment/\nchmod +x aws_setup.sh\n\n./aws_setup.sh \\\n  --region us-west-2 \\\n  --study-name \"Your Study\" \\\n  --study-id \"YOUR-STUDY-ID\" \\\n  --admin-password \"SecurePassword123!\" \\\n  --domain trial.example.org \\\n  --instance-type t3.medium\nStep 3: Point domain to instance - Wait for DNS to propagate (can take up to 24 hours) - Access application at https://trial.example.org\nDetailed instructions: See deployment/AWS_DEPLOYMENT_GUIDE.md or vignettes/quick-start-aws-devops.Rmd"
  },
  {
    "objectID": "posts/zzedcindependence/analysis/paper/index.html#for-migration-from-other-systems",
    "href": "posts/zzedcindependence/analysis/paper/index.html#for-migration-from-other-systems",
    "title": "Taking Control of Your Clinical Trial: Running ZZedc Independently",
    "section": "6.3 For Migration from Other Systems",
    "text": "6.3 For Migration from Other Systems\nMigrating from commercial EDC, Excel, or other sources?\n\nExport from source system (typically CSV or Excel)\nImport into ZZedc using the data loader tools\nVerify data quality in ZZedc’s validation interface\nComplete study in ZZedc\nArchive final data as standard SQLite database\n\nDetailed migration guides available in documentation."
  },
  {
    "objectID": "posts/zzedcindependence/analysis/paper/index.html#ownership-and-control",
    "href": "posts/zzedcindependence/analysis/paper/index.html#ownership-and-control",
    "title": "Taking Control of Your Clinical Trial: Running ZZedc Independently",
    "section": "7.1 Ownership and Control",
    "text": "7.1 Ownership and Control\nWhen you deploy ZZedc, you own: - The infrastructure: Your EC2 instance, your VPC, your data storage - The data: Standard SQLite format, completely portable - The system configuration: You control every setting - The codebase: Open source on GitHub; you can fork and modify if needed"
  },
  {
    "objectID": "posts/zzedcindependence/analysis/paper/index.html#maintenance-and-updates",
    "href": "posts/zzedcindependence/analysis/paper/index.html#maintenance-and-updates",
    "title": "Taking Control of Your Clinical Trial: Running ZZedc Independently",
    "section": "7.2 Maintenance and Updates",
    "text": "7.2 Maintenance and Updates\nMonthly: - Monitor disk usage and instance health - Check logs for errors\nQuarterly: - Update OS packages on EC2 - Update ZZedc package if new version available - Test backup/restore procedure\nAnnually: - Security audit - Capacity planning (do you need a larger instance?) - Archive completed studies\nSee IT_STAFF_DEPLOYMENT_CHECKLIST.md and IT_STAFF_TROUBLESHOOTING.md for detailed guidance."
  },
  {
    "objectID": "posts/zzedcindependence/analysis/paper/index.html#migration-path",
    "href": "posts/zzedcindependence/analysis/paper/index.html#migration-path",
    "title": "Taking Control of Your Clinical Trial: Running ZZedc Independently",
    "section": "7.3 Migration Path",
    "text": "7.3 Migration Path\nIf you want to migrate away from ZZedc in the future:\n\nExport database:\n# SQLite is a standard database format\nsqlite3 zzedc.db \".dump\" &gt; database_export.sql\nAccess with any tool: Your data can be accessed by any SQL tool, Python, R, Stata, SAS, etc.\nComplete ownership: No vendor locks, no proprietary formats"
  },
  {
    "objectID": "posts/zzedcindependence/analysis/paper/index.html#next-steps",
    "href": "posts/zzedcindependence/analysis/paper/index.html#next-steps",
    "title": "Taking Control of Your Clinical Trial: Running ZZedc Independently",
    "section": "9.1 Next Steps",
    "text": "9.1 Next Steps\nStart exploring: - Solo researcher? See vignettes/quick-start-solo-researcher.Rmd - Team with AWS? See vignettes/quick-start-aws-devops.Rmd - IT staff deploying? See deployment/IT_STAFF_DEPLOYMENT_CHECKLIST.md - Need help? Visit https://github.com/rgt47/zzedc for documentation and issues\nThe goal of ZZedc is simple: Put clinical research data management back in the hands of investigators."
  },
  {
    "objectID": "posts/zzedcindependence/analysis/paper/index.html#resources",
    "href": "posts/zzedcindependence/analysis/paper/index.html#resources",
    "title": "Taking Control of Your Clinical Trial: Running ZZedc Independently",
    "section": "9.2 Resources",
    "text": "9.2 Resources\n\nZZedc GitHub: https://github.com/rgt47/zzedc\nDocumentation: See vignettes/ directory for comprehensive guides\nDeployment guides: See deployment/ directory for AWS, Docker, and operational guides\nSupport: Open an issue on GitHub or contact zzedc@ucsd.edu\n\n\nPublished: December 2025 Author: Clinical Research Technology Team License: Open source (see GitHub repository for license details) Status: Production ready"
  },
  {
    "objectID": "references/index.html",
    "href": "references/index.html",
    "title": "References",
    "section": "",
    "text": "Quick reference materials for when you need answers fast. These living documents are continuously updated and expanded based on real-world usage.\nFind: - Command cheat sheets - Configuration templates - Common patterns and snippets - Troubleshooting checklists - Best practices summaries\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\nCategories\n\n\n\nDescription\n\n\n\n\n\n\n\n\nR Commands Quick Reference\n\n\nR, reference, cheatsheet\n\n\nQuick lookup table of commonly used R commands for data manipulation, visualization, and analysis.\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "research/index.html",
    "href": "research/index.html",
    "title": "Research",
    "section": "",
    "text": "321 total publications spanning multiple research domains in biostatistics, clinical trials, and medical research.\n\n\nUse the search box below to filter publications by title, author, journal, topic, or year.\n\n  \n    \n      \n      \n    \n  \n  \n    \n      All Categories\n      Medical/Clinical Research (213)\n      Statistical Methods (92)\n      Military/Defense Research (8)\n      Neuroimaging/Biomarkers (5)\n      Public Health/COVID (3)"
  },
  {
    "objectID": "research/index.html#publications-research",
    "href": "research/index.html#publications-research",
    "title": "Research",
    "section": "",
    "text": "321 total publications spanning multiple research domains in biostatistics, clinical trials, and medical research.\n\n\nUse the search box below to filter publications by title, author, journal, topic, or year.\n\n  \n    \n      \n      \n    \n  \n  \n    \n      All Categories\n      Medical/Clinical Research (213)\n      Statistical Methods (92)\n      Military/Defense Research (8)\n      Neuroimaging/Biomarkers (5)\n      Public Health/COVID (3)"
  },
  {
    "objectID": "research/index.html#section",
    "href": "research/index.html#section",
    "title": "Research",
    "section": "2024",
    "text": "2024\n\nA multicenter, randomized, double-blind, placebo-controlled ascending dose study to evaluate the safety, tolerability, pharmacokinetics (PK) and pharmacodynamic (PD) effects of Posiphen in subjects with Early Alzheimer's Disease\n Galasko, Douglas, Farlow, Martin R, Lucey, Brendan P, Honig, Lawrence S, Elbert, Donald, Bateman, Randall, Momper, Jeremiah, **Thomas, Ronald G**, Rissman, Robert A, Pa, Judy, & others | Alzheimer's Research \\& Therapy | (2024) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Clinical trials\nLinks: 📄 Article\n\n\nCumulative blast impulse is predictive for changes in chronic neurobehavioral symptoms following low level blast exposure during military training\n McEvoy, Cory, Crabtree, Adam, Case, John, Means, Gary E, Muench, Peter, **Thomas, Ronald G**, Ivory, Rebecca A, Mihalik, Jason, & Meabon, James S | Military medicine | (2024) \nSummary: Investigating health impacts in military populations and combat environments\nTopics: Biostatistics R Military health\nLinks: 📄 Article • 📋 Open Access PDF\n\n\nIncreased [18F] Fluorodeoxyglucose Uptake in the Left Pallidum in Military Veterans with Blast-Related Mild Traumatic Brain Injury: Potential as an Imaging Biomarker and Mediation with Executive Dysfunction and Cognitive Impairment\n Terry, Garth, Pagulayan, Kathleen F, Muzi, Mark, Mayer, Cynthia, Murray, Daniel R, Schindler, Abigail G, Richards, Todd L, McEvoy, Cory, Crabtree, Adam, McNamara, Chris, & others | Journal of Neurotrauma | (2024) \nSummary: Investigating health impacts in military populations and combat environments\nTopics: Biostatistics R Traumatic brain injury Military health Neuroimaging Biomarkers Cognitive decline\nLinks: 📄 Article"
  },
  {
    "objectID": "research/index.html#section-1",
    "href": "research/index.html#section-1",
    "title": "Research",
    "section": "2023",
    "text": "2023\n\nA public resource of baseline data from the Alzheimer's Prevention Initiative Autosomal-Dominant Alzheimer's Disease Trial\n Reiman, Eric M, Pruzin, Jeremy J, Rios-Romenets, Silvia, Brown, Chris, Giraldo, Margarita, Acosta-Baena, Natalia, Tobon, Carlos, Hu, Nan, Chen, Yinghua, Ghisays, Valentina, & others | Alzheimer's \\& Dementia | (2023) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Prevention trials\nLinks: 📄 Article • 📋 Open Access PDF\n\n\nA randomized controlled clinical trial of prazosin for alcohol use disorder in active duty soldiers: Predictive effects of elevated cardiovascular parameters\n Raskind, Murray A, Williams, Tammy, Holmes, Hollie, Hart, Kim, Crews, Laura, Poupore, Eileen L, **Thomas, Ronald G**, Darnell, Jolee, Daniels, Colin, Goke, Kevin, & others | Alcoholism: Clinical and Experimental Research | (2023) \nSummary: Evaluating therapeutic interventions through rigorous clinical trials\nTopics: Biostatistics R Clinical trials\nLinks: 📄 Article • 📋 Open Access PDF\n\n\nFDG-PET as a Clinical Diagnostic Biomarker for Repetitive Blast Mild Traumatic Brain Injury\n Terry, Garth, Pagulayan, Kati, Muzi, Mark, Mayer, Cynthia, Murray, Daniel, Schindler, Abigail, Richards, Todd, McEvoy, Cory, Crabtree, Adam, McNamara, Chris, & others | NEUROPSYCHOPHARMACOLOGY | (2023) \nSummary: Investigating health impacts in military populations and combat environments\nTopics: Biostatistics R Traumatic brain injury Neuroimaging Biomarkers\nLinks: 📄 Article\n\n\nImpact of reference region on longitudinal florbetapir PET SUVR changes from the API ADAD Colombia Trial\n Ghisays, Valentina, Lopera, Francisco, Su, Yi, Malek-Ahmadi, Michael H, Chen, Yinghua, Protas, Hillary D, Luo, Ji, Hu, Nan, Clayton, David, Schiffman, Courtney, & others | Alzheimer's \\& Dementia | (2023) \nSummary: Developing biomarkers and imaging techniques for disease detection\nTopics: Biostatistics R Neuroimaging Longitudinal studies\nLinks: 📄 Article\n\n\nLongitudinal Sleep Patterns and Cognitive Impairment in Older Adults\n Keil, Samantha A, Schindler, Abigail G, Wang, Marie X, Piantino, Juan, Silbert, Lisa C, Elliott, Jonathan E, Werhane, Madeleine L, **Thomas, Ronald G**, Willis, Sherry, Lim, Miranda M, & others | JAMA Network Open | (2023) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R Sleep disorders Cognitive decline Longitudinal studies\nLinks: 📄 Article\n\n\nPontine pathology mediates common symptoms of blast-induced chronic mild traumatic brain injury\n Meabon, James S, Schindler, Abigail G, Murray, Daniel R, Colasurdo, Elizabeth A, Sikkema, Carl L, Rodriguez, Joshua W, Omer, Mohamed, Cline, Marcella M, Logsdon, Aric F, Cross, Donna J, & others | medRxiv | (2023) \nSummary: Investigating health impacts in military populations and combat environments\nTopics: Biostatistics R Traumatic brain injury\nLinks: 📄 Article • 📋 Open Access PDF\n\n\nPrognostic value of plasma biomarkers in a clinical trial of mild-to-moderate Alzheimer's Disease\n Qiu, Yuqi, Messer, Karen, Jacobs, Diane M, Salmon, David P, Kaplita, Stephen, Wellington, Cheryl L, Stukas, Sophie K, Askew, Brianna, Brewer, James B, Brody, Mark, & others | Alzheimer's \\& Dementia | (2023) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Clinical trials Biomarkers\nLinks: 📄 Article\n\n\nThe relative contribution of COVID-19 infection versus COVID-19 related occupational stressors to insomnia in healthcare workers\n Hendrickson, Rebecca C, McCall, Catherine A, Rosser, Aaron F, Pagulayan, Kathleen F, Chang, Bernard P, Sano, Ellen D, **Thomas, Ronald G**, & Raskind, Murray A | Sleep medicine: X | (2023) \nSummary: Addressing public health challenges during global health crises\nTopics: Biostatistics R COVID-19 Sleep disorders\nLinks: 📄 Article • 📋 Open Access PDF"
  },
  {
    "objectID": "research/index.html#section-2",
    "href": "research/index.html#section-2",
    "title": "Research",
    "section": "2022",
    "text": "2022\n\nMarkers of Cerebrovascular Injury, Inflammation, and Plasma Lipids Are Associated with Alzheimer's Disease Cerebrospinal Fluid Biomarkers in Cognitively Normal Persons\n Jansson, Deidre, Wang, Marie, **Thomas, Ronald G.**, Erickson, Michelle A., Peskind, Elaine R., Li, Ge, & Iliff, Jeffrey | Journal of Alzheimer's Disease | (2022) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: biomarkers blood-brain barrier cerebrospinal fluid hdl ldl ptau tau Alzheimer's disease Biomarkers Cognitive decline\nLinks: 📄 Article\n\n\nSex differences in cognitive resilience in preclinical autosomal-dominant Alzheimer's disease carriers and non-carriers: baseline findings from the API ADAD Colombia Trial\n Vila-Castelar, Clara, Tariot, Pierre N, Sink, Kaycee M, Clayton, David, Langbaum, Jessica B, **Thomas, Ronald G**, Chen, Yinghua, Su, Yi, Chen, Kewei, Hu, Nan, & others | Alzheimer's \\& Dementia | (2022) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Cognitive decline\nLinks: 📄 Article\n\n\nT2 Protect AD: Achieving a rapid recruitment timeline in a multisite clinical trial for individuals with mild to moderate Alzheimer's disease\n Shadyab, Aladdin H, LaCroix, Andrea Z, Matthews, Genevieve, Bennett, Daniel, Shadyab, Alexandre A, Tan, Donna, **Thomas, Ronald G**, Mason, Jennifer, Lopez, Alex, Askew, Brianna, & others | Alzheimer's \\& Dementia: Translational Research \\& Clinical Interventions | (2022) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Clinical trials\nLinks: 📄 Article • 📋 Open Access PDF\n\n\nThe impact of the COVID-19 pandemic on mental health, occupational functioning, and professional retention among health care workers and first responders\n Hendrickson, Rebecca C & Slevin, Rois\\'\\i | Journal of general internal medicine | (2022) \nSummary: Addressing public health challenges during global health crises\nTopics: Biostatistics R COVID-19\nLinks: 📄 Article • 📋 Open Access PDF"
  },
  {
    "objectID": "research/index.html#section-3",
    "href": "research/index.html#section-3",
    "title": "Research",
    "section": "2021",
    "text": "2021\n\nA trial of gantenerumab or solanezumab in dominantly inherited Alzheimer's disease\n Salloway, Stephen, Farlow, Martin, McDade, Eric, Clifford,    David B, Wang, Guoqiao, Llibre-Guerra, Jorge J, Hitchcock,      Janice M, Mills, Susan L, Santacruz, Anna M, Aschenbrenner,     Andrew J, Hassenstab, Jason, Benzinger, Tammie L S, Gordon,     Brian A, Fagan, Anne M, Coalier, Kelley A, Cruchaga, Carlos, Goate, Alison A, Perrin, Richard J, Xiong, Chengjie, ..., S\\'a | Nature Medicine | (2021) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease\nLinks: 📄 Article\n\n\nAmyloid and tau pathology associations with personality traits, neuropsychiatric symptoms, and cognitive lifestyle in the preclinical phases of sporadic and autosomal dominant Alzheimer’s disease\n Binette, Alexa Pichet, Vachon-Presseau, Etienne, Morris, John, Bateman, Randall, Benzinger, Tammie, Collins, D Louis, Poirier, Judes, Breitner, John CS, Villeneuve, Sylvia, Allegri, Ricardo, & others | Biological psychiatry | (2021) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Cognitive decline\n\n\nOptimizing aggregated N-of-1 trial designs for predictive biomarker validation: statistical methods and theoretical findings\n Hendrickson, Rebecca C, **Thomas, Ronald G**, Schork, Nicholas J, & Raskind, Murray A | Creating Evidence from Real World Patient Digital Data | (2021) \nSummary: Evaluating therapeutic interventions through rigorous clinical trials\nTopics: Biostatistics R Biomarkers Statistical methods\nLinks: 📄 Article • 📋 Open Access PDF\n\n\nPower and sample size for random coefficient regression models in randomized experiments with monotone missing data\n Hu, Nan, Mackey, Howard, & **Thomas, Ronald** | Biometrical Journal | (2021) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R Clinical trials\nLinks: 📄 Article • 📋 Open Access PDF\n\n\nRasagiline effects on glucose metabolism, cognition, and tau in Alzheimer's dementia\n Matthews, Dawn C, Ritter, Aaron, **Thomas, Ronald G**, Andrews, Randolph D, Lukic, Ana S, Revta, Carolyn, Kinney, Jefferson W, Tousi, Babak, Leverenz, James B, Fillit, Howard, & others | Alzheimer's \\& Dementia: Translational Research \\& Clinical Interventions | (2021) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease\n\n\nRecruitment of a multi-site randomized controlled trial of aerobic exercise for older adults with amnestic mild cognitive impairment: the EXERT trial\n Shadyab, Aladdin & others | Alzheimer's \\& Dementia | (2021) \nSummary: Evaluating therapeutic interventions through rigorous clinical trials\nTopics: Biostatistics R Clinical trials Cognitive decline\n\n\nThe Impact of the COVID-19 Pandemic on Mental Health, Occupational   Functioning, and Professional Retention Among Health Care Workers and   First Responders\n Hendrickson, Rebecca C & Slevin, Rois\\'\\i | Journal of general internal medicine | (2021) \nSummary: Addressing public health challenges during global health crises\nTopics: Biostatistics R COVID-19"
  },
  {
    "objectID": "research/index.html#section-4",
    "href": "research/index.html#section-4",
    "title": "Research",
    "section": "2020",
    "text": "2020\n\nBaseline demographic, clinical, and cognitive characteristics of the Alzheimer's Prevention Initiative (API) Autosomal-Dominant Alzheimer's Disease Colombia Trial\n Rios-Romenets, Silvia, Lopera, Francisco, Sink, Kaycee M, Hu, Nan, Lian, Qinshu, Guthrie, Heather, Smith, Jillian, Cho, William, Mackey, Howard, Langbaum, Jessica B, & others | Alzheimer's \\& Dementia | (2020) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Cognitive decline Prevention trials\n\n\nDevelopment of a novel cognitive composite outcome to assess therapeutic effects of exercise in the EXERT trial for adults with MCI: The ADAS-Cog-Exec\n Jacobs, Diane M, **Thomas, Ronald G**, Salmon, David P, Jin, Shelia, Feldman, Howard H, Cotman, Carl W, Baker, Laura D, Alzheimer's Disease Cooperative Study EXERT Study Group, & Alzheimer's Disease Neuroimaging Initiative | Alzheimer's \\& Dementia: Translational Research \\& Clinical Interventions | (2020) \nSummary: Evaluating therapeutic interventions through rigorous clinical trials\nTopics: Biostatistics R Cognitive decline\n\n\nOptimizing aggregated n-of-1 trial designs for predictive biomarker validation: statistical methods and theoretical findings\n Hendrickson, Rebecca C, **Thomas, Ronald G**, Schork, Nicholas J, & Raskind, Murray A | Frontiers in Digital Health | (2020) \nSummary: Evaluating therapeutic interventions through rigorous clinical trials\nTopics: Biostatistics R Biomarkers Statistical methods\nLinks: 📄 Article\n\n\nPET evidence of preclinical cerebellar amyloid plaque deposition in autosomal dominant Alzheimer's disease\n Ghisays, Valentina, Lopera, Francisco, Goradia, Dhruman D, Protas, Hillary D, Malek-Ahmadi, Michael H, Chen, Yinghua, Devadas, Vivek, Luo, Ji, Lee, Wendy, Brown, Christopher T, & others | 2020 Alzheimer's Association International Conference | (2020) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Neuroimaging\n\n\nSex differences in neurodegeneration and memory performance in preclinical autosomal dominant Alzheimer’s disease: Baseline findings from the API ADAD trial: Intersections of sex/gender and race/ethnicity in cognitive aging and Alzheimer’s disease trajectories\n Vila-Castelar, Clara, Tariot, Pierre N, Sink, Kaycee M, Clayton, David, Langbaum, Jessica B, **Thomas, Ronald G**, Chen, Yinghua, Su, Yi, Hu, Nan, Giraldo-Chica, Margarita, & others | Alzheimer's \\& Dementia | (2020) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Cognitive decline\n\n\nThe Alzheimer’s Prevention Initiative Composite Cognitive Test: a practical measure for tracking cognitive decline in preclinical Alzheimer’s disease\n Langbaum, Jessica B, Ellison, Noel N, Caputo, Angelika, **Thomas, Ronald G**, Langlois, Carolyn, Riviere, Marie-Emmanuelle, Graf, Ana, Lopez Lopez, Cristina, Reiman, Eric M, Tariot, Pierre N, & others | Alzheimer's Research \\& Therapy | (2020) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Cognitive decline Prevention trials\n\n\nThe Effects of Rasagiline on Glucose Metabolism and Cognition and Their Relationship to Tau Burden in a Double-Blind, Placebo-Controlled Phase Ii Clinical Trial of Participants with Alzheimer's Dementia\n Matthews, Dawn, Ritter, Aaron, **Thomas, Ronald G**, Andrews, Randolph D, Lukic, Ana S, Revta, Carolyn, Tousi, Babak, Leverenz, James B, Fillit, Howard, Zhong, Kate, & others | Placebo-Controlled Phase Ii Clinical Trial of Participants with Alzheimer's Dementia (2/21/2020) | (2020) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Clinical trials"
  },
  {
    "objectID": "research/index.html#section-5",
    "href": "research/index.html#section-5",
    "title": "Research",
    "section": "2019",
    "text": "2019\n\nA randomized clinical trial to evaluate home-based assessment of people over 75 years old\n Sano, Mary, Zhu, Carolyn W, Kaye, Jeffrey, Mundt, James C, Hayes, Tamara L, Ferris, Steven, **Thomas, Ronald G**, Sun, Chung-Kai, Jiang, Yanxin, Donohue, Michael C, & others | Alzheimer's \\& Dementia | (2019) \nSummary: Evaluating therapeutic interventions through rigorous clinical trials\nTopics: Biostatistics R Clinical trials\nLinks: 📄 Article\n\n\nF4-04-01: TRIAL DESIGN, DATA SHARING RISK MITIGATION, AND BASELINE CLINICAL AND COGNITIVE DATA FROM THE API AUTOSOMAL DOMINANT ALZHEIMER'S DISEASE COLOMBIA TRIAL\n Tariot, Pierre N, Lopera, Francisco, Sink, Kaycee, Hu, Nan, Guthrie, Heather, Smith, Jillian, Cho, William, Langbaum, Jessica B, **Thomas, Ronald G**, Giraldo, Margarita, & others | Alzheimer's \\& Dementia | (2019) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Cognitive decline\nLinks: 📄 Article • 📋 Open Access PDF\n\n\nF4-04-02: AGE-RELATED CHANGES IN BASELINE COGNITIVE MEASURES IN UNIMPAIRED PSEN1 E280A MUTATION CARRIERS AND NON-CARRIERS IN THE API AUTOSOMAL DOMINANT ALZHEIMER'S DISEASE COLOMBIA TRIAL\n Acosta-Baena, Natalia, Rios-Romenets, Silvia, Munoz, Claudia, Bocanegra, Yamile, Henao, Eliana, Giraldo, Margarita, Tobon, Carlos, Sink, Kaycee, Hu, Nan, Guthrie, Heather, & others | Alzheimer's \\& Dementia | (2019) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Cognitive decline\nLinks: 📄 Article • 📋 Open Access PDF\n\n\nF4-04-03: RELATIONSHIPS BETWEEN BASELINE BRAIN IMAGING BIOMARKER MEASUREMENTS AND AGE IN THE API AUTOSOMAL DOMINANT ALZHEIMER'S DISEASE COLOMBIA TRIAL\n Su, Yi, Rios-Romenets, Silvia, Tariot, Pierre N, Sink, Kaycee, Clayton, David, Hu, Nan, Guthrie, Heather, Smith, Jillian, Cho, William, Langbaum, Jessica B, & others | Alzheimer's \\& Dementia | (2019) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Neuroimaging Biomarkers\n\n\nF4-04-04: ASSOCIATION BETWEEN CEREBRAL AMYLOIDOSIS AND WORSE COGNITIVE PERFORMANCE IN PRECLINICAL AUTOSOMAL DOMINANT ALZHEIMER'S DISEASE: BASELINE FINDINGS FROM THE API COLOMBIA AUTOSOMAL DOMINANT AD TRIAL\n Quiroz, Yakeel T, Tariot, Pierre N, Sink, Kaycee, Clayton, David, Langbaum, Jessica B, **Thomas, Ronald G**, Giraldo, Margarita, Tobon, Carlos, Acosta-Baena, Natalia, Luna, Ernesto, & others | Alzheimer's \\& Dementia | (2019) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Cognitive decline\n\n\nLow-dose ladostigil for mild cognitive impairment: A phase 2 placebo-controlled clinical trial\n Schneider, Lon S, Geffen, Yona, Rabinowitz, Jonathan, **Thomas, Ronald G**, Schmidt, Reinhold, Ropele, Stefan, Weinstock, Marta, Ladostigil Study Group, & others | Neurology | (2019) \nSummary: Evaluating therapeutic interventions through rigorous clinical trials\nTopics: Biostatistics R Clinical trials Cognitive decline\nLinks: 📄 Article\n\n\nMemantine and acetylcholinesterase inhibitor use in Alzheimer’s disease clinical trials: Potential for confounding by indication\n Huisa, Branko N, **Thomas, Ronald G**, Jin, Shelia, Oltersdorf, Tilman, Taylor, Curtis, & Feldman, Howard H | Journal of Alzheimer's Disease | (2019) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Clinical trials\n\n\nMultimodal Hippocampal Subfield Grading For Alzheimer’s Disease Classification\n Kilian, Hett, Vinh-Thong, Ta, Gwenaelle, Catheline, Tourdias, Thomas, Manjon, Jose V, Pierrick, Coupe, Weiner, Michael W, Aisen, Paul, Petersen, Ronald, Jack Jr, Clifford R, & others | Scientific Reports (Nature Publisher Group) | (2019) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease\n\n\nNew Perspective for Non-invasive Brain Stimulation Site Selection in Mild Cognitive Impairment: Based on Meta-and Functional Connectivity Analyses\n Liu, Jiao, Zhang, Binlong, Wilson, Georgia, Kong, Jian, Weiner, Michael W, Aisen, Paul, Weiner, Michael, Petersen, Ronald, Jack Jr, Clifford R, Jagust, William, & others | Frontiers in aging neuroscience | (2019) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R Cognitive decline\n\n\nPrediction and classification of Alzheimer’s disease based on combined features from apolipoprotein-E genotype, cerebrospinal fluid, MR, and FDG-PET imaging biomarkers\n Gupta, Yubraj, Lama, Ramesh Kumar, Kwon, Goo-Rak, Weiner, Michael W, Aisen, Paul, Weiner, Michael, Petersen, Ronald, Jack Jr, Clifford R, Jagust, William, Trojanowki, John Q, & others | Frontiers in computational neuroscience | (2019) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Neuroimaging Biomarkers\nLinks: 📄 Article • 📋 Open Access PDF\n\n\nQuantitative 18F-AV1451 brain tau PET imaging in cognitively normal older adults, mild cognitive impairment, and Alzheimer's disease patients\n Zhao, Qian, Liu, Min, Ha, Lingxia, Zhou, Yun, Weiner, Michael W, Aisen, Paul, Weiner, Michael, Petersen, Ronald, Jack Jr, Clifford R, Jagust, William, & others | Frontiers in neurology | (2019) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Neuroimaging Cognitive decline\n\n\nSafety and efficacy of edonerpic maleate for patients with mild to moderate Alzheimer disease: a phase 2 randomized clinical trial\n Schneider, Lon S, **Thomas, Ronald G**, Hendrix, Suzanne, Rissman, Robert A, Brewer, James B, Salmon, David P, Oltersdorf, Tilman, Okuda, Tomohiro, Feldman, Howard H, & others | JAMA neurology | (2019) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Clinical trials\n\n\nThe Alzheimer Prevention Initiative Generation Program: Evaluation of CNP520 in Preclinical Alzheimer’s Disease (P4. 1-005)\n Borowsky, Beth, Lopez, Cristina Lopez, Tariot, Pierre, Caputo, Angelika, Liu, Fonda, Riviere, Marie-Emmanuelle, Rouzade-Dominguez, Marie-Laure, **Thomas, Ronald**, Langbaum, Jessica, Viglietta, Vissia, & others | (2019) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Prevention trials\nLinks: 📄 Article\n\n\nThe Alzheimer's Prevention Initiative Generation Program: study design of two randomized controlled trials for individuals at risk for clinical onset of Alzheimer's disease\n Lopez, Cristina Lopez, Tariot, Pierre N, Caputo, Angelika, Langbaum, Jessica B, Liu, Fonda, Riviere, Marie-Emmanuelle, Langlois, Carolyn, Rouzade-Dominguez, Marie-Laure, Zalesak, Martin, Hendrix, Suzanne, & others | Alzheimer's \\& Dementia: Translational Research \\& Clinical Interventions | (2019) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Clinical trials Prevention trials\nLinks: 📄 Article"
  },
  {
    "objectID": "research/index.html#section-6",
    "href": "research/index.html#section-6",
    "title": "Research",
    "section": "2018",
    "text": "2018\n\n18F-florbetapir Positron Emission Tomography--determined Cerebral beta-Amyloid Deposition and Neurocognitive Performance after Cardiac Surgery\n Klinger, Rebecca Y, James, Olga G, Borges-Neto, Salvador, Bisanar, Tiffany, Li, Yi-Ju, Qi, Wenjing, Berger, Miles, Terrando, Niccolo, Newman, Mark F, Doraiswamy, P Murali, & others | Anesthesiology | (2018) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R Cognitive decline\n\n\nAdeno-associated viral vector (serotype 2)--nerve growth factor for patients with alzheimer disease: a randomized clinical trial\n Rafii, Michael S, Tuszynski, Mark H, **Thomas, Ronald G**, Barba, David, Brewer, James B, Rissman, Robert A, Siffert, Joao, Aisen, Paul S, AAV2-NGF Study Team, & others | JAMA neurology | (2018) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Clinical trials\nLinks: 📄 Article\n\n\nMemantine and Cholinesterase Inhibitor Use in Alzheimer Disease Trials: Potential for Confounding by Indication (P6. 178)\n Huisa, Branko, **Thomas, Ronald**, Jin, Shelia, Oltersdorf, Tilman, Taylor, Curtis, & Feldman, Howard | (2018) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease\nLinks: 📄 Article\n\n\nP3-032: SCREENING-TO-BASELINE COGNITIVE VARIABILITY DOES NOT PREDICT RATE OF DECLINE IN A CLINICAL TRIAL OF MILD-TO-MODERATE AD\n Jacobs, Diane M, **Thomas, Ronald G**, Salmon, David P, Huisa, Branko N, Feldman, Howard H, & Schneider, Lon S | Alzheimer's \\& Dementia | (2018) \nSummary: Evaluating therapeutic interventions through rigorous clinical trials\nTopics: Biostatistics R Clinical trials Cognitive decline\n\n\nP4-209: A PUBLIC RESOURCE OF BASELINE DATA FROM THE API AUTOSOMAL DOMINANT ALZHEIMER'S DISEASE COLOMBIA TRIAL\n Reiman, Eric M, Sink, Kaycee M, Hu, Nan, Guthrie, Heather, Smith, Jillian, Cho, William, Knoll, Katie L, Langbaum, Jessica B, **Thomas, Ronald G**, Toga, Arthur W, & others | Alzheimer's \\& Dementia | (2018) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease\nLinks: 📄 Article • 📋 Open Access PDF\n\n\nSubjective cognitive decline is associated with altered default mode network connectivity in individuals with a family history of Alzheimer’s disease\n Verfaillie, Sander CJ, Binette, Alexa Pichet, Vachon-Presseau, Etienne, Tabrizi, Shirin, Savard, Melissa, Bellec, Pierre, Ossenkoppele, Rik, Scheltens, Philip, van der Flier, Wiesje M, Breitner, John CS, & others | Biological Psychiatry: Cognitive Neuroscience and Neuroimaging | (2018) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Cognitive decline\n\n\nThe Alzheimer's Prevention Initiative Autosomal-Dominant Alzheimer's Disease Trial: A study of crenezumab versus placebo in preclinical PSEN1 E280A mutation carriers to evaluate efficacy and safety in the treatment of autosomal-dominant Alzheimer's disease, including a placebo-treated noncarrier cohort\n Tariot, Pierre N, Lopera, Francisco, Langbaum, Jessica B, **Thomas, Ronald G**, Hendrix, Suzanne, Schneider, Lon S, Rios-Romenets, Silvia, Giraldo, Margarita, Acosta, Natalia, Tobon, Carlos, & others | Alzheimer's \\& Dementia: Translational Research \\& Clinical Interventions | (2018) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Drug development Prevention trials\nLinks: 📄 Article"
  },
  {
    "objectID": "research/index.html#section-7",
    "href": "research/index.html#section-7",
    "title": "Research",
    "section": "2017",
    "text": "2017\n\nA phase 3 trial of IV immunoglobulin for Alzheimer disease\n Relkin, Norman R, **Thomas, Ronald G**, Rissman, Robert A, Brewer, James B, Rafii, Michael S, Van Dyck, Christopher H, Jack, Clifford R, Sano, Mary, Knopman, David S, Raman, Rema, & others | Neurology | (2017) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease\nLinks: 📄 Article • 📋 Open Access PDF\n\n\nAPOE genotype and early beta-amyloid accumulation in older adults without dementia\n Lim, Yen Ying, Mormino, Elizabeth C, Alzheimer's Disease Neuroimaging Initiative, & others | Neurology | (2017) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R\nLinks: 📄 Article\n\n\nAdding recognition discriminability index to the delayed recall is useful to predict conversion from mild cognitive impairment to Alzheimer's disease in the Alzheimer's disease neuroimaging initiative\n Russo, Mar\\'\\i | Frontiers in aging neuroscience | (2017) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Neuroimaging Cognitive decline\n\n\nAlzheimer disease brain atrophy subtypes are associated with cognition and rate of decline\n Risacher, Shannon L, Anderson, Wesley H, Charil, Arnaud, Castelluccio, Peter F, Shcherbinin, Sergey, Saykin, Andrew J, Schwarz, Adam J, Alzheimer's Disease Neuroimaging Initiative, & others | Neurology | (2017) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease\nLinks: 📄 Article • 📋 Open Access PDF\n\n\nConstruction and analysis of weighted brain networks from sice for the study of Alzheimer's disease\n Munilla, Jorge, Ortiz, Andres, Gorriz, Juan M, Ramirez, Javier, Weiner, Michael W, Aisen, Paul, Weiner, Michael, Petersen, Ronald, Jack Jr, Clifford R, Jagust, William, & others | Frontiers in neuroinformatics | (2017) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Statistical methods\nLinks: 📄 Article\n\n\nConversion discriminative analysis on mild cognitive impairment using multiple cortical features from MR images\n Guo, Shengwen, Lai, Chunren, Wu, Congling, Cen, Guiyin, Weiner, Michael W, Aisen, Paul, Weiner, Michael, Petersen, Ronald, Jack Jr, Clifford R, Jagust, William, & others | Frontiers in aging neuroscience | (2017) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R Cognitive decline Statistical methods\nLinks: 📄 Article\n\n\nRandomized controlled trials in mild cognitive impairment: sources of variability\n Petersen, Ronald C, **Thomas, Ronald G**, Aisen, Paul S, Mohs, Richard C, Carrillo, Maria C, Albert, Marilyn S, Alzheimer's Disease Neuroimaging Initiative (ADNI, & others | Neurology | (2017) \nSummary: Evaluating therapeutic interventions through rigorous clinical trials\nTopics: Biostatistics R Clinical trials Cognitive decline\nLinks: 📄 Article\n\n\nRobust identification of Alzheimer’s disease subtypes based on cortical atrophy patterns\n Park, Jong-Yun, Na, Han Kyu, Kim, Sungsoo, Kim, Hyunwook, Kim, Hee Jin, Seo, Sang Won, Na, Duk L, Han, Cheol E, & Seong, Joon-Kyung | Scientific reports | (2017) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease\n\n\nThe Alzheimer's Prevention Initiative (API) Generation Program: Evaluating the Efficacy of the BACE-1 Inhibitor CNP520 in Preclinical Alzheimer's Disease\n Tariot, Pierre, Lopez-Lopez, Cristina, Caputo, Angelika, **Thomas, Ronald G**, Langbaum, Jessica, Lenz, Robert, Vargas, Gabriel, Viglietta, Vissia, Reiman, Eric M, & Graf, Ana | NEUROPSYCHOPHARMACOLOGY | (2017) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Prevention trials\nLinks: 📄 Article\n\n\nThe Alzheimer’s Prevention Initiative Generation Program: evaluating CNP520 efficacy in the prevention of Alzheimer’s disease\n Lopez, C Lopez, Caputo, A, Liu, F, Riviere, ME, Rouzade-Dominguez, ML, Thomas, RG, Langbaum, JB, Lenz, R, Reiman, EM, Graf, A, & others | J Prev Alzheimers Dis | (2017) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Prevention trials\nLinks: 📄 Article\n\n\n[O5--01--02]: RATIONALE FOR SELECTION OF PRIMARY ENDPOINTS IN THE ALZHEIMER PREVENTION INITIATIVE GENERATION STUDY IN COGNITIVELY HEALTHY APOE4 HOMOZYGOTES\n Caputo, Angelika, Racine, Amy, Paule, Ines, Martens, Edwin P, Tariot, Pierre, Langbaum, Jessica B, **Thomas, Ronald G**, Hendrix, Suzanne, Ryan, J Michael, Lopez-Lopez, Cristina, & others | Alzheimer's \\& Dementia | (2017) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Cognitive decline Prevention trials\nLinks: 📄 Article\n\n\n[P4--573]: A PHASE 2 MULTICENTER, RANDOMIZED, PLACEBO-CONTROLLED TRIAL TO EVALUATE THE EFFICACY AND SAFETY OF EDONERPIC (T-817) IN PATIENTS WITH MILD TO MODERATE ALZHEIMER's DISEASE\n Schneider, Lon S, **Thomas, Ronald G**, Hendrix, Suzanne, Brewer, James B, Rissman, Robert A, Salmon, David P, Kobayashi, Hiroshi, & Feldman, Howard H | Alzheimer's \\& Dementia | (2017) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Clinical trials"
  },
  {
    "objectID": "research/index.html#section-8",
    "href": "research/index.html#section-8",
    "title": "Research",
    "section": "2016",
    "text": "2016\n\nA semi-mechanism approach based on MRI and proteomics for prediction of conversion from mild cognitive impairment to Alzheimer’s disease\n Liu, Haochen, Zhou, Xiaoting, Jiang, Hao, He, Hua, & Liu, Xiaoquan | Scientific reports | (2016) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Neuroimaging Cognitive decline\nLinks: 📄 Article • 📋 Open Access PDF\n\n\nDemonstration of safety of intravenous immunoglobulin in geriatric patients in a long-term, placebo-controlled study of Alzheimer's disease\n Gelmont, David, **Thomas, Ronald G**, Britt, Jonathan, Dyck-Jones, Jacqueline A, Doralt, Jennifer, Fritsch, Sandor, Brewer, James B, Rissman, Robert A, & Aisen, Paul | Alzheimer's \\& Dementia: Translational Research \\& Clinical Interventions | (2016) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease\n\n\nGenetic studies of plasma analytes identify novel potential biomarkers for several complex traits\n Deming, Yuetiva, Xia, Jian, Cai, Yefei, Lord, Jenny, Del-Aguila, Jorge L, Fernandez, Maria Victoria, Carrell, David, Black, Kathleen, Budde, John, Ma, ShengMei, & others | Scientific Reports | (2016) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R Biomarkers\nLinks: 📄 Article\n\n\nLongitudinal decline in mild-to-moderate Alzheimer's disease: analyses of placebo data from clinical trials\n **Thomas, Ronald G**, Albert, Marilyn, Petersen, Ronald C, & Aisen, Paul S | Alzheimer's \\& Dementia | (2016) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Clinical trials Longitudinal studies\nLinks: 📄 Article\n\n\nThe Alzheimer's Prevention Initiative Generation Study: A Preclinical Trial in APOE4 Homozygotes\n Tariot, Pierre, Langbaum, Jessica, Schneider, Lon, **Thomas, Ronald G**, Graf, Ana, Lopez-Lopez, Cristina, Caputo, Angelika, Lenz, Robert, Vargas, Gabriel, & Reiman, Eric M | NEUROPSYCHOPHARMACOLOGY | (2016) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Clinical trials Prevention trials\nLinks: 📄 Article"
  },
  {
    "objectID": "research/index.html#section-9",
    "href": "research/index.html#section-9",
    "title": "Research",
    "section": "2015",
    "text": "2015\n\nA randomized, double-blind, placebo-controlled trial of resveratrol for Alzheimer disease\n Turner, R Scott, **Thomas, Ronald G**, Craft, Suzanne, Van Dyck, Christopher H, Mintzer, Jacobo, Reynolds, Brigid A, Brewer, James B, Rissman, Robert A, Raman, Rema, Aisen, Paul S, & others | Neurology | (2015) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Clinical trials\nLinks: 📄 Article\n\n\nLarge-scale genomics unveil polygenic architecture of human cortical surface area\n Chen, Chi-Hua, Peng, Qian, Schork, Andrew J, Lo, Min-Tzu, Fan, Chun-Chieh, Wang, Yunpeng, Desikan, Rahul S, Bettella, Francesco, Hagler, Donald J, Westlye, Lars T, & others | Nature communications | (2015) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R\nLinks: 📄 Article • 📋 Open Access PDF\n\n\nLongitudinal plasma amyloid beta in Alzheimer's disease clinical trials\n Donohue, Michael C, Moghadam, Setareh H, Roe, Allyson D, Sun, Chung-Kai, Edland, Steven D, **Thomas, Ronald G**, Petersen, Ronald C, Sano, Mary, Galasko, Douglas, Aisen, Paul S, & others | Alzheimer's \\& Dementia | (2015) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Clinical trials Longitudinal studies\n\n\nPeripheral and central effects of $\\\\gamma$-secretase inhibition by semagacestat in Alzheimer’s disease\n Doody, Rachelle S, Raman, Rema, Sperling, Reisa A, Seimers, Eric, Sethuraman, Gopalan, Mohs, Richard, Farlow, Martin, Iwatsubo, Takeshi, Vellas, Bruno, Sun, Xiaoying, & others | Alzheimer's research \\& therapy | (2015) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease\n\n\nResveratrol is safe and well-tolerated in individuals with mild-moderate dementia due to Alzheimer’s disease.(S33. 009)\n Turner, R, **Thomas, Ronald**, Craft, Suzanne, van Dyck, Christopher, Mintzer, Jacobo, Reynolds, Brigid, Brewer, James, Rissman, Robert, Raman, Rema, & Aisen, Paul | (2015) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease\n\n\nSafety of Intravenous Immunoglobulin Therapy in Patients with Probable Alzheimer's Disease: A Randomized, Placebo-Controlled Clinical Study\n Gelmont, D, Thomas, RG, Dyck-Jones, JA, Fritsch, S, Aisen, P, & Relkin, N | ANNALS OF ALLERGY ASTHMA \\& IMMUNOLOGY | (2015) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Clinical trials Drug development\nLinks: 📄 Article\n\n\nTest-retest resting-state fMRI in healthy elderly persons with a family history of Alzheimer’s disease\n Orban, Pierre & Madjar, C\\'e | Scientific data | (2015) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Neuroimaging\nLinks: 📄 Article • 📋 Open Access PDF\n\n\nUsing novel methodologies to examine the impact of artificial light at night on the cortisol stress response in dispersing Atlantic salmon (Salmo salar L.) fry\n Newman, Rhian C, Ellis, Tim, Davison, Phil I, Ives, Mark J, Thomas, Rob J, Griffiths, Sian W, & Riley, William D | Conservation physiology | (2015) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R Statistical methods\nLinks: 📄 Article • 📋 Open Access PDF"
  },
  {
    "objectID": "research/index.html#section-10",
    "href": "research/index.html#section-10",
    "title": "Research",
    "section": "2014",
    "text": "2014\n\nAlzheimer’s Disease Cooperative Study Steering Committee; Solanezumab study group. Phase 3 trials of solanezumab for mild-to-moderate Alzheimer’s disease\n Doody, RS, Thomas, RG, Farlow, M, Iwatsubo, T, Vellas, B, Joffe, S, Kieburtz, K, Raman, R, Sun, X, Aisen, PS, & others | N Engl J Med | (2014) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease\n\n\nAustralian imaging, biomarkers, and lifestyle flagship study of ageing; Alzheimer’s disease neuroimaging initiative; Alzheimer’s disease cooperative study. The preclinical Alzheimer cognitive composite: measuring amyloid-related decline\n Donohue, MC, Sperling, RA, Salmon, DP, Rentz, DM, Raman, R, Thomas, RG, Weiner, M, & Aisen, PS | JAMA Neurol | (2014) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Neuroimaging Biomarkers Cognitive decline\n\n\nBayesian Longitudinal Modeling on Placebo Data from Alzheimer’s Disease Clinical Studies (P1. 010)\n Chen, Yun-Fei, Mohs, Richard, Ding, Ying, Aisen, Paul, & **Thomas, Ronald** | (2014) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Longitudinal studies\nLinks: 📄 Article • 📋 Open Access PDF\n\n\nClinical trial of an inhibitor of RAGE-A beta interactions in Alzheimer disease\n Galasko, Douglas, Bell, Joanne, Mancuso, Jessica Y, Kupiec, James W, Sabbagh, Marwan N, van Dyck, Christopher, **Thomas, Ronald G**, Aisen, Paul S, & others | Neurology | (2014) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Clinical trials\nLinks: 📄 Article\n\n\nEstimating long-term multivariate progression from short-term data\n Donohue, Michael C, Jacqmin-Gadda, Hene, Le Goff, Melanie, **Thomas, Ronald G**, Raman, Rema, Gamst, Anthony C, Beckett, Laurel A, Jack Jr, Clifford R, Weiner, Michael W, Dartigues, Jean-Francois, & others | Alzheimer's \\& Dementia | (2014) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R\n\n\nF3-02-02: MODELING LONG-TERM DISEASE PROGRESSION WITH COVARIATES\n Donohue, Michael C, Gamst, Anthony, Jack, Clifford, Beckett, Laurel, Weiner, Michael, Aisen, Paul, Raman, Rema, & **Thomas, Ronald** | Alzheimer's \\& Dementia | (2014) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R\n\n\nP1-357: ADCS EDC: INVESTIGATIONAL PRODUCT MANAGEMENT SYSTEM\n Jimenez-Maggiora, Gustavo Adolfo, **Thomas, Ronald G**, Qiu, Hongmei, Hong, Phuoc, & Aisen, Paul S | Alzheimer's \\& Dementia | (2014) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R\nLinks: 📄 Article\n\n\nPhase 3 trials of solanezumab and bapineuzumab for Alzheimer’s disease\n Laske, Christoph | N Engl J Med | (2014) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease\n\n\nPhase 3 trials of solanezumab for mild-to-moderate Alzheimer's disease\n Doody, Rachelle S, **Thomas, Ronald G**, Farlow, Martin, Iwatsubo, Takeshi, Vellas, Bruno, Joffe, Steven, Kieburtz, Karl, Raman, Rema, Sun, Xiaoying, Aisen, Paul S, & others | New England Journal of Medicine | (2014) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease\nLinks: 📄 Article\n\n\nThe preclinical Alzheimer cognitive composite: measuring amyloid-related decline\n Donohue, Michael C, Sperling, Reisa A, Salmon, David P, Rentz, Dorene M, Raman, Rema, **Thomas, Ronald G**, Weiner, Michael, & Aisen, Paul S | JAMA neurology | (2014) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Cognitive decline\nLinks: 📄 Article"
  },
  {
    "objectID": "research/index.html#section-11",
    "href": "research/index.html#section-11",
    "title": "Research",
    "section": "2013",
    "text": "2013\n\nA phase 3 trial of semagacestat for treatment of Alzheimer's disease\n Doody, Rachelle S, Raman, Rema, Farlow, Martin, Iwatsubo, Takeshi, Vellas, Bruno, Joffe, Steven, Kieburtz, Karl, He, Feng, Sun, Xiaoying, **Thomas, Ronald G**, & others | New England Journal of Medicine | (2013) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Drug development\nLinks: 📄 Article\n\n\nAge and apolipoprotein E genotype influence rate of cognitive decline in nondemented elderly.\n Salmon, David P, Ferris, Steven H, **Thomas, Ronald G**, Sano, Mary, Cummings, Jeffery L, Sperling, Reisa A, Petersen, Ronald C, & Aisen, Paul S | Neuropsychology | (2013) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R Cognitive decline\n\n\nAlzheimer’s Disease Cooperative Study Steering Committee, Siemers E, Sethuraman G, Mohs R, Semagacestat Study Group. A phase 3 trial of semagacestat for treatment of Alzheimer’s disease\n Doody, RS, Raman, R, Farlow, M, Iwatsubo, T, Vellas, B, Joffe, S, Kieburtz, K, He, F, Sun, X, Thomas, RG, & others | N Engl J Med | (2013) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Drug development\nLinks: 📄 Article • 📋 Open Access PDF\n\n\nEstablishing the psychometric underpinning of cognition measures for clinical trials of Alzheimer's disease and its precursors: a new approach\n Posner, Holly B, Cano, Stefan, Carrillo, Maria C, Selnes, Ola, Stern, Yaakov, **Thomas, Ronald G**, Zajicek, John, Hobart, Jeremy, Alzheimer's Disease Neuroimaging Initiative, & others | Alzheimer's \\& Dementia | (2013) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Clinical trials\nLinks: 📄 Article\n\n\nP1--332: Evaluation of the relationship between TTP488 plasma concentrations and changes in ADAS-cog relative to placebo\n Burstein, Aaron, Galasko, Douglas, Aisen, Paul, **Thomas, Ronald**, Grimes, Imogene, Clark, David J, Mjalli, Adnan, & Orlande, Cesare | Alzheimer's \\& Dementia | (2013) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R\nLinks: 📄 Article\n\n\nP3--295: The Placebo Data Analysis in Alzheimer's Disease (AD) and Mild Cognitive Impairment (MCI) Clinical Trials Project: Overview of progress in trial data collection, and key findings from the pooled Alzheimer's disease trial datasets\n **Thomas, Ronald**, Petersen, Ronald, Siuciak, Judith, Carrillo, Maria, Albert, Marilyn, Aisen, Paul, Alzheimer's Disease Neuroimaging Initiative, & Foundation for NIH Biomarkers Consortium AD MCI Placebo Data Project Team | Alzheimer's \\& Dementia | (2013) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Clinical trials Cognitive decline Statistical methods\nLinks: 📄 Article\n\n\nP4--157: Adcs electronic data capture: Collaborative development and management of clinical trial databases\n Jimenez-Maggiora, Gustavo, **Thomas, Ronald**, Bruschi, Stefania, Qiu, Hongmei, Hong, Phuoc, & Aisen, Paul | Alzheimer's \\& Dementia | (2013) \nSummary: Evaluating therapeutic interventions through rigorous clinical trials\nTopics: Biostatistics R Clinical trials\n\n\nPreclinical trials in autosomal dominant AD: implementation of the DIAN-TU trial\n Mills, Sarah M, Mallmann, J, Santacruz, Anna M, Fuqua, A, Carril, M, Aisen, Paul S, Althage, MC, Belyew, S, Benzinger, Tammie L, Brooks, William S, & others | Revue neurologique | (2013) \nSummary: Evaluating therapeutic interventions through rigorous clinical trials\nTopics: Biostatistics R Clinical trials\nLinks: 📄 Article\n\n\nPutting the Alzheimer's cognitive test to the test II: Rasch Measurement Theory\n Hobart, Jeremy, Cano, Stefan, Posner, Holly, Selnes, Ola, Stern, Yaakov, **Thomas, Ronald**, Zajicek, John, & Alzheimer's Disease Neuroimaging Initiative | Alzheimer's \\& dementia | (2013) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Cognitive decline\nLinks: 📄 Article"
  },
  {
    "objectID": "research/index.html#section-12",
    "href": "research/index.html#section-12",
    "title": "Research",
    "section": "2012",
    "text": "2012\n\nA Garden before the Garden: Landscape, History and the National Botanic Garden of Wales\n Austin, David & Thomas, Rob | Landscapes | (2012) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R\n\n\nAlzheimer’s Disease Cooperative Study. Antioxidants for Alzheimer disease: a randomized clinical trial with cerebrospinal fluid biomarker measures\n Galasko, DR, Peskind, E, Clark, CM, Quinn, JF, Ringman, JM, Jicha, GA, Cotman, C, Cottrell, B, Montine, TJ, Thomas, RG, & others | Arch Neurol | (2012) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Clinical trials Biomarkers\n\n\nAlzheimer’s disease therapeutic trials: EU/US Task Force report on recruitment, retention, and methodology\n Vellas, B, Hampel, H, & Roug\\'e | The journal of nutrition, health \\& aging | (2012) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Statistical methods\nLinks: 📄 Article\n\n\nAntioxidants for Alzheimer disease: a randomized clinical trial with cerebrospinal fluid biomarker measures\n Galasko, Douglas R, Peskind, Elaine, Clark, Christopher M, Quinn, Joseph F, Ringman, John M, Jicha, Gregory A, Cotman, Carl, Cottrell, Barbara, Montine, Thomas J, **Thomas, Ronald G**, & others | Archives of neurology | (2012) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Clinical trials Biomarkers\nLinks: 📄 Article\n\n\nIncidence of new-onset seizures in mild to moderate Alzheimer disease\n Irizarry, Michael C, Jin, Shelia, He, Feng, Emond, Jennifer A, Raman, Rema, **Thomas, Ronald G**, Sano, Mary, Quinn, Joseph F, Tariot, Pierre N, Galasko, Douglas R, & others | Archives of neurology | (2012) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease\n\n\nO3-13-05: The Gammaglobulin Alzheimer Partnership Study (GAP): Design, screening, enrollment and futility analysis results\n Relkin, Norman, Gessert, Devon, Stokes, Karen, Adamiak, Basia, Ngo, Leock Y, **Thomas, Ronald**, Gelmont, David, & Aisen, Paul | Alzheimer's \\& Dementia | (2012) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Statistical methods\nLinks: 📄 Article • 📋 Open Access PDF\n\n\nP3-364: ADCS EDC\n Jimenez-Maggiora, Gustavo, **Thomas, Ronald**, Hong, Phuoc, & Aisen, Paul | Alzheimer's \\& Dementia | (2012) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R\nLinks: 📄 Article • 📋 Open Access PDF\n\n\nP3-383: ADCS data sharing\n **Thomas, Ronald**, Jimenez, Gustavo, Brewer, James, Rissman, Robert A, & Aisen, Paul | Alzheimer's \\& Dementia | (2012) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R\nLinks: 📄 Article\n\n\nP3-384: The placebo data analysis in Alzheimer's disease and mild cognitive impairment (MCI) clinical trials project: Overview of progress in trial data collection, and key findings from the pooled MCI trial datasets\n Aisen, Paul, **Thomas, Ronald**, Carrillo, Maria, Mohs, Richard, Petersen, Ronald, Siuciak, Judith, Albert, Marilyn, Alzheimer's Disease Neuroimaging Initiative, & Foundation for NIH Biomarkers Consortium CSF Proteomics Project Team | Alzheimer's \\& Dementia | (2012) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Clinical trials Cognitive decline Statistical methods"
  },
  {
    "objectID": "research/index.html#section-13",
    "href": "research/index.html#section-13",
    "title": "Research",
    "section": "2011",
    "text": "2011\n\nA phase II trial of huperzine A in mild to moderate Alzheimer disease\n Rafii, MS, Walsh, S, Little, JT, Behan, K, Reynolds, B, Ward, C, Jin, S, Thomas, R, Aisen, PS, & others | Neurology | (2011) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease\nLinks: 📄 Article\n\n\nA randomized, double-blind, placebo-controlled trial of simvastatin to treat Alzheimer disease\n Sano, M, Bell, KL, Galasko, D, Galvin, JE, Thomas, RG, Van Dyck, CH, & Aisen, PS | Neurology | (2011) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Clinical trials\nLinks: 📄 Article • 📋 Open Access PDF\n\n\nAdding delayed recall to the Alzheimer Disease Assessment Scale is useful in studies of mild cognitive impairment but not Alzheimer disease\n Sano, Mary, Raman, Rema, Emond, Jennifer, **Thomas, Ronald G**, Petersen, Ronald, Schneider, Lon S, & Aisen, Paul S | Alzheimer disease and associated disorders | (2011) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Cognitive decline\nLinks: 📄 Article\n\n\nChronic divalproex sodium to attenuate agitation and clinical progression of Alzheimer disease\n Tariot, Pierre N, Schneider, Lon S, Cummings, Jeffrey, **Thomas, Ronald G**, Raman, Rema, Jakimovich, Laura J, Loy, Rebekah, Bartocci, Barbara, Fleisher, Adam, Ismail, M Saleem, & others | Archives of General Psychiatry | (2011) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease\n\n\nChronic divalproex sodium use and brain atrophy in Alzheimer disease\n Fleisher, AS, Truran, D, Mai, JT, Langbaum, JBS, Aisen, PS, Cummings, JL, Jack, CR, Weiner, MW, Thomas, RG, Schneider, LS, & others | Neurology | (2011) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease\n\n\nDietary self-monitoring and its impact on weight loss in overweight children\n Mockus, Danyte S, Macera, Caroline A, Wingard, Deborah L, Peddecord, Michael, **Thomas, Ronald G**, & Wilfley, Denise E | International Journal of Pediatric Obesity | (2011) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R\n\n\nOmega-3 Fatty Acids for Alzheimer's Disease. What a Pill Can Tell Us about Eating Fish\n Quinn, JF, Raman, R, Thomas, RG, Yurko-Mauro, K, Nelson, EB, Van Dyck, C, Galvin, JE, Emond, J, Jack Jr, CR, Weiner, M, & others | (2011) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease\nLinks: 📄 Article\n\n\nP3-406: Role of caregiver in subject's compliance with treatment\n Messick, Viviana, Donohue, Michael, Raman, Rema, Sano, Mary, Quinn, Joseph, Thomas, Ron, Emond, Jennifer, & Aisen, Paul | Alzheimer's \\& Dementia | (2011) \nSummary: Evaluating therapeutic interventions through rigorous clinical trials\nTopics: Biostatistics R Drug development\nLinks: 📄 Article\n\n\nRadiological follow up of perineal repair with cross-linked acellular porcine dermal collagen following extralevator abdominoperineal excision for low rectal cancer: P115\n Smart, N, George, A, Khan, D, Thomas, R, & Daniels, I | Colorectal Disease | (2011) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R Longitudinal studies\nLinks: 📄 Article\n\n\nReport of the task force on designing clinical trials in early (predementia) AD\n Aisen, PS, Andrieu, S, Sampaio, C, Carrillo, M, Khachaturian, ZS, Dubois, Bruno, Feldman, HH, Petersen, RC, Siemers, E, Doody, RS, & others | Neurology | (2011) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Clinical trials\nLinks: 📄 Article\n\n\nThe relative efficiency of time-to-threshold and rate of change in longitudinal data\n Donohue, Michael C, Gamst, AC, Thomas, RG, Xu, R, Beckett, L, Petersen, Ronald Carl, Weiner, MW, Aisen, P, Alzheimer's Disease Neuroimaging Initiative, & others | Contemporary clinical trials | (2011) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R Longitudinal studies\nLinks: 📄 Article • 📋 Open Access PDF"
  },
  {
    "objectID": "research/index.html#section-14",
    "href": "research/index.html#section-14",
    "title": "Research",
    "section": "2010",
    "text": "2010\n\nClinical Core of the Alzheimer's Disease Neuroimaging Initiative: progress and plans\n Aisen, Paul S, Petersen, Ronald C, Donohue, Michael C, Gamst, Anthony, Raman, Rema, **Thomas, Ronald G**, Walter, Sarah, Trojanowski, John Q, Shaw, Leslie M, Beckett, Laurel A, & others | Alzheimer's \\& Dementia | (2010) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Neuroimaging\n\n\nDocosahexaenoic acid supplementation and cognitive decline in Alzheimer disease: a randomized trial\n Quinn, Joseph F, Raman, Rema, **Thomas, Ronald G**, Yurko-Mauro, Karin, Nelson, Edward B, Van Dyck, Christopher, Galvin, James E, Emond, Jennifer, Jack, Clifford R, Weiner, Michael, & others | Jama | (2010) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Clinical trials Cognitive decline\nLinks: 📄 Article\n\n\nO3-01-07: Rate of decline in ADNI normal controls with evidence of amyloid burden\n Donohue, Michael, Gamst, Anthony, Thomas, Ron, Brewer, Jim, Weiner, Michael, & Aisen, Paul | Alzheimer's \\& Dementia | (2010) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R\nLinks: 📄 Article\n\n\nP1-433: Brain volume changes with divalproex sodium in Alzheimer's disease\n Fleisher, Adam S, Jack Jr, Clifford R, Weiner, Michael W, Truran, Diana, Mai, Jacqueline, Aisen, Paul S, Cummings, Jeffrey L, **Thomas, Ronald G**, Schneider, Lon S, & Tariot, Pierre N | Alzheimer's \\& Dementia | (2010) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease\nLinks: 📄 Article\n\n\nP1-447: Cerebrospinal fluid biomarker outcomes in a trial of docosahexaenoic acid (DHA) for Alzheimer's disease\n Quinn, Joseph F, **Thomas, Ronald**, Raman, Rema, Yurko-Mauro, Karin, Bailey-Hall, Eileen, Nelson, Edward, Shaw, Les, & Aisen, Paul | Alzheimer's \\& Dementia | (2010) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Biomarkers"
  },
  {
    "objectID": "research/index.html#section-15",
    "href": "research/index.html#section-15",
    "title": "Research",
    "section": "2009",
    "text": "2009\n\nMRI substudy participation in Alzheimer disease (AD) clinical trials: baseline comparability of a substudy sample to entire study population\n Raman, Rema, **Thomas, Ronald G**, Weiner, Michael W, Jack, Clifford R, Ernstrom, Karin, Aisen, Paul S, Tariot, Pierre N, & Quinn, Joseph F | Alzheimer disease and associated disorders | (2009) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Clinical trials Neuroimaging Epidemiology\nLinks: 📄 Article\n\n\nO1-04-02: A clinical trial of docosahexanoic acid (DHA) for the treatment of Alzheimer's disease\n Quinn, Joseph F, Raman, Rema, **Thomas, Ronald G**, Ernstrom, Karin, Yurko-Mauro, Karin, Nelson, Edward B, Shinto, Lynne, Nair, Anil K, & Aisen, Paul | Alzheimer's \\& Dementia | (2009) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Clinical trials Drug development\nLinks: 📄 Article\n\n\nO1-04-03: The ADCS valproate neuroprotection trial: Primary efficacy and safety results\n Tariot, Pierre N, Aisen, Paul, Cummings, Jeffrey, Jakimovich, Laura, Schneider, Lon, **Thomas, Ronald**, Becerra, Lida, & Loy, Rebekah | Alzheimer's \\& Dementia | (2009) \nSummary: Evaluating therapeutic interventions through rigorous clinical trials\nTopics: Biostatistics R\nLinks: 📄 Article\n\n\nO4-04-07: The ADAS-cog's performance as a measure—lessons from the ADNI study: Part 2-evaluation using modern psychometric methods\n Cano, Stefan, Posner, Holly, Aisen, Paul, Selnes, Ola, Stern, Yaakov, **Thomas, Ronald**, Weiner, Michael, Zajicek, John, Zeger, Scott, & Hobart, Jeremy | Alzheimer's \\& Dementia | (2009) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R Statistical methods\n\n\nP1-269: The ADAS-cog's performance as a measure-lessons from the ADNI study: Part 1-evaluation using traditional psychometric methods\n Posner, Holly, Cano, Stefan, Aisen, Paul, Selnes, Ola, Stern, Yaakov, **Thomas, Ronald**, Weiner, Michael, Zajicek, John, Zeger, Scott, & Hobart, Jeremy | Alzheimer's \\& Dementia | (2009) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R Statistical methods\nLinks: 📄 Article\n\n\nP1-270: The ADAS-cog's performance as a measure-lessons from the ADNI study: Part 3-do the scale modifications add value?\n Hobart, Jeremy, Posner, Holly, Aisen, Paul, Selnes, Ola, Stern, Yaakov, **Thomas, Ronald**, Weiner, Michael, Zajicek, John, Zeger, Scott, & Cano, Stefan | Alzheimer's \\& Dementia | (2009) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R\n\n\nPlasma urate and progression of mild cognitive impairment\n Irizarry, Michael C, Raman, Rema, Schwarzschild, Michael A, Becerra, Lida M, **Thomas, Ronald G**, Peterson, Ronald C, Ascherio, Alberto, & Aisen, Paul S | Neurodegenerative Diseases | (2009) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R Cognitive decline\nLinks: 📄 Article\n\n\nREVIEWS IN THE NEUROSCIENCES\n Machado, C, Leisman, G, Koch, P, Rodriguez, R, Caiballo, M, Korein, J, Sanchez-Catasus, C, Perez, J, Djuric, S, Djuric, V, & others | NEUROSCIENCES | (2009) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R\n\n\nThe ADAS-cog's Performance as a Measure Lessons from the ADNI Study: Part 1-Evaluation Using Traditional Psychometric Methods\n Pasner, Holly, Cano, Stefan J, Aisen, Paul, Selnes, Ola, Stern, Yaakov, **Thomas, Ronald**, Weiner, Michael, Zajicek, John, Zeger, Scott, & Hobart, Jeremy | Neurology | (2009) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R Statistical methods\nLinks: 📄 Article"
  },
  {
    "objectID": "research/index.html#section-16",
    "href": "research/index.html#section-16",
    "title": "Research",
    "section": "2008",
    "text": "2008\n\nA roadmap for the prevention of dementia: the inaugural Leon Thal Symposium\n Khachaturian, Zaven S, Petersen, Ronald C, Gauthier, Serge, Buckholtz, Neil, Corey-Bloom, Jodey P, Evans, Bill, Fillit, Howard, Foster, Norman, Greenberg, Barry, Grundman, Michael, & others | Alzheimer's \\& dementia: the journal of the Alzheimer's Association | (2008) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Prevention trials\nLinks: 📄 Article • 📋 Open Access PDF\n\n\nAlzheimer Disease Cooperative Study. High-dose B vitamin supplementation and cognitive decline in Alzheimer disease: a randomized controlled trial\n Aisen, PS, Schneider, LS, Sano, M, Diaz-Arrastia, R, Van Dyck, CH, Weiner, MF, Bottiglieri, T, Jin, S, Stokes, KT, Thomas, RG, & others | Jama | (2008) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Clinical trials Cognitive decline\n\n\nAmetropia, preschoolers' cognitive abilities, and effects of spectacle correction\n Roch-Levecq, Anne-Catherine, Brody, Barbara L, **Thomas, Ronald G**, & Brown, Stuart I | Archives of ophthalmology | (2008) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R Cognitive decline\nLinks: 📄 Article\n\n\nDimebon investigators: Effect of dimebon on cognition, activities of daily living, behaviour, and global function in patients with mild-to-moderate Alzheimer's disease: a randomised, double-blind, placebo-controlled study\n Doody, RS, Gavrilova, SI, Sano, M, Thomas, RG, Aisen, PS, Bachurin, SO, Seely, L, & Hung, D | Lancet | (2008) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease\n\n\nDual Task Test Could Help Diagnose Dementia\n Aisen, Paul S, Schneider, Lon S, Sano, Mary, Diaz-Arrastia, Ramon, van Dyck, Christopher H, Weiner, Myron F, Bottiglieri, Teodoro, Jin, Shelia, Stokes, Karen T, **Thomas, Ronald G**, & others | JAMA | (2008) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R\nLinks: 📄 Article\n\n\nEffect of dimebon on cognition, activities of daily living, behaviour, and global function in patients with mild-to-moderate Alzheimer's disease: a randomised, double-blind, placebo-controlled study\n Doody, Rachelle S, Gavrilova, Svetlana I, Sano, Mary, **Thomas, Ronald G**, Aisen, Paul S, Bachurin, Sergey O, Seely, Lynn, Hung, David, & others | The Lancet | (2008) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease\nLinks: 📄 Article\n\n\nEfficacy of site-independent telemedicine in the STRokE DOC trial: a randomised, blinded, prospective study\n Meyer, Brett C, Raman, Rema, Hemmen, Thomas, Obler, Richard, Zivin, Justin A, Rao, Ramesh, **Thomas, Ronald G**, & Lyden, Patrick D | The Lancet Neurology | (2008) \nSummary: Evaluating therapeutic interventions through rigorous clinical trials\nTopics: Biostatistics R\nLinks: 📄 Article\n\n\nHigh-dose B vitamin supplementation and cognitive decline in Alzheimer disease: a randomized controlled trial\n Aisen, Paul S, Schneider, Lon S, Sano, Mary, Diaz-Arrastia, Ramon, Van Dyck, Christopher H, Weiner, Myron F, Bottiglieri, Teodoro, Jin, Shelia, Stokes, Karen T, **Thomas, Ronald G**, & others | Jama | (2008) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Clinical trials Cognitive decline\n\n\nO2-04--06: Randomized clinical trial of antioxidant treatment in Alzheimer's disease with CSF biomarker measures\n Galasko, Douglas, Peskind, Elaine, Clark, Christopher M, Quinn, Joseph, Ringman, John, Jicha, Gregory A, Cottrell, Barbara, & **Thomas, Ronald G** | Alzheimer's \\& Dementia | (2008) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Clinical trials Biomarkers Drug development\nLinks: 📄 Article\n\n\nP4-337: Dimebon improves cognition, function, and behavior in mild and moderate Alzheimer's disease: Results by severity of a one-year, double-blind, placebo-controlled study\n Doody, Rachelle S, Gavrilova, Svetlana, **Thomas, Ronald**, Aisen, Paul, Bachurin, Sergey, Seely, Lynn, & Hung, David | Alzheimer''s and Dementia | (2008) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease\nLinks: 📄 Article\n\n\nP4-343: Omega 3 fatty acids and Alzheimer's disease: Trial design and baseline study population characteristics in a clinical trial of docosahexanoic acid for Alzheimer's disease\n Quinn, Joseph F, Raman, Rema, **Thomas, Ronald**, Ernstrom, Karin, Yurko-Mauro, Karin, Nelson, Edward, & Aisen, Paul S | Alzheimer's \\& Dementia | (2008) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Clinical trials Epidemiology\n\n\nP4-387: Adding delayed recall to the Alzheimer's disease assessment scale-cognitive subscale (ADAS-cog): Sensitivity in a clinical trial for Alzheimer's disease and mild cognitive impairment\n Raman, Rema, Emond, Jennifer, **Thomas, Ronald G**, Petersen, Ronald C, Schneider, Lon S, Aisen, Paul S, & Sano, Mary | Alzheimer's \\& Dementia | (2008) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Clinical trials Cognitive decline\nLinks: 📄 Article\n\n\nStatistical treatment of withdrawal in trials of anti-dementia drugs--Authors' reply\n Doody, Rachelle, Seely, Lynn, **Thomas, Ronald**, Sano, Mary, & Aisen, Paul | The Lancet | (2008) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Drug development Statistical methods\nLinks: 📄 Article"
  },
  {
    "objectID": "research/index.html#section-17",
    "href": "research/index.html#section-17",
    "title": "Research",
    "section": "2007",
    "text": "2007\n\nDimebon improves cognition, function, and behavior in patients with mild-moderate Alzheimer's disease: Results of a randomized, double-blind, placebo-controlled study\n Doody, Rochelle, Gavrilova, Svetlana, Sano, Mary, **Thomas, Ronald**, Aisen, Paul, Seely, Lynn, & Hung, David | Neurology | (2007) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Clinical trials\nLinks: 📄 Article\n\n\nP-081: The ACDS home assessment instrument: A pilot study\n Sano, Mary, Kaye, Jeffrey, Ferris, Steven, Hayes, Tamara, Egelko, Susan, Mundt, James, Li, Yan, Walter, Sarah, **Thomas, Ronald**, Edland, Steven, & others | Alzheimer's \\& Dementia | (2007) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R\n\n\nPopulation Dynamics of Meloidogyne incognita and RotylenchulusTenchulus reniformis Alone and in Combination, and Their Effects on Sweet Potato1\n THOMAS, RONALD J & CLARK, CHRISTOPHER A | (2007) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R Epidemiology\n\n\nPotential outcome measures and trial design issues for multiple system atrophy\n May, Susanne, Gilman, Sid, Sowell, B Brooke, **Thomas, Ronald G**, Stern, Matthew B, Colcher, Amy, Tanner, Caroline M, Huang, Neng, Novak, Peter, Reich, Stephen G, & others | Movement disorders | (2007) \nSummary: Evaluating therapeutic interventions through rigorous clinical trials\nTopics: Biostatistics R\nLinks: 📄 Article\n\n\nS3--02--01: ADCS homocysteine trial\n Aisen, Paul S, Jin, Shelia, **Thomas, Ronald G**, Sano, Mary, Diaz-Arrastia, Ramon, Thal, Leon, & Alzheimer's Disease Cooperative Study NIA | Alzheimer's \\& Dementia | (2007) \nSummary: Evaluating therapeutic interventions through rigorous clinical trials\nTopics: Biostatistics R\nLinks: 📄 Article\n\n\nS3--02--03: Results of a one-year randomized, placebo-controlled trial of dimebon for the treatment of mild to moderate Alzheimer's disease\n Doody, Rachelle S, Gavrilova, Svetlana, Sano, Mary, **Thomas, Ronald**, Aisen, Paul, Bachurin, Sergey, Seely, Lynn, & Hung, David | Alzheimer's \\& Dementia | (2007) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Clinical trials Drug development"
  },
  {
    "objectID": "research/index.html#section-18",
    "href": "research/index.html#section-18",
    "title": "Research",
    "section": "2006",
    "text": "2006\n\nADCS Prevention Instrument Project: pharmacoeconomics: assessing health-related resource use among healthy elderly\n Sano, Mary, Zhu, Carolyn W, Whitehouse, Peter J, Edland, Steven, Jin, Shelia, Ernstrom, Karin, **Thomas, Ronald G**, Thal, Leon J, & Ferris, Steven H | Alzheimer disease and associated disorders | (2006) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R Prevention trials\nLinks: 📄 Article\n\n\nCognitive Changes in the Treatment of Mild Cognitive Impairment with Donepezil and Vitamin E: P02. 187\n Petersen, Ronald, **Thomas, Ronald**, Grundman, Michael, & Thai, Leon | Neurology | (2006) \nSummary: Evaluating therapeutic interventions through rigorous clinical trials\nTopics: Biostatistics R Cognitive decline Drug development"
  },
  {
    "objectID": "research/index.html#section-19",
    "href": "research/index.html#section-19",
    "title": "Research",
    "section": "2005",
    "text": "2005\n\nAnalysis of apoptosis protein expression in early-stage colorectal cancer suggests opportunities for new prognostic biomarkers\n Krajewska, Maryla, Kim, Hoguen, Kim, Chul, Kang, Haeyoun, Welsh, Kate, Matsuzawa, Shu-ichi, Tsukamoto, Michelle, **Thomas, Ronald G**, Assa-Munt, Nuria, Piao, Zhe, & others | Clinical Cancer Research | (2005) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R Biomarkers Statistical methods\nLinks: 📄 Article\n\n\nCognition in the treatment of mild cognitive impairment with donepezil and vitamin E\n Pfeiffer, E, Petersen, RC, Thomas, RG, & Thal, LJ | INTERNATIONAL PSYCHOGERIATRICS | (2005) \nSummary: Evaluating therapeutic interventions through rigorous clinical trials\nTopics: Biostatistics R Cognitive decline Drug development\nLinks: 📄 Article\n\n\nCognitive Outcomes of Corrective Lenses on Low Income Preschoolers With Hyperopia/Astimgatism: A Longitudinal Pilot Study\n Roch--Levecq, A--C, Brody, B, Thomas, RG, & Brown, SI | Investigative Ophthalmology \\& Visual Science | (2005) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R Cognitive decline Longitudinal studies\nLinks: 📄 Article\n\n\nDetailed assessment of activities of daily living in moderate to severe Alzheimer's disease\n Galasko, D, Schmitt, F, Thomas, R, Jin, S, Bennett, D, & Ferris, S | Journal of the International Neuropsychological Society: JINS | (2005) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease\nLinks: 📄 Article\n\n\nDivalproex sodium in nursing home residents with possible or probable Alzheimer disease complicated by agitation: a randomized, controlled trial\n Tariot, Pierre N, Raman, Rema, Jakimovich, Laura, Schneider, Lon, Porsteinsson, Anton, **Thomas, Ronald**, Mintzer, Jacobo, Brenner, Ronald, Schafer, Kim, & Thal, Leon | The American journal of geriatric psychiatry | (2005) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Clinical trials\nLinks: 📄 Article\n\n\nPropranolol for disruptive behaviors in nursing home residents with probable or possible Alzheimer disease: a placebo-controlled study\n Peskind, Elaine R, Tsuang, Debby W, Bonner, Lauren T, Pascualy, Marcella, Riekse, Robert G, Snowden, Mark B, **Thomas, Ronald**, & Raskind, Murray A | Alzheimer Disease \\& Associated Disorders | (2005) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease\nLinks: 📄 Article\n\n\nSample size considerations in dementia prevention trials: Data from the Alzheimer's disease Cooperative Study MCI Trial\n Edland, SD, May, S, Emond, JA, Wolfson, T, Thal, L, Petersen, RC, & Thomas, RG | NEUROLOGY | (2005) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Prevention trials\nLinks: 📄 Article\n\n\nSelf-management of age-related macular degeneration at the 6-month follow-up: a randomized controlled trial\n Brody, Barbara L, Roch-Levecq, Anne-Catherine, **Thomas, Ronald G**, Kaplan, Robert M, & Brown, Stuart I | Archives of Ophthalmology | (2005) \nSummary: Evaluating therapeutic interventions through rigorous clinical trials\nTopics: Biostatistics R Clinical trials Longitudinal studies\nLinks: 📄 Article\n\n\nTreatment of MCI with cholinesterase inhibitors: current data\n Petersen, RC, Thomas, R, Grundman, M, & Thal, L | INTERNATIONAL PSYCHOGERIATRICS | (2005) \nSummary: Evaluating therapeutic interventions through rigorous clinical trials\nTopics: Biostatistics R Drug development\nLinks: 📄 Article\n\n\nVitamin E and donepezil for the treatment of mild cognitive impairment\n Petersen, Ronald C, **Thomas, Ronald G**, Grundman, Michael, Bennett, David, Doody, Rachelle, Ferris, Steven, Galasko, Douglas, Jin, Shelia, Kaye, Jeffrey, Levey, Allan, & others | New England Journal of Medicine | (2005) \nSummary: Evaluating therapeutic interventions through rigorous clinical trials\nTopics: Biostatistics R Cognitive decline Drug development\nLinks: 📄 Article\n\n\n[O1-04-04]: Operational criteria for patient recruitment in trials of mild cognitive impairment\n Petersen, Ronald C, **Thomas, Ronald G**, Grundman, Michael, Bennett, David A, Kaye, Jeffrey, Levey, Allan I, Pfeiffer, Eric, Sano, Mary, van Dyck, Christopher H, & Thal, Leon J | Alzheimer's \\& Dementia | (2005) \nSummary: Evaluating therapeutic interventions through rigorous clinical trials\nTopics: Biostatistics R Cognitive decline\nLinks: 📄 Article\n\n\n[O2-01-01]: Donepezil and vitamin E in the progression of mild cognitive impairment to Alzheimer's disease: A hazard-ratio analysis\n Thal, Leon J, **Thomas, Ronald G**, Grundman, Michael, Bennett, David A, Doody, Rachelle S, Ferris, Steven H, Galasko, Douglas R, Jin, Shelia, Levey, Allan I, & Petersen, Ronald C | Alzheimer's \\& Dementia | (2005) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Cognitive decline Statistical methods\nLinks: 📄 Article • 📋 Open Access PDF"
  },
  {
    "objectID": "research/index.html#section-20",
    "href": "research/index.html#section-20",
    "title": "Research",
    "section": "2004",
    "text": "2004\n\nA comparison of episodic memory deficits in neuropathologically-confirmed Dementia with Lewy bodies and Alzheimer's disease\n Hamilton, Joanne M, Salmon, David P, Galasko, Douglas, Delis, Dean C, Hansen, Lawrence A, Masliah, Eliezer, **Thomas, Ronald G**, & Thal, Leon J | Journal of the International Neuropsychological Society: JINS | (2004) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Cognitive decline\nLinks: 📄 Article\n\n\nDistinguished From Alzheimer Disease and Normal Aging for Clinical Trials Michael Grundman, MD, MPH; Ronald C. Petersen, PhD, MD; Steven H. Ferris, PhD\n **Thomas, Ronald G**, Aisen, Paul S, Shoulson, Ira, Rosenberg, Roger N, Gwinn-Hardy, Katrina, Mathews, Katherine D, Moore, Steven A, Darnell, Robert B, Lu, Chin-Song, Chou, Yah-Huei Wu, & others | (2004) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Clinical trials Neuroimaging\nLinks: 📄 Article\n\n\nDonepezil and vitamin E for mild cognitive impairment\n Petersen, RC, Thomas, R, & Thal, L | 9th International Congress on Alzheimer’s Disease. Philadelphia | (2004) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R Cognitive decline\nLinks: 📄 Article • 📋 Open Access PDF\n\n\nMAP kinase inhibitors reduce Helicobacter pylori-induced interleukin-8 secretion and the phosphorylation of I$\\\\kappa$B$\\\\\\\\alpha$\n Argent, R, Thomas, R, Boughan, P, Kidd, M, Smith, J, James, M, Bajaj-Elliott, M, & Atherton, J | Helicobacter | (2004) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R\n\n\nMild cognitive impairment can be distinguished from Alzheimer disease and normal aging for clinical trials\n Grundman, Michael, Petersen, Ronald C, Ferris, Steven H, **Thomas, Ronald G**, Aisen, Paul S, Bennett, David A, Foster, Norman L, Jack Jr, Clifford R, Galasko, Douglas R, Doody, Rachelle, & others | Archives of neurology | (2004) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Clinical trials Cognitive decline\n\n\nP1-003 ADCS Prevention instrument project: assessment of activities of daily living (ADL)\n Galasko, Douglas, Bennett, David, Sano, Mary, Marson, Daniel, Jin, Shelia, & **Thomas, Ronald** | Neurobiology of Aging | (2004) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R Prevention trials\nLinks: 📄 Article\n\n\nP1-322 A multicenter, randomized, double-blind, placebo-controlled trial of valproate for agitation associated with dementia\n Tariot, Pierre N, Thal, Leon, Jakimovich, Laura, **Thomas, Ronald**, & Raman, Rema | Neurobiology of Aging | (2004) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Clinical trials\nLinks: 📄 Article\n\n\nReliability of monitoring the clinical dementia rating in multicenter clinical trials\n Schafer, Kimberly A, Tractenberg, Rochelle E, Sano, Mary, Mackell, Joan A, **Thomas, Ronald G**, Gamst, Anthony, Thal, Leon J, Morris, John C, & others | Alzheimer disease and associated disorders | (2004) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Clinical trials\n\n\nS5-03-02 Prevention trials in Alzheimer's disease: design issues\n Thal, Leon | Neurobiology of Aging | (2004) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Prevention trials"
  },
  {
    "objectID": "research/index.html#section-21",
    "href": "research/index.html#section-21",
    "title": "Research",
    "section": "2003",
    "text": "2003\n\nA multicenter, placebo-controlled trial of melatonin for sleep disturbance in Alzheimer's disease\n Singer, Clifford, Tractenberg, Rochelle E, Kaye, Jeffrey, Schafer, Kim, Gamst, Anthony, Grundman, Michael, **Thomas, Ronald**, & Thal, Leon J | Sleep | (2003) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Sleep disorders\n\n\nA multicenter, randomized, placebo controlled, multiple-dose, safety and pharmacokinetic study of AIT-082 (Neotrofin™) in mild Alzheimer's disease patients\n Grundman, M, Capparelli, E, Kim, HT, Morris, JC, Farlow, M, Rubin, EH, Heidebrink, J, Hake, A, Ho, G, Schultz, AN, & others | Life sciences | (2003) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Clinical trials\nLinks: 📄 Article\n\n\nAlzheimer's Disease Cooperative Study. Effects of rofecoxib or naproxen vs placebo on Alzheimer disease progression: a randomized controlled trial\n Aisen, PS, Schafer, KA, Grundman, M, Pfeiffer, E, Sano, M, Davis, KL, Farlow, MR, Jin, S, Thomas, RG, & Thal, LJ | Jama | (2003) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Clinical trials\nLinks: 📄 Article\n\n\nEffects of rofecoxib or naproxen vs placebo on Alzheimer disease progression: a randomized controlled trial\n Aisen, Paul S, Schafer, Kimberly A, Grundman, Michael, Pfeiffer, Eric, Sano, Mary, Davis, Kenneth L, Farlow, Martin R, Jin, Shelia, **Thomas, Ronald G**, Thal, Leon J, & others | Jama | (2003) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Clinical trials\n\n\nEstrogen levels do not correlate with improvement in cognition\n Thal, Leon J, **Thomas, Ronald G**, Mulnard, Ruth, Sano, Mary, Grundman, Michael, & Schneider, Lon | Archives of Neurology | (2003) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R\nLinks: 📄 Article • 📋 Open Access PDF\n\n\nFission fragment angular distributions: A probe to study heavy-ion fusion dynamics\n Thomas, RG, Choudhury, RK, Mohanty, AK, Saxena, A, & Kapoor, SS | Physical Review C | (2003) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R\nLinks: 📄 Article • 📋 Open Access PDF\n\n\nHippocampal volume is associated with memory but not nonmemory cognitive performance in patients with mild cognitive impairment\n Grundman, Michael, Jack, Clifford R, Petersen, Ronald C, Kim, Hyun T, Taylor, Curtis, Datvian, Marina, Weiner, Myron F, DeCarli, Charles, DeKosky, Steven T, Van Dyck, Christopher, & others | Journal of Molecular Neuroscience | (2003) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R Cognitive decline\nLinks: 📄 Article • 📋 Open Access PDF\n\n\nIdebenone treatment fails to slow cognitive decline in Alzheimer’s disease\n Thal, LJ, Grundman, M, Berg, J, Ernstrom, K, Margolin, R, Pfeiffer, E, Weiner, MF, Zamrini, E, & Thomas, RG | Neurology | (2003) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Cognitive decline Drug development\n\n\nNSAIDs and hypertension\n Aisen, Paul S, Schafer, Kimberly, Grundman, Michael, **Thomas, Ronald**, & Thal, Leon J | Archives of internal medicine | (2003) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R\nLinks: 📄 Article\n\n\nReduction of nightmares and other PTSD symptoms in combat veterans by prazosin: a placebo-controlled study\n Raskind, Murray A, Peskind, Elaine R, Kanter, Evan D, Petrie, Eric C, Radant, Allen, Thompson, Charles E, Dobie, Dorcas J, Hoff, David, Rein, Rebekah J, Straits-Troster, Kristy, & others | American Journal of Psychiatry | (2003) \nSummary: Investigating health impacts in military populations and combat environments\nTopics: Biostatistics R Military health\nLinks: 📄 Article • 📋 Open Access PDF\n\n\nSelf-Management of Age-Related Macular Degeneration and Quality of Life at 6 Months Follow-Up: A Randomized Controlled Trial\n Brody, BL, Roch-Levecq, AC, Thomas, RG, Maclean, KK, Kaplan, RM, & Brown, SI | Investigative Ophthalmology \\& Visual Science | (2003) \nSummary: Evaluating therapeutic interventions through rigorous clinical trials\nTopics: Biostatistics R Clinical trials Longitudinal studies\nLinks: 📄 Article\n\n\nSteroid-induced elevation of glucose in Alzheimer’s disease: relationship to gender, apolipoprotein E genotype and cognition\n Aisen, PS, Berg, JD, Craft, S, Peskind, ER, Sano, M, Teri, L, Mulnard, RA, Thomas, RG, & Thal, LJ | Psychoneuroendocrinology | (2003) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease\nLinks: 📄 Article"
  },
  {
    "objectID": "research/index.html#section-22",
    "href": "research/index.html#section-22",
    "title": "Research",
    "section": "2002",
    "text": "2002\n\nA phase I study of AIT-082 in healthy elderly volunteers\n Grundman, Michael, Farlow, Martin, Peavy, Guerry, Kim, Hyun T, Capparelli, Edmund, Schultz, Arlan N, Salmon, David P, Ferris, Steven H, Mohs, Richard, **Thomas, Ronald G**, & others | Journal of Molecular Neuroscience | (2002) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R\nLinks: 📄 Article\n\n\nA randomized trial of the effect of a plant-based dietary pattern on additional breast cancer events and survival:: the Women's Healthy Eating and Living (WHEL) Study\n Pierce, John P, Faerber, Susan, Wright, Fred A, Rock, Cheryl L, Newman, Vicky, Flatt, Shirley W, Kealey, Sheila, Jones, Vicky E, Caan, Bette J, Gold, Ellen B, & others | Controlled clinical trials | (2002) \nSummary: Evaluating therapeutic interventions through rigorous clinical trials\nTopics: Biostatistics R Clinical trials\nLinks: 📄 Article\n\n\nAlzheimer’s disease can be accurately diagnosed in very mildly impaired individuals\n Salmon, David P, Thomas, RG, Pay, MM, Booth, A, Hofstetter, CR, Thal, LJ, & Katzman, R | Neurology | (2002) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease\nLinks: 📄 Article • 📋 Pre-print PDF\n\n\nAssessing Alzheimer's disease patients with the Cohen-Mansfield Agitation Inventory: scoring and clinical implications\n Weiner, Myron F, Tractenberg, Rochelle E, Jin, Shelia, Gamst, Anthony, **Thomas, Ronald G**, Koss, Elisabeth, & Thal, Leon J | Journal of psychiatric research | (2002) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease\nLinks: 📄 Article\n\n\nBrain MRI hippocampal volume and prediction of clinical status in a mild cognitive impairment trial\n Grundman, Michael, Sencakova, Drahomira, Jack, Clifford R, Petersen, Ronald C, Kim, Hyun T, Schultz, Arlan, Weiner, Myron F, DeCarli, Charles, DeKosky, Steven T, Van Dyck, Christopher, & others | Journal of Molecular Neuroscience | (2002) \nSummary: Developing biomarkers and imaging techniques for disease detection\nTopics: Biostatistics R Neuroimaging Cognitive decline\n\n\nClinical correlates of hippocampal atrophy in patients with mild cognitive impairment\n Grundman, M, Kim, HT, Schultz, AN, Thomas, RG, Thal, L, Jack, CR, & Peterson, RC | NEUROBIOLOGY OF AGING | (2002) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R Cognitive decline\nLinks: 📄 Article\n\n\nCorrelates of weight change in Huntington's disease\n Hamilton, JM, Corey-Bloom, J, Thomas, RG, Peavy, G, & Jacobson, MW | NEUROLOGY | (2002) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R\nLinks: 📄 Article\n\n\nDecline in verbal memory during preclinical Alzheimer's disease: examination of the effect of APOE genotype\n Lange, Kelly L, Bondi, Mark W, Salmon, David P, Galasko, Douglas, Delis, Dean C, **Thomas, Ronald G**, & Thal, Leon J | Journal of the International Neuropsychological Society: JINS | (2002) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Cognitive decline\nLinks: 📄 Article • 📋 Pre-print PDF\n\n\nIndomethacin reduces Helicobacter pylori-induced interleukin-8 (IL-8) production by the gastric epithelial cell line, AGS.\n James, MW, Argent, RH, Thomas, R, Hawkey, C, & Atherton, J | GASTROENTEROLOGY | (2002) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R\nLinks: 📄 Article\n\n\nInvestigating emergent symptomatology as an outcome measure in a behavioral study of Alzheimer's disease\n Tractenberg, Rochelle E, Gamst, Anthony, **Thomas, Ronald G**, Patterson, Marian, Schneider, Lon S, & Thal, Leon J | The Journal of neuropsychiatry and clinical neurosciences | (2002) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease\nLinks: 📄 Article • 📋 Open Access PDF\n\n\nNo long-term effect of behavioral treatment on psychotropic drug use for agitation in Alzheimer's disease patients\n Weiner, Myron F, Tractenberg, Rochelle E, Sano, Mary, Logsdon, Rebecca, Teri, Linda, Galasko, Douglas, Gamst, Anthony, Thomas, Ron, & Thal, Leon J | Journal of geriatric psychiatry and neurology | (2002) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Drug development\nLinks: 📄 Article\n\n\nPsychosocial and Functional Parameters in Patients with Age Related Macular Degeneration and Choroidal Neovascularization with and without Photodynamic Therapy\n Goldberg, DE, Roch-Levecq, AC, Maclean, KK, Brody, BL, McGuire, DE, Goldbaum, MH, Thomas, RG, Brown, SI, & Freeman, WR | Investigative Ophthalmology \\& Visual Science | (2002) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R Drug development\nLinks: 📄 Article • 📋 Open Access PDF\n\n\nResults of a multicenter trial of rofecoxib and naproxen in Alzheimer's disease\n Aisen, P, Schafer, K, Grundman, M, Farlow, M, Sano, M, Jin, S, Thomas, R, & Thal, L | Neurobiology of Aging | (2002) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease\nLinks: 📄 Article\n\n\nThe ADCS clinical trial of melatonin for the sleep disturbance of alzheimer's disease: Case report of an unusual sleep/wake cycle and response to melatonin.\n Singer, C, Colling, E, Tractenberg, R, Grundman, M, Gamst, A, Thomas, R, & Thal, L | AMERICAN JOURNAL OF GERIATRIC PSYCHIATRY | (2002) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Sleep disorders Clinical trials\nLinks: 📄 Article"
  },
  {
    "objectID": "research/index.html#section-23",
    "href": "research/index.html#section-23",
    "title": "Research",
    "section": "2001",
    "text": "2001\n\nFrequency of behavioral symptoms characterizes agitation in Alzheimer's disease\n Tractenberg, Rochelle E, Gamst, Anthony, Weiner, Myron F, Koss, Elisabeth, **Thomas, Ronald G**, Teri, Linda, & Thal, Leon | International journal of geriatric psychiatry | (2001) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease\nLinks: 📄 Article\n\n\nIncidence and persistence of psychosis in Alzheimer's disease\n Sano, MC, Berg, JD, Thomas, RG, Schneider, LS, Aisen, PS, Mulnard, R, & Thal, LJ | Neurology | (2001) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease\nLinks: 📄 Article\n\n\nMetabolic syndrome and ischemic heart disease in elderly men and women\n Lindblad, Ulf, Langer, Robert D, Wingard, Deborah L, **Thomas, Ronald G**, & Barrett-Connor, Elizabeth L | American journal of epidemiology | (2001) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R\nLinks: 📄 Article"
  },
  {
    "objectID": "research/index.html#section-24",
    "href": "research/index.html#section-24",
    "title": "Research",
    "section": "2000",
    "text": "2000\n\nA randomized controlled trial of prednisone in Alzheimer’s disease\n Aisen, Paul S, Davis, KL, Berg, JD, Schafer, K, Campbell, K, Thomas, RG, Weiner, MF, Farlow, MR, Sano, M, Grundman, M, & others | Neurology | (2000) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Clinical trials\n\n\nA randomized controlled trial of prednisone in Alzheimer’s disease\n Koch, HJ & Szecsey, A | Neurology | (2000) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Clinical trials\nLinks: 📄 Article\n\n\nAnalysis of longitudinal data in an Alzheimer's disease clinical trial\n **Thomas, Ronald G**, Berg, Julie D, Sano, Mary, & Thal, Leon | Statistics in medicine | (2000) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Clinical trials Statistical methods Longitudinal studies\n\n\nComparison of the serial position effect in very mild Alzheimer's disease, mild Alzheimer's disease, and amnesia associated with electroconvulsive therapy\n Bayley, Peter J, Salmon, David P, Bondi, Mark W, Bui, Barbara K, Olichney, John, Delis, Dean C, **Thomas, Ronald G**, & Thal, Leon J | Journal of the International Neuropsychological Society | (2000) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Drug development\nLinks: 📄 Article\n\n\nDescription of behaviors emerging in community-dwelling persons with Alzheimer's disease over 12 months\n Gamst, A, Thomas, RG, Patterson, M, & Schneider, L | ANNALS OF NEUROLOGY | (2000) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease\nLinks: 📄 Article\n\n\nDisorder among Veterans with Substance Abuse\n THOMAS, R & MICHAEL, J | THE JOURNaL OF NERvOUs AND MEntal Disease | (2000) \nSummary: Investigating health impacts in military populations and combat environments\nTopics: Biostatistics R Military health\n\n\nEstrogen replacement therapy for treatment of mild to moderate Alzheimer disease: a randomized controlled trial\n Mulnard, Ruth A, Cotman, Carl W, Kawas, Claudia, van Dyck, Christopher H, Sano, Mary, Doody, Rachelle, Koss, Elizabeth, Pfeiffer, Eric, Jin, Shelia, Gamst, Anthony, & others | Jama | (2000) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Clinical trials Drug development\n\n\nIncidence of and risk factors for hallucinations and delusions in patients with probable AD\n Paulsen, Jane S, Salmon, DP, Thal, Leon J, Romero, R, Weisstein--Jenkins, C, Galasko, D, Hofstetter, CR, Thomas, R, Grant, I, & Jeste, DV | Neurology | (2000) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R\nLinks: 📄 Article\n\n\nPredicting nursing home placement with change on cognitive measures in Alzheimer's disease\n Sano, MC, Berg, JD, Knopman, D, Farlow, MR, & Thomas, RG | Neurology | (2000) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Cognitive decline\nLinks: 📄 Article\n\n\nPrevalence of symptoms on the CERAD behavior rating scale for dementia in normal elderly subjects and Alzheimer’s disease patients\n Tractenberg, Rochelle E, Patterson, Marian, Weiner, Myron F, Teri, Linda, Grundman, Michael, **Thomas, Ronald G**, & Thal, Leon J | The Journal of neuropsychiatry and clinical neurosciences | (2000) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease\n\n\nQualifying change: a method for defining clinically meaningful outcomes of change score computation\n Tractenberg, Rochelle E, Jin, Shelia, Patterson, Marian, Schneider, Lon S, Gamst, Anthony, **Thomas, Ronald G**, & Thal, Leon J | Journal of the American Geriatrics Society | (2000) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R Statistical methods\nLinks: 📄 Article\n\n\nQuantifying behavioral disturbance in Alzheimer's disease patients\n Weiner, Myron F, Tractenberg, Rochelle, Teri, Linda, Logsdon, Rebecca, **Thomas, Ronald G**, Gamst, Anthony, & Thal, Leon J | Journal of Psychiatric Research | (2000) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease\nLinks: 📄 Article\n\n\nTreatment of agitation in AD: a randomized, placebo-controlled clinical trial\n Teri, Linda, Logsdon, RG, Peskind, E, Raskind, M, Weiner, MF, Tractenberg, RE, Foster, NL, Schneider, LS, Sano, M, Whitehouse, P, & others | Neurology | (2000) \nSummary: Evaluating therapeutic interventions through rigorous clinical trials\nTopics: Biostatistics R Clinical trials Drug development\nLinks: 📄 Article • 📋 Open Access PDF\n\n\nUse of brain MRI volumetric analysis in a mild cognitive impairment trial to delay the diagnosis of Alzheimer’s disease\n Grundman, Michael, Sencakova, Drahomira, Jack, CR, Fillit, H, & O’Connell, A | Drug discovery and development for Alzheimer’s disease | (2000) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Neuroimaging Cognitive decline Statistical methods\n\n\nfor the Alzheimer's Disease Cooperative Study: Estrogen replacement therapy for treatment of mild to moderate Alzheimer disease: a randomized controlled trial\n Mulnard, RA, Cotman, CW, Kawas, C, Van Dyck, CH, Sano, M, Doody, R, Koss, E, Pfeiffer, E, Jin, S, Gamst, A, Grundman, M, Thomas R, & Thal L | JAMA | (2000) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Clinical trials Drug development\nLinks: 📄 Article"
  },
  {
    "objectID": "research/index.html#section-25",
    "href": "research/index.html#section-25",
    "title": "Research",
    "section": "1999",
    "text": "1999\n\nA normative study of Nelson's (1976) modified version of the Wisconsin Card Sorting Test in healthy older adults\n Lineweaver, Tara T, Bondi, Mark W, **Thomas, Ronald G**, & Salmon, David P | The Clinical Neuropsychologist | (1999) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R\nLinks: 📄 Article\n\n\nAge-related macular degeneration: a randomized clinical trial of a self-management intervention\n Brody, Barbara L, Williams, Rebecca A, **Thomas, Ronald G**, Kaplan, Robert M, Chu, Ray M, & Brown, Stuart I | Annals of Behavioral Medicine | (1999) \nSummary: Evaluating therapeutic interventions through rigorous clinical trials\nTopics: Biostatistics R Clinical trials\nLinks: 📄 Article\n\n\nAssessment of agitation in Alzheimer's disease: the agitated behavior in dementia scale\n Logsdon, Rebecca G, Teri, Linda, Weiner, Myron F, Gibbons, Laura E, Raskind, Murray, Peskind, Elaine, Grundman, &gt; Michael, Koss, Elisabeth, **Thomas, Ronald G**, Thai, Leon J, & others | Journal of the American Geriatrics Society | (1999) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease\nLinks: 📄 Article\n\n\nBrief Methodological Reports-Assessment of Agitation in Alzheimer's Disease: The Agitated Behavior in Dementia Scale\n Logsdon, Rebecca G, Teri, Linda, Weiner, Myron F, Gibbons, Laura E, Raskind, Murray, Peskind, Elaine, Grundman, Michael, Koss, Elisabeth, **Thomas, Ronald G**, & Thal, Leon J | Journal of the American Geriatrics Society | (1999) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Statistical methods\nLinks: 📄 Article\n\n\nClinical symptoms of dementia with Lewy bodies: Secondary analyses of the Alzheimer's disease cooperative study selegiline and vitamin E clinical trial\n Olin, JT, Papka, M, Jin, S, Sano, M, Grundman, M, & Thomas, R | European Neuropsychopharmacology | (1999) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Clinical trials\nLinks: 📄 Article\n\n\nEthnic differences in clinical measures among participants in Alzheimer's disease clinical trials\n Bell, Karen, Sano, Mary, Jin, Shelia, **Thomas, Ronald**, & Thal, Leon | Neurology | (1999) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Clinical trials\nLinks: 📄 Article\n\n\nMini-mental state examination and Mattis Dementia Rating Scale performance differs in Hispanic and non-Hispanic Alzheimer's disease patients\n Hohl, Ursula, Grundman, Michael, Salmon, David P, **Thomas, Ronald G**, & Thal, Leon J | Journal of the International Neuropsychological Society | (1999) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease\nLinks: 📄 Article\n\n\nNeurochemical markers do not correlate with cognitive decline in the Lewy body variant of Alzheimer disease\n Sabbagh, Marwan N, Corey-Bloom, Jody, Tiraboschi, Pietro, **Thomas, Ronald**, Masliah, Eliezer, & Thal, Leon J | Archives of neurology | (1999) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Cognitive decline\nLinks: 📄 Article • 📋 Open Access PDF\n\n\nNeuropsychological function and apolipoprotein E genotype in the preclinical detection of Alzheimer's disease.\n Bondi, Mark W, Salmon, David P, Galasko, Douglas, **Thomas, Ronald G**, & Thal, Leon J | Psychology and aging | (1999) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease\nLinks: 📄 Article\n\n\nNursing home placement is related to dementia progression: experience from a clinical trial\n Knopman, David S, Berg, JD, Thomas, R, Grundman, M, Thal, LJ, Sano, M, & others | Neurology | (1999) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Clinical trials\nLinks: 📄 Article\n\n\nThe Beneficial Effects of Vitamin E and Selegiline in a Controlled Trial in Alzheimer's Disease Are Independent of the Apolipoprotein E e4 Allele\n Galasko, Douglas, Sano, Mary, Berg, Julie, **Thomas, Ronald**, Grundman, Michael, & Thal, Leon | Neurology | (1999) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease\nLinks: 📄 Article\n\n\nWeight gain in infants breastfed by mothers who take fluoxetine\n Chambers, Christina D, Anderson, Philip O, **Thomas, Ronald G**, Dick, Lyn M, Felix, Robert J, Johnson, Kathleen A, & Jones, Kenneth Lyons | Pediatrics | (1999) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R\nLinks: 📄 Article"
  },
  {
    "objectID": "research/index.html#section-26",
    "href": "research/index.html#section-26",
    "title": "Research",
    "section": "1998",
    "text": "1998\n\nA comparison of the Cohen-Mansfield agitation inventory with the cerad behavioral rating scale for dementia in community-dwelling persons with Alzheimers disease\n Weiner, Myron F, Koss, Elisabeth, Patterson, Marian, Jin, Shelia, Teri, Linda, Thomas, Ron, Thal, Leon J, & Whitehouse, Peter | Journal of psychiatric research | (1998) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease\nLinks: 📄 Article\n\n\nAgreement on CDR ratings by committee\n Tractenberg, Rochelle, Schafer, Kimberly, Thomas, Ron, & Morris, John C | Controlled Clinical Trials | (1998) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R\nLinks: 📄 Article\n\n\nAssociation of CYP2D microsatellite polymorphism with Lewy body variant of Alzheimer's disease\n Tanaka, S, Chen, X, Xia, Y, Kang, DE, Matoh, N, Sundsmo, M, Thomas, RG, Katzman, R, Thal, LJ, Trojanowski, JQ, & others | Neurology | (1998) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease\nLinks: 📄 Article • 📋 Open Access PDF\n\n\nCognitive and functional abilities in severely demented Alzheimer's patients\n Peavy, GM, Salmon, DP, & Thomas, RG | CLINICAL NEUROPSYCHOLOGIST | (1998) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Cognitive decline\nLinks: 📄 Article\n\n\nDesign and baseline characteristics of the veterans affairs non-Q-wave infarction strategies in-hospital (VANQWISH) trial\n Ferry, David R, O’Rourke, Robert A, Blaustein, Alvin S, Crawford, Michael H, Deedwania, Prakash C, Carson, Peter E, Zoble, Robert G, Pepine, Carl J, **Thomas, Ronald G**, Chow, Bruce K, & others | Journal of the American College of Cardiology | (1998) \nSummary: Investigating health impacts in military populations and combat environments\nTopics: Biostatistics R Military health\n\n\nDynamic measurement scale development for clinical trials in targeted populations\n Jin, Shelia, **Thomas, Ronald G**, Galasko, Douglas, & Thal, Leon J | Controlled Clinical Trials | (1998) \nSummary: Evaluating therapeutic interventions through rigorous clinical trials\nTopics: Biostatistics R Clinical trials Epidemiology\nLinks: 📄 Article • 📋 Open Access PDF\n\n\nHigh cerebrospinal fluid tau and low amyloid beta42 levels in the clinical diagnosis of Alzheimer disease and relation to apolipoprotein E genotype\n Galasko, D, Chang, L, Motter, R, Clark, CM, Kaye, Jeffrey, Knopman, D, Thomas, R, Kholodenko, D, Schenk, D, Lieberburg, I, & others | Archives of neurology | (1998) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease\n\n\nInteraction of apolipoprotein E 4 with other genetic and non-genetic risk factors in late onset Alzheimer disease: problems facing the investigator\n Katzman, R, Kang, D, & Thomas, R | Neurochemical research | (1998) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease\nLinks: 📄 Article\n\n\nMeasuring cognitive progression in Alzheimer's disease\n Berg, Julie D, **Thomas, Ronald G**, Thal, Leon J, & Sano, Mary | Controlled Clinical Trials | (1998) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Cognitive decline\n\n\nPower calculation for randomized start design\n Kean, Yin M, **Thomas, Ronald G**, & Thal, Leon J | Controlled Clinical Trials | (1998) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R Clinical trials\nLinks: 📄 Article\n\n\nPractice effects on the modified Wisconsin card sorting test in normally aging adults\n Lineweaver, TT, Bondi, MW, & Thomas, RG | Archives of Clinical Neuropsychology | (1998) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R\nLinks: 📄 Article\n\n\nThe electrocardiographic exercise test in a population with reduced workup bias: diagnostic performance, computerized interpretation, and multivariable prediction\n Froelicher, Victor F, Lehmann, Kenneth G, **Thomas, Ronald**, Goldman, Steven, Morrison, Douglas, Edson, Robert, Lavori, Philip, Myers, Jonathan, Dennis, Charles, Shabetai, Ralph, & others | Annals of internal medicine | (1998) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R Epidemiology\nLinks: 📄 Article\n\n\nThe psychosocial impact of macular degeneration\n Williams, Rebecca A, Brody, Barbara L, **Thomas, Ronald G**, Kaplan, Robert M, & Brown, Stuart I | Archives of ophthalmology | (1998) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R\n\n\nThe relationship between Nursing home placement and measures of change in Alzheimer's disease\n Knopman, David, Sano, Mary, Berg, Julie, & **Thomas, Ronald** | Neurology | (1998) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease\nLinks: 📄 Article\n\n\nUse of the world wide web in data dissemination to central review committees\n Schafer, Kimberly A, Welty, Greg, & **Thomas, Ronald G** | Controlled Clinical Trials | (1998) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R\nLinks: 📄 Article\n\n\nUtility of psychophysiological measurement in the diagnosis of posttraumatic stress disorder: results from a Department of Veterans Affairs Cooperative Study.\n Keane, Terence M, Kolb, Lawrence C, Kaloupek, Danny G, Orr, Scott P, Blanchard, Edward B, **Thomas, Ronald G**, Hsieh, Frank Y, & Lavori, Philip W | Journal of consulting and clinical psychology | (1998) \nSummary: Investigating health impacts in military populations and combat environments\nTopics: Biostatistics R Military health"
  },
  {
    "objectID": "research/index.html#section-27",
    "href": "research/index.html#section-27",
    "title": "Research",
    "section": "1997",
    "text": "1997\n\n53 Power comparisons among different number of categories under ordered polytomous logistic regression model\n Jeong, Jong-Hyeon, Klauber, Melville R, **Thomas, Ronald G**, Grundman, Michael, & Thal, Leon J | Controlled Clinical Trials | (1997) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R\nLinks: 📄 Article • 📋 Open Access PDF\n\n\nA Multicenter Evaluation of New Treatment Efficacy\n Whitehouse, Peter J, Schmitt, HFrederick A, Sano, Mary, & **Thomas, Ronald G** | Alzheimer Disease and Associated Disorders | (1997) \nSummary: Evaluating therapeutic interventions through rigorous clinical trials\nTopics: Biostatistics R Drug development\n\n\nA controlled trial of selegiline, alpha-tocopherol, or both as treatment for Alzheimer's disease\n Sano, Mary, Ernesto, Christopher, **Thomas, Ronald G**, Klauber, Melville R, Schafer, Kimberly, Grundman, Michael, Woodbury, Peter, Growdon, John, Cotman, Carl W, Pfeiffer, Eric, & others | New England Journal of Medicine | (1997) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Drug development\nLinks: 📄 Article • 📋 Open Access PDF\n\n\nA longitudinal study of behavioral pathology across five levels of dementia severity in Alzheimer's disease: The CERAD Behavior Rating Scale for Dementia.\n Patterson, Marian B, Mack, James L, Mackell, Joan A, **Thomas, Ronald**, Tariot, Pierre, Weiner, Myron, & Whitehouse, Peter J | Alzheimer disease and associated disorders | (1997) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Longitudinal studies\nLinks: 📄 Article\n\n\nAlpha-tocopherol and Alzheimer's disease\n Sano, Mary, **Thomas, Ronald G**, & Thal, Leon J | The New England Journal of Medicine | (1997) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease\nLinks: 📄 Article\n\n\nAn inventory to assess activities of daily living for clinical trials in Alzheimer's disease.\n Galasko, Douglas, Bennett, David, Sano, Mary, Ernesto, Chris, **Thomas, Ronald**, Grundman, Michael, & Ferris, Steven | Alzheimer disease and associated disorders | (1997) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Clinical trials\nLinks: 📄 Article\n\n\nApoE genotype influences the CSF level of A $\\\\beta$ 42 in Alzheimer's disease\n Seubert, PA, Motter, RN, Schenk, DB, Lieberburg, IM, Kholodenko, D, Galasko, D, Thomas, R, Chang, L, Miller, B, Clark, C, & others | Neurology | (1997) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease\n\n\nAssessing patterns of agitation in Alzheimer's disease patients with the Cohen-Mansfield Agitation Inventory. The Alzheimer's Disease Cooperative Study.\n Koss, Elisabeth, Weiner, Myron, Ernesto, Christopher, Cohen-Mansfield, Jiska, Ferris, Steven H, Grundman, Michael, Schafer, Kimberly, Sano, Mary, Thal, Leon J, **Thomas, Ronald**, & others | Alzheimer Disease and Associated Disorders | (1997) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease\nLinks: 📄 Article\n\n\nCSF levels of A beta 432 and tau as aids to diagnosing Alzheimer's disease\n Galasko, D, Seubert, P, Motter, R, Schenk, D, Kholodenko, D, Lieberburg, I, Chang, L, Miller, B, Clark, C, Kaye, J, & others | Neurology | (1997) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease\nLinks: 📄 Article • 📋 Open Access PDF\n\n\nDefective Neurite Extension Is Caused by a Mutation in Amyloid /A4 (A ) Protein Precursor Found in Familial Alzheimer's Disease\n Roch, J-M, Sundsmo, M, Otero, D, Sisodia, S, Thomas, R, & Saitoh, T | JOURNAL OF NEUROBIOLOGY | (1997) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease\nLinks: 📄 Article\n\n\nDefective neurite extension is caused by a mutation in amyloid beta  protein precursor found in familial Alzheimer's disease\n Li, Hai Ling, Roch, Jean-Marc, Sundsmo, Mary, Otero, Deborah, Sisodia, Sangram, **Thomas, Ronald**, & Saitoh, Tsunao | Journal of neurobiology | (1997) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease\nLinks: 📄 Article\n\n\nDiagnostic accuracy of dementia with Lewy bodies: A prospective evaluation\n Hohl, U, CoreyBloom, J, Hansen, LA, Thomas, RG, & Thal, LJ | Neurology | (1997) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R\nLinks: 📄 Article • 📋 Open Access PDF\n\n\nEffects of Selegiline and alpha-Tocopherol on cognitive and functional outcome measures in moderately impaired patients with Alzheimer's disease\n Sano, M, Ernesto, C, Thomas, RG, Klauber, MR, Schafer, K, Grundman, M, Woodbury, P, Growdon, J, Cotman, CW, Pfeiffer, E, & others | Neurology | (1997) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Cognitive decline\nLinks: 📄 Article\n\n\nEffects of apolipoprotein E on dementia and aging in the Shanghai Survey of Dementia\n Katzman, R, Zhang, M-Y, Chen, PJ, Gu, N, Jiang, S, Saitoh, T, Chen, X, Klauber, M, Thomas, RG, Liu, WT, & others | Neurology | (1997) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R\nLinks: 📄 Article • 📋 Open Access PDF\n\n\nEstrogen, apolipoprotein E, and dementia\n Barrett-Connor, Elizabeth & **Thomas, Ronald G** | Journal of women's health | (1997) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R\n\n\nGenetic association of the low-density lipoprotein receptor-related protein gene (LRP), and apolipoprotein E receptor, with late-onset Alzheimer's disease\n Kang, DE, Saitoh, T, Chen, X, Xia, Y, Masliah, E, Hansen, LA, Thomas, RG, Thal, LJ, & Katzman, R | Neurology | (1997) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease\n\n\nP23 use of the world wide web for clinical monitoring in multicenter clinical trials\n Schafer, Kimberly, **Thomas, Ronald G**, Welty, Greg, Berry, Angela Lambert, & Schittini, Mario | Controlled Clinical Trials | (1997) \nSummary: Evaluating therapeutic interventions through rigorous clinical trials\nTopics: Biostatistics R Clinical trials\nLinks: 📄 Article\n\n\nQuality of life among elderly adults with macular degeneration\n Williams, RA, Brody, BL, Kaplan, RM, Thomas, RG, & Brown, SI | INVESTIGATIVE OPHTHALMOLOGY \\& VISUAL SCIENCE | (1997) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R\nLinks: 📄 Article\n\n\nTacrine and nursing home placement\n Thal, Leon J, **Thomas, Ronald G**, & Sano, Mary | Neurology | (1997) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R\n\n\nThe Spanish Instrument Protocol: Design and implementation of a study to evaluate treatment efficacy instruments for Spanish-speaking patients with Alzheimer's disease.\n Sano, M, Mackell, JA, Ponton, M, Ferreira, P, Wilson, J, Pawluczyk, S, Pfeiffer, E, Thomas, RG, Jin, S, Schafer, K, & others | Alzheimer disease and associated disorders | (1997) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Drug development\nLinks: 📄 Article\n\n\nValidity and reliability of the Alzheimer’s Disease Cooperative Study-Clinical global impression of change (ADCS-CGIC)\n Schneider, Lon S, Olin, Jason T, Doody, Rachelle S, Clark, Christopher M, Morris, John C, Reisberg, Barry, Ferris, Steven H, Schmitt, Frederick A, Grundman, Michael, & **Thomas, Ronald G** | Alzheimer Disease | (1997) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease\nLinks: 📄 Article\n\n\nfor the members of the Alzheimer’s Disease Cooperative Study\n Sano, M, Ernesto, C, Thomas, RG, Klauber, MR, Schafer, K, Grundman, M, Woodbury, P, Growden, J, Cotnman, C, Pfeiffer, E, & others | A controlled trial of selegiline, alpha-tocopherol, or both as treatment for Alzheimer’s disease. N Engl J Med | (1997) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease\n\n\nthe Alzheimer's Disease Cooperative Study. Validity and reliability of the Alzheimer's disease cooperative study-clinical global impression of change\n Schneider, LS, Olin, JT, Doody, RS, Clark, CM, Morris, JC, Reisberg, B, Schmitt, FA, Grundman, M, Thomas, RG, & Ferris, SH | Alzheimer Dis Assoc Disord | (1997) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease\nLinks: 📄 Article\n\n\nthe Alzheimer’s Disease Cooperative Study. An inventory to assess activities of daily living for clinical trials in Alzheimer’s disease\n Galasko, D, Bennett, D, Sano, M, Ernesto, C, Thomas, R, Grundman, M, & Ferris, S | Alzheimer Dis Assoc Disord | (1997) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Clinical trials"
  },
  {
    "objectID": "research/index.html#section-28",
    "href": "research/index.html#section-28",
    "title": "Research",
    "section": "1996",
    "text": "1996\n\nA32 computer-aided clinical monitoring: Results of a controlled experiment\n **Thomas, Ronald G**, Schafer, Kimberly, Woodbury, Peter, White, Beverly, Mackell, Joan, Lambert, Angie, & Scattini, Mario | Controlled Clinical Trials | (1996) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R\n\n\nA34 Clinical monitoring of rating scales in multicenter clinical trials\n Schafer, Kimberly, Ernesto, Christopher, Sano, Mary, Mackell, Joan, **Thomas, Ronald**, & Morris, John C | Controlled Clinical Trials | (1996) \nSummary: Evaluating therapeutic interventions through rigorous clinical trials\nTopics: Biostatistics R Clinical trials\nLinks: 📄 Article • 📋 Open Access PDF\n\n\nApoE and CYP2D6 polymorphism with and without parkinsonism-dementia complex in the people of Chamorro, Guam\n Chen, X, Xia, Y, Gresham, LS, Molgaard, CA, Thomas, RG, Galasko, D, Wiederholt, WC, & Saitoh, T | Neurology | (1996) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R\nLinks: 📄 Article\n\n\nApolipoprotein-epsilon4 and head trauma: Synergistic or additive risks?\n Katzman, R, Galasko, DR, Saitoh, T, Chen, X, Pay, MM, Booth, A, & Thomas, RG | Neurology | (1996) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R\n\n\nDevelopment of a pool of items to assess activities of daily living in clinical trials for Alzheimer's disease\n Galasko, D, Bennett, D, Ernesto, C, Thomas, R, & Sano, M | Neurology | (1996) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Clinical trials\nLinks: 📄 Article\n\n\nEvaluation of efficacy measures in clinical trials for Alzheimer's disease: Does psychometric test performance predict clinically relevant outcomes?\n Sano, M, Growdon, J, Thomas, R, Ernesto, C, Schafer, K, Woodbury, P, Grundman, M, & Thal, L | Neurology | (1996) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Clinical trials\nLinks: 📄 Article\n\n\nFamilial melanoma and pancreatic cancer\n Wright, Fred A & **Thomas, Ronald G** | The New England journal of medicine | (1996) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R\nLinks: 📄 Article\n\n\nP63 Informed consent issues when including genetic testing in clinical trials\n Schafer, Kimberly, **Thomas, Ronald**, Galasko, Douglas, Morris, John C, Whitehouse, Peter, Bochenek, Jacqueline, & Thal, Leon | Controlled Clinical Trials | (1996) \nSummary: Evaluating therapeutic interventions through rigorous clinical trials\nTopics: Biostatistics R Clinical trials\nLinks: 📄 Article\n\n\nRate of dementia of the Alzheimer type (DAT) in subjects with mild cognitive impairment\n Grundman, Michael, Petersen, Ronald C, Morris, JC, Ferris, S, Sano, Mary, Farlow, Martin R, Doody, Rachel S, Galasko, D, Ernesto, C, Thomas, RG, & others | Neurology | (1996) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Cognitive decline\nLinks: 📄 Article\n\n\nRationale and design of a multicenter study of selegiline and alpha-tocopherol in the treatment of Alzheimer disease using novel clinical outcomes. Alzheimer's Disease Cooperative Study.\n Sano, Mary, Ernesto, Christopher, Klauber, Melville R, Schafer, Kimberly, Woodbury, Peter, **Thomas, Ronald**, Grundman, Michael, Growdon, John, & Thal, Leon J | Alzheimer disease and associated disorders | (1996) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Drug development\nLinks: 📄 Article • 📋 Open Access PDF\n\n\nSubstance-dependent inpatients who accept smoking treatment\n Seidner, Andrea L, Burling, Thomas A, Gaither, David E, & **Thomas, Ronald G** | Journal of Substance Abuse | (1996) \nSummary: Evaluating therapeutic interventions through rigorous clinical trials\nTopics: Biostatistics R Drug development\n\n\nValidity and reliability of the Alzheimers disease cooperative study-clinical global impression of change (ADCS-CGIC)\n Schneider, Lon S, Olin, Jason T, Doody, Rachelle S, Clark, Christopher M, Morris, John C, Reisberg, Barry, Ferris, Steven H, Schmitt, Frederick A, Grundman, Michael, & **Thomas, Ronald G** | Alzheimer Disease Springer | (1996) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease"
  },
  {
    "objectID": "research/index.html#section-29",
    "href": "research/index.html#section-29",
    "title": "Research",
    "section": "1995",
    "text": "1995\n\nProspective study of hospitalization for asthma: a preliminary risk factor model\n Li, Dominic, German, Donald, Lulla, Sulochina, **Thomas, Ronald G**, & Wilson, Sandra R | American journal of respiratory and critical care medicine | (1995) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R\nLinks: 📄 Article\n\n\nThe Spanish Instrument Protocol: a Study to Evaluate Treatment Efficacy Instruments for Spanish-Speaking Patients with Alzheimer's Disease\n Thomas, RG, Jin, S, Schafer, K, Schittini, M, Grundman, M, & Ferris, SH | Alzheimer Disease and Associated Disorders | (1995) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Drug development"
  },
  {
    "objectID": "research/index.html#section-30",
    "href": "research/index.html#section-30",
    "title": "Research",
    "section": "1994",
    "text": "1994\n\nApplication of neural networks to the classification of giant cell arteritis\n Astion, Michael L, Wener, Mark H, **Thomas, Ronald G**, Hunder, Gene G, & Bloch, Daniel A | Arthritis \\& Rheumatism: Official Journal of the American College of       Rheumatology | (1994) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R\n\n\nDrug and alcohol abuse inpatients' attitudes about smoking cessation\n Irving, Lori M, Seidner, Andrea L, Burling, Thomas A, **Thomas, Ronald G**, & Brenner, Gail F | Journal of Substance Abuse | (1994) \nSummary: Evaluating therapeutic interventions through rigorous clinical trials\nTopics: Biostatistics R Drug development\nLinks: 📄 Article\n\n\nZweibaryonensysteme mit Strangeness und die Antikaon-Deuteron Streuung\n Thomas, Ralf | (1994) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R\nLinks: 📄 Article"
  },
  {
    "objectID": "research/index.html#section-31",
    "href": "research/index.html#section-31",
    "title": "Research",
    "section": "1993",
    "text": "1993\n\nA controlled trial of two forms of self-management education for adults with asthma\n Wilson, Sandra R, Scamagas, Peter, German, Donald F, Hughes, Gary W, Lulla, Sulochina, Coss, Stamatiki, Chardon, Luis, **Thomas, Ronald G**, Starr-Schneidkraut, Norma, Stancavage, Frances B, & others | The American journal of medicine | (1993) \nSummary: Evaluating therapeutic interventions through rigorous clinical trials\nTopics: Biostatistics R\n\n\nAlgorithm AS 280: the power function for Fisher's exact test\n Conlon, Michael & **Thomas, Ronald G** | Journal of the Royal Statistical Society. Series C (Applied Statistics) | (1993) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R\n\n\nOvertraining in neural networks that interpret clinical data\n Astion, ML, Wener, MH, Thomas, RG, Hunder, GG, & Bloch, DA | Clinical chemistry | (1993) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R\n\n\nPrediction of cardiovascular death in men undergoing noninvasive evaluation for coronary artery disease\n Morrow, Kiernan, Morris, Charles K, Froelicher, Victor F, Hideg, Alisa, Hunter, Dodie, Johnson, Eileen, Kawaguchi, Takeo, Lehmann, Kenneth, Ribisl, Paul M, **Thomas, Ronald**, & others | Annals of Internal Medicine | (1993) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R\nLinks: 📄 Article\n\n\nTrimethoprim-sulfamethoxazole prophylaxis in granulocytopenic patients with acute leukemia: evaluation of serum antibiotic levels in a randomized, double-blind, placebo-controlled Department of Veterans Affairs Cooperative Study\n Ward, TT, Thomas, RG, Fye, CL, Arbeit, R, Coltman Jr, CA, Craig, W, Dana, BW, Finegold, SM, Lentino, J, Penn, RL, & others | Clinical infectious diseases | (1993) \nSummary: Investigating health impacts in military populations and combat environments\nTopics: Biostatistics R Clinical trials Military health"
  },
  {
    "objectID": "research/index.html#section-32",
    "href": "research/index.html#section-32",
    "title": "Research",
    "section": "1992",
    "text": "1992\n\nAn algorithm for the rapid evaluation of the power function for Fisher's Exact Test\n **Thomas, Ronald G** & Conlon, Michael | Journal of statistical computation and simulation | (1992) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R\nLinks: 📄 Article\n\n\nCalculation of observables in the pion-deuteron system. Berechnung von Observablen im Pion Deuteron-System\n Beuschel, T, Feldkeller, B, Fuchs, M, Huber, MG, Metsch, BC, & Thomas, R | Verhandlungen der Deutschen Physikalischen Gesellschaft;(Germany) | (1992) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R\n\n\nConcave hymenal variations in suspected child sexual abuse victims\n Kerns, David L, Ritter, Mary L, & **Thomas, Ronald G** | Pediatrics | (1992) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R\nLinks: 📄 Article\n\n\nPROGRESSION OF FUNCTIONAL DISABILITY IN RHEUMATOID-ARTHRITIS\n RAYNAULD, JP, THOMAS, RG, & BLOCH, DA | ARTHRITIS AND RHEUMATISM | (1992) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R\nLinks: 📄 Article\n\n\nSample size determination based on Fisher's exact test for use in 2 x 2 comparative trials with low event rates\n **Thomas, Ronald G** & Conlon, Michael | Controlled clinical trials | (1992) \nSummary: Evaluating therapeutic interventions through rigorous clinical trials\nTopics: Biostatistics R\n\n\nTwo-baryon systems with strangeness S=-1 and S=-2\n Fuchs, M, Huber, MG, Metsch, BC, & Thomas, R | Verhandlungen der Deutschen Physikalischen Gesellschaft | (1992) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R\nLinks: 📄 Article • 📋 Open Access PDF"
  },
  {
    "objectID": "research/index.html#section-33",
    "href": "research/index.html#section-33",
    "title": "Research",
    "section": "1991",
    "text": "1991\n\nAn analysis of methods of communication in clinical trials\n Sheridan, Lenore & **Thomas, Ronald G** | Controlled Clinical Trials | (1991) \nSummary: Evaluating therapeutic interventions through rigorous clinical trials\nTopics: Biostatistics R Clinical trials Statistical methods\n\n\nIntegration of statutory provisions of NEPA, RCRA, and CERCLA at the Savannah River site. Revision 1\n Gordon, DE, Thomas, R, Shedrow, CB, & Wilson, MP | (1991) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R\nLinks: 📄 Article"
  },
  {
    "objectID": "research/index.html#section-34",
    "href": "research/index.html#section-34",
    "title": "Research",
    "section": "1990",
    "text": "1990\n\nA new confidence interval for the difference of two binomial proportions\n Conlon, Michael & **Thomas, Ronald G** | Computational Statistics \\& Data Analysis | (1990) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R\nLinks: 📄 Article\n\n\nData monitoring through stochastic curtailing when the outcome proportions are small: An exact approach\n **Thomas, Ronald G** | Controlled Clinical Trials | (1990) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R\nLinks: 📄 Article\n\n\nDevelopment of an active target for scattering of neutral baryons\n Thomas, R, Empl, E, Kilian, K, Oelert, W, Roderburg, E, Sefzick, T, Sehl, G, Steinkamp, O, & Ziolkowski, M | Verhandlungen der Deutschen Physikalischen Gesellschaft | (1990) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R\nLinks: 📄 Article • 📋 Open Access PDF\n\n\nTest measurements of an asymmetric induction drift chamber with flash ADC's\n Decker, G, Kilian, K, Lippert, C, Oelert, W, Roderburg, E, Sefzick, T, Sehl, G, Steinkamp, O, Thomas, R, Ziolkowski, M, & others | Verhandlungen der Deutschen Physikalischen Gesellschaft | (1990) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R\n\n\n\\textperiodcentered\n Tyrell, Doris, Cline, Dorothy R, & **Thomas, Ronald G** | Controlled Clinical Trials | (1990) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R\nLinks: 📄 Article\n\n\n\\textperiodcentered\n Lee, Kelvin K & **Thomas, Ronald G** | Controlled Clinical Trials | (1990) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R"
  },
  {
    "objectID": "research/index.html#section-35",
    "href": "research/index.html#section-35",
    "title": "Research",
    "section": "1989",
    "text": "1989\n\nEffect of zinc supplementation on the development of cardiovascular disease in the elderly\n Hale, William E, May, Franklin E, **Thomas, Ronald G**, Moore, Mary T, & Stewart, Ronald B | Journal of Nutrition for the Elderly | (1989) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R\nLinks: 📄 Article\n\n\nExact sample size calculations for 2x2 comparative trials when the outcome proportions are small\n **Thomas, Ronald G** | Controlled Clinical Trials | (1989) \nSummary: Evaluating therapeutic interventions through rigorous clinical trials\nTopics: Biostatistics R\nLinks: 📄 Article • 📋 Open Access PDF\n\n\nRisk factors, time course and treatment effect for restenosis after successful percutaneous transluminal coronary angioplasty of chronic total occlusion\n Ellis, Stephen G, Shaw, Richard E, Gershony, Gary, **Thomas, Ronald**, Roubin, Gary S, Douglas Jr, John S, Topol, Eric J, Startzer, Simon H, Myler, Richard K, & King III, Spencer B | The American journal of cardiology | (1989) \nSummary: Evaluating therapeutic interventions through rigorous clinical trials\nTopics: Biostatistics R Drug development\nLinks: 📄 Article"
  },
  {
    "objectID": "research/index.html#section-36",
    "href": "research/index.html#section-36",
    "title": "Research",
    "section": "1988",
    "text": "1988\n\nAngiographic and clinical predictors of acute closure after native vessel coronary angioplasty.\n Ellis, SG, Roubin, GS, King 3rd, SB, Douglas Jr, JS, Weintraub, WS, Thomas, RG, & Cox, WR | Circulation | (1988) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R\n\n\nFORMS INVENTORY SYSTEM FOR A COMPLEX CLINICAL-TRIAL\n MELLEN, BG, THOMAS, RG, & CASTANO, D | CONTROLLED CLINICAL TRIALS | (1988) \nSummary: Evaluating therapeutic interventions through rigorous clinical trials\nTopics: Biostatistics R\nLinks: 📄 Article\n\n\nInfluence of balloon size on initial success, acute complications, and restenosis after percutaneous transluminal coronary angioplasty. A prospective randomized study.\n Roubin, Gary S, Douglas Jr, John S, King 3rd, SB, Lin, SF, Hutchison, Nancy, Thomas, RG, & Gruentzig, AR | Circulation | (1988) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R Clinical trials\nLinks: 📄 Article"
  },
  {
    "objectID": "research/index.html#section-37",
    "href": "research/index.html#section-37",
    "title": "Research",
    "section": "1987",
    "text": "1987\n\nA comparison of single lesion dilatation in single vessel and multivessel disease\n Mufson, LGAR, Roubin, GS, Black, A, & Thomas, RG | Circulation | (1987) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R\nLinks: 📄 Article\n\n\nEffects of behavioral self-management on oral hygiene adherence among orthodontic patients\n McGlynn, F Dudley, LeCompte, E Joseph, **Thomas, Ronald G**, Courts, Frank J, & Melamed, Barbara G | American Journal of Orthodontics and Dentofacial Orthopedics | (1987) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R\n\n\nImpact of air filtration on nosocomial Aspergillus infections: unique risk of bone marrow transplant recipients\n Sherertz, Robert J, Belani, Anusha, Kramer, Barnett S, Elfenbein, Gerald J, Weiner, Roy S, Sullivan, Marsha L, **Thomas, Ronald G**, & Samsa, Gregory P | The American journal of medicine | (1987) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R\nLinks: 📄 Article • 📋 Open Access PDF\n\n\nImpact of air filtration on nosocomial aspergillus infections\n Shcrertz, RJ, Belani, A, Kramer, BS, & others | Am J Med | (1987) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R\nLinks: 📄 Article • 📋 Open Access PDF\n\n\nIs there dependence between sites for continued success or restenosis after successful multisite coronary angioplasty\n Thomas, RG, Black, A, Lin, S, Chin, H, & Weintraub, WS | Circulation | (1987) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R\nLinks: 📄 Article\n\n\nPROGNOSIS AFTER MULTIPLE VESSEL ANGIOPLASTY (PTCA) IN PATIENTS WITH CORONARY-ARTERY DISEASE\n Roubin, GS, Sutor, C, Lembo, NJ, Hoffmeister, J, Thomas, RG, Douglas, JS, & King, SB | Circulation | (1987) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R\nLinks: 📄 Article"
  },
  {
    "objectID": "research/index.html#section-38",
    "href": "research/index.html#section-38",
    "title": "Research",
    "section": "1986",
    "text": "1986\n\nAlzheimer's disease: a correlational analysis of the Blessed Information-Memory-Concentration test and the Mini-Mental State Exam\n Thal, Leon J, Grundman, Michael, & Golden, Robert | Neurology | (1986) \nSummary: Advancing understanding of neurodegenerative diseases through clinical research\nTopics: Biostatistics R Alzheimer's disease Cognitive decline Statistical methods"
  },
  {
    "objectID": "research/index.html#section-39",
    "href": "research/index.html#section-39",
    "title": "Research",
    "section": "1985",
    "text": "1985\n\nELUTION OF PROSTAGLANDIN-E2 FROM FILTER-PAPER STRIPS-EFFICIENCY AND REPRODUCIBILITY\n HUWS, DA, FAN, TP, & THOMAS, RU | JOURNAL OF DENTAL RESEARCH | (1985) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R"
  },
  {
    "objectID": "research/index.html#section-40",
    "href": "research/index.html#section-40",
    "title": "Research",
    "section": "1984",
    "text": "1984\n\nChanges in shoulder and leg strength in athletes wearing mandibular orthopedic repositioning appliances\n Schubert, Mark M, Guttu, Ronald L, Hunter, Letha H, Hall, Richard, & **Thomas, Ronald** | The Journal of the American Dental Association | (1984) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R\nLinks: 📄 Article"
  },
  {
    "objectID": "research/index.html#section-41",
    "href": "research/index.html#section-41",
    "title": "Research",
    "section": "1983",
    "text": "1983\n\nEarly prediction of the adult respiratory distress syndrome by a simple scoring method\n Pepe, Paul E, **Thomas, Ronald G**, Stager, Marie Anne, Hudson, Leonard D, & Carrico, C James | Annals of emergency medicine | (1983) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R Statistical methods\n\n\nEffects of concomitant development on reproduction of Meloidogyne incognita and Rotylenchulus reniformis on sweet potato\n **Thomas, Ronald J** & Clark, Christopher A | Journal of Nematology | (1983) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R\nLinks: 📄 Article • 📋 Open Access PDF\n\n\nPopulation dynamics of Meloidogyne incognita and Rotylenchulus reniformis alone and in combination, and their effects on sweet potato\n **Thomas, Ronald J** & Clark, Christopher A | Journal of Nematology | (1983) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R Epidemiology\nLinks: 📄 Article • 📋 Open Access PDF\n\n\nTHE EFFECT OF MANDIBULAR ORTHOPEDIC REPOSITIONING APPLIANCES ON BODY STRENGTH\n Schubert, M, Guttu, R, Hunter, L, Hall, R, & Thomas, R | JOURNAL OF DENTAL RESEARCH | (1983) \nSummary: Contributing to evidence-based medicine and biostatistical research\nTopics: Biostatistics R\nLinks: 📄 Article"
  },
  {
    "objectID": "teaching/index.html",
    "href": "teaching/index.html",
    "title": "Teaching",
    "section": "",
    "text": "R programming for statistics\nReproducible research methods\nAdvanced data visualization\nStatistical software development\n\n\n\n\n\nClinical trial design\nSurvival analysis\nLongitudinal data analysis\nBayesian statistics applications"
  },
  {
    "objectID": "teaching/index.html#current-courses",
    "href": "teaching/index.html#current-courses",
    "title": "Teaching",
    "section": "",
    "text": "R programming for statistics\nReproducible research methods\nAdvanced data visualization\nStatistical software development\n\n\n\n\n\nClinical trial design\nSurvival analysis\nLongitudinal data analysis\nBayesian statistics applications"
  },
  {
    "objectID": "teaching/index.html#workshops-and-training",
    "href": "teaching/index.html#workshops-and-training",
    "title": "Teaching",
    "section": "2 Workshops and Training",
    "text": "2 Workshops and Training\n\n2.1 Professional Development\n\nR package development workshops\nReproducible research training\nStatistical consulting methodology\nAcademic writing for statisticians\n\n\n\n2.2 Conference Presentations\n\nInvited talks on statistical methods\nSoftware demonstrations\nMethodology tutorials\nBest practices sessions"
  },
  {
    "objectID": "teaching/index.html#educational-resources",
    "href": "teaching/index.html#educational-resources",
    "title": "Teaching",
    "section": "3 Educational Resources",
    "text": "3 Educational Resources\n\n3.1 Course Materials\n\nLecture slides and notes\nLab exercises and solutions\nAssignment templates\nAssessment rubrics\n\n\n\n3.2 Open Educational Content\n\nOnline tutorials and guides\nVideo lectures and demonstrations\nInteractive learning materials\nStudent project examples\n\n\nThis section will feature detailed course descriptions, syllabi, teaching materials, and educational resources developed for statistics and data science instruction."
  },
  {
    "objectID": "tutorials/docker-for-beginners/index.html",
    "href": "tutorials/docker-for-beginners/index.html",
    "title": "Securely Deploying Your Shiny App Online: A Step-by-Step Guide",
    "section": "",
    "text": "Photo by Nathan Waters on Unsplash"
  },
  {
    "objectID": "tutorials/docker-for-beginners/index.html#introduction",
    "href": "tutorials/docker-for-beginners/index.html#introduction",
    "title": "Securely Deploying Your Shiny App Online: A Step-by-Step Guide",
    "section": "1 Introduction",
    "text": "1 Introduction\nThis guide demonstrates how to deploy a Shiny application from your local workstation to a secure web environment. We’ll use a stack of open-source technologies including Linux, R, Shiny, Docker, and Caddy, deployed on AWS EC2. While we focus on AWS here, the principles apply to other cloud providers like Hetzner, which we’ll cover in future posts."
  },
  {
    "objectID": "tutorials/docker-for-beginners/index.html#prerequisites",
    "href": "tutorials/docker-for-beginners/index.html#prerequisites",
    "title": "Securely Deploying Your Shiny App Online: A Step-by-Step Guide",
    "section": "2 Prerequisites",
    "text": "2 Prerequisites\nBefore beginning this tutorial, you’ll need:\n\nA working Shiny application on your local machine\nAn AWS account with permissions to create EC2 instances\nBasic familiarity with the Linux command line\nGit (optional, for version control)"
  },
  {
    "objectID": "tutorials/docker-for-beginners/index.html#the-example-application",
    "href": "tutorials/docker-for-beginners/index.html#the-example-application",
    "title": "Securely Deploying Your Shiny App Online: A Step-by-Step Guide",
    "section": "3 The Example Application",
    "text": "3 The Example Application\nLet’s start with a simple but practical example: hosting a shiny web application that provides a power calculator for two-sample t-tests. While straightforward, this application demonstrates all the key deployment concepts.\nHere is the code for the Shiny app (The app is intentionally minimal, using only base R functions, with a minimum of reactive widgets and layout commands.):\n\nPower Calculator Shiny App Code (power1_shiny/app.R)\nui &lt;- fluidPage(\n  titlePanel(\"Power Calculator for Two Group Parallel Designs\"),\n  sliderInput(\"N\", \"Total Sample Size:\", min = 0, max = 300, value = 100),\n  plotOutput(\"plot\"),\n  verbatimTextOutput(\"eff\"))\n\nserver &lt;- function(input, output, session) {\n  delta = seq(0, 1.5,.05)\n  pow = reactive(sapply(delta, function(x) power.t.test(input$N, d=x)$power ))\n  eff =  renderText(power.t.test(input$N, power=.8)$d)\n  output$plot &lt;- renderPlot({\n    plot(delta, pow(), cex=1.5, ylab=\"power\")\n    abline(h = .8,  col = \"red\", lwd =2.5, lty = 4)\n    abline(v = eff(), col = \"blue\",lwd =2.5, lty = 4)})\n  output$eff &lt;- renderText(\n    paste0(\"Std. effect detectable with power 80% = \", eff()) )\n}\nshinyApp(ui, server)\n\n\n\n\n\n\nShiny app interface"
  },
  {
    "objectID": "tutorials/docker-for-beginners/index.html#step-by-step-implementation",
    "href": "tutorials/docker-for-beginners/index.html#step-by-step-implementation",
    "title": "Securely Deploying Your Shiny App Online: A Step-by-Step Guide",
    "section": "4 Step-by-Step Implementation",
    "text": "4 Step-by-Step Implementation\n\n4.1 Deployment Checklist\nAs an overview, to host our Shiny app securely online, we need to:\n\nObtain a static IP address\nRegister a domain name\nConfigure a firewall\nSet up the virtual server\nInstall and configure a web server\nImplement SSL encryption\nSet up user authentication\nConfigure reverse proxy routing\n\nWhile this might seem complex, we’ll break it down into manageable steps.\nDetailed instructions for setting up a virtual server (items 1 through 4 above) on EC2 both through the EC2 console and the command line interface can be found: here and here\n\n\n4.2 Step 1: Server Setup\nFirst, we’ll prepare our AWS EC2 environment: In the course of setting up your server, you’ll need to: 1. Create or access your AWS account 2. Generate SSH key-pair, named for example, power1_app.pem 3. Configure firewall settings, allowing SSH (port 22), HTTP (port 80) traffic and HTTPS (port 443) traffic. 4. Obtain static IP, e.g., 13.57.139.31 5. Register domain name, e.g. rgtlab.org 6. Launch Ubuntu instance (t2-micro is sufficient)\n\n\n4.3 Step 2: Installing Required Software\nconnect to your server via SSH:\nssh -i \"~/.ssh/power1_app.pem\"  ubuntu@rgtlab.org\nOn your server, install Docker and Caddy (a modern web server with automatic HTTPS) using the following commands.\nsudo apt update\nsudo apt install docker.io -y\nsudo apt install -y curl debian-keyring debian-archive-keyring apt-transport-https\ncurl -1sLf 'https://dl.cloudsmith.io/public/caddy/stable/gpg.key' | \\\nsudo gpg --dearmor -o /usr/share/keyrings/caddy-stable-archive-keyring.gpg\ncurl -1sLf 'https://dl.cloudsmith.io/public/caddy/stable/debian.deb.txt' | \\\nsudo tee /etc/apt/sources.list.d/caddy-stable.list\nsudo apt update\nsudo apt install caddy -y\n\n\n4.4 Step 3: Containerizing the Application\nCreate a Dockerfile in your app directory:\n\nDockerfile Configuration\nFROM rocker/shiny:4.2.0\nRUN rm -rf /srv/shiny-server\nCOPY /power1_shiny/* /srv/shiny-server/\nUSER shiny\nCMD [\"/usr/bin/shiny-server\"]\n\n\n\n4.5 Step 4: Configuring the Web Server\nCreate a Caddyfile:\n\nCaddy Server Configuration\nrgtlab.org {\n    basicauth * /power1_shiny/* {\n        bob $2a$14$pYWd5O7JqNeGLS4m4CKkzemM2pq5ezn9bcTDowofZTl5wRVl8NTJm\n    }\n    root * /var/www/html\n    handle_path /power1_shiny/* {\n            reverse_proxy 0.0.0.0:3838\n    }\n    file_server\n}\n\nCreate an index.html:\n\nLanding Page HTML\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n  &lt;body&gt;\n    &lt;h1&gt;Power1 app&lt;/h1&gt;\n    &lt;ul&gt;\n      &lt;li&gt;&lt;a href=\"./power1_shiny/\"&gt;Power1 app&lt;/a&gt;&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n\n\n\n4.6 Step 5: Deployment\n\nCopy files to server:\n\nscp -r ~/prj/power1_app/ ubuntu@rgtlab.org:~\n\nBuild and run Docker container:\n\ndocker build -t power1_image .\ndocker run -d --name=power1_shiny -p 3838:3838 --restart=always power1_image\n\nConfigure Caddy:\n\nsudo cp ./Caddyfile /etc/caddy/\ncp ./index.html /var/www/html/\nsudo systemctl reload caddy\nYour app should now be available at https://rgtlab.org!"
  },
  {
    "objectID": "tutorials/docker-for-beginners/index.html#advanced-tips",
    "href": "tutorials/docker-for-beginners/index.html#advanced-tips",
    "title": "Securely Deploying Your Shiny App Online: A Step-by-Step Guide",
    "section": "5 Advanced Tips",
    "text": "5 Advanced Tips\nFor easier SSH access, create a ~/.ssh/config file:\nHost rgtlab.org\nHostName 13.57.139.31\nStrictHostKeyChecking no\nUser ubuntu\nPort 22\nIdentityFile ~/.ssh/power1_app.pem\nThis enables simple SSH access:\nssh rgtlab.org"
  },
  {
    "objectID": "tutorials/docker-for-beginners/index.html#key-takeaways",
    "href": "tutorials/docker-for-beginners/index.html#key-takeaways",
    "title": "Securely Deploying Your Shiny App Online: A Step-by-Step Guide",
    "section": "6 Key Takeaways",
    "text": "6 Key Takeaways\n\nDocker containers provide isolation and reproducibility for your Shiny applications\nCaddy web server automatically handles SSL certificates and security\nBasic authentication provides a simple access control mechanism\nAWS EC2 offers a reliable platform for hosting web applications\nThe entire deployment can be automated for continuous delivery workflows"
  },
  {
    "objectID": "tutorials/docker-for-beginners/index.html#further-reading",
    "href": "tutorials/docker-for-beginners/index.html#further-reading",
    "title": "Securely Deploying Your Shiny App Online: A Step-by-Step Guide",
    "section": "7 Further Reading",
    "text": "7 Further Reading\n\nShiny Server documentation\nDocker documentation\nCaddy Web Server documentation\nAWS EC2 documentation"
  },
  {
    "objectID": "tutorials/docker-for-beginners/index.html#step-by-step-implementation-1",
    "href": "tutorials/docker-for-beginners/index.html#step-by-step-implementation-1",
    "title": "Securely Deploying Your Shiny App Online: A Step-by-Step Guide",
    "section": "8 Step-by-Step Implementation",
    "text": "8 Step-by-Step Implementation\nIn development"
  },
  {
    "objectID": "tutorials/docker-for-beginners/index.html#key-takeaways-1",
    "href": "tutorials/docker-for-beginners/index.html#key-takeaways-1",
    "title": "Securely Deploying Your Shiny App Online: A Step-by-Step Guide",
    "section": "9 Key Takeaways",
    "text": "9 Key Takeaways\nIn development"
  },
  {
    "objectID": "tutorials/docker-for-beginners/index.html#further-reading-1",
    "href": "tutorials/docker-for-beginners/index.html#further-reading-1",
    "title": "Securely Deploying Your Shiny App Online: A Step-by-Step Guide",
    "section": "10 Further Reading",
    "text": "10 Further Reading\nIn development"
  },
  {
    "objectID": "tutorials/index.html",
    "href": "tutorials/index.html",
    "title": "Tutorials",
    "section": "",
    "text": "Comprehensive tutorials designed to teach you new skills from the ground up. These evergreen resources are regularly updated and expanded with new content.\nEach tutorial includes: - Clear learning objectives - Step-by-step instructions - Working examples - Practice exercises - Troubleshooting tips\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nR Package Development: From Idea to CRAN\n\n\nComplete tutorial for creating your first R package\n\n\n\nR\n\npackages\n\ndevelopment\n\ntutorial\n\n\n\nStep-by-step guide to developing, documenting, and submitting an R package to CRAN.\n\n\n\n\n\n\n\n\n\n\n\n\nSecurely Deploying Your Shiny App Online: A Step-by-Step Guide\n\n\n\nDeployment & Operations\n\n\n\nA practical guide for data scientists on how to deploy R Shiny applications securely using open-source technologies.\n\n\n\n\n\n\n\n\n\n\n\n\nSetting up git for (solo) data science workflow\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "white-papers/index-old.html",
    "href": "white-papers/index-old.html",
    "title": "White Papers",
    "section": "",
    "text": "All Categories\n        Research Methodology\n        Statistical Computing\n        Data Science\n        Technical Infrastructure\nThese white papers represent in-depth technical analyses, methodological frameworks, and implementation guides developed for research and statistical computing applications. Each document provides detailed specifications, best practices, and reproducible workflows."
  },
  {
    "objectID": "white-papers/index-old.html#research-methodology-workflows",
    "href": "white-papers/index-old.html#research-methodology-workflows",
    "title": "White Papers",
    "section": "1 Research Methodology & Workflows",
    "text": "1 Research Methodology & Workflows\n\n1.1 Mac Workflow for Tracking Daily Research Progress\nA comprehensive framework for organizing research activities, maintaining progress logs, and implementing version control for academic projects.\nResearch management Workflow automation Version control Academic productivity\n\n🔗 Full Report • 📄 PDF\n\n\n\n1.2 Setting Up a Comprehensive Research Backup System on macOS\nTechnical specification for implementing a multi-layered backup strategy for research data, ensuring redundancy and security across local and cloud storage systems.\nData management Backup systems macOS Research infrastructure\n\n🔗 Full Report • 📄 PDF"
  },
  {
    "objectID": "white-papers/index-old.html#statistical-computing-development",
    "href": "white-papers/index-old.html#statistical-computing-development",
    "title": "White Papers",
    "section": "2 Statistical Computing & Development",
    "text": "2 Statistical Computing & Development\n\n2.1 RCT Validation Language\nSpecification for a domain-specific programming language designed to capture clinical trial database validation logic, with compilation targets for Lua and JavaScript.\nClinical trials Programming languages Data validation DSL design\n\n🔗 Full Report • 📄 PDF\n\n\n\n2.2 Setting up an R Development Environment on GitHub\nBest practices and step-by-step methodology for establishing reproducible R package development workflows using GitHub integration and continuous integration.\nR development GitHub Package development CI/CD\n\n🔗 Full Report • 📄 PDF"
  },
  {
    "objectID": "white-papers/index-old.html#data-science-applications",
    "href": "white-papers/index-old.html#data-science-applications",
    "title": "White Papers",
    "section": "3 Data Science Applications",
    "text": "3 Data Science Applications\n\n3.1 Making Optimal Use of ChatGPT and Other Chatbots for Data Science\nEvaluation framework and practical guidelines for integrating large language models into data science workflows, including prompt engineering and quality assessment.\nAI tools Data science LLM integration Prompt engineering\n\n🔗 Full Report • 📄 PDF\n\n\n\n3.2 Minimalist EDC Application Framework\nTechnical architecture for building lightweight electronic data capture systems for clinical research, emphasizing simplicity and regulatory compliance.\nEDC systems Clinical research Software architecture Regulatory compliance\n\n🔗 Full Report • 📄 PDF"
  },
  {
    "objectID": "white-papers/index-old.html#technical-infrastructure",
    "href": "white-papers/index-old.html#technical-infrastructure",
    "title": "White Papers",
    "section": "4 Technical Infrastructure",
    "text": "4 Technical Infrastructure\n\n4.1 Containerized R Analysis Workflows with Docker\nImplementation guide for reproducible R analysis environments using Docker containerization, including best practices for sharing and deployment.\nDocker Reproducibility R environment Containerization\n\n🔗 Full Report • 📄 PDF\n\n\n\n4.2 AWS Server Configuration for Research Computing\nComprehensive guide for setting up and configuring AWS instances for statistical computing and research data analysis.\nAWS Cloud computing Server configuration Research computing\n\n🔗 Full Report • 📄 PDF"
  }
]