---
title: "Constructing Tests for Data Analysis Workflows"
subtitle: "A practical guide to testing reproducible research pipelines in R"
author: "RG Thomas"
date: "2025-07-25"
document-type: "blog"
draft: true
categories: [R Programming, Data Science, Statistical Computing, Reproducibility]
description: "Learn how to construct comprehensive tests for data analysis
  workflows using testthat, assertr, and other R testing frameworks to ensure
  computational reproducibility and result correctness"
image: "../../images/posts/ucsd-geisel-library.jpg"
execute:
  echo: true
  warning: false
  message: false
format:
  html:
    code-fold: false
    code-tools: false
---

![UCSD Geisel Library](../../images/posts/ucsd-geisel-library.jpg){.img-fluid}

# Introduction

Testing data analysis workflows presents unique challenges compared to
traditional software development. While package development focuses on testing
isolated functions with predictable inputs and outputs, data analysis involves
testing complex pipelines, validating data quality, ensuring reproducibility,
and verifying that analytical results are correct and meaningful.

Traditional software engineering has embraced testing for decades. Data science
often lags behind, with practitioners relying on informal validation: "It's
just exploratory analysis," "The results look reasonable," or "I checked it
manually." These approaches fail to scale and provide no protection against
regression when code or data change.

This guide provides strategies for testing data analysis workflows in R,
drawing on established software engineering practices while addressing the
specific needs of computational research.

By the end of this post, you will be able to:

- Distinguish between computational reproducibility and result correctness
- Implement unit tests, data validation tests, and integration tests
- Use the testthat framework for structured testing
- Apply assertr for pipeline-friendly data validation
- Test reproducibility through seed management and known-result comparison
- Set up continuous integration for automated testing

# Prerequisites and Setup

**Required Packages:**

```{r}
#| eval: false
install.packages(c("testthat", "assertr", "palmerpenguins", "dplyr",
                   "ggplot2", "validate"))
```

**Load Libraries:**

```{r}
library(testthat)
library(assertr)
library(palmerpenguins)
library(dplyr)
library(ggplot2)
```

**Sample Data:**

```{r}
data(penguins)
glimpse(penguins)
```

# Why Test Data Analysis Code?

## The Testing Gap in Data Science

Data analysis workflows fail in ways that differ from traditional software:

**Data problems:**

- Column types change between data pulls
- Missing values appear in unexpected places
- IDs fail to match across tables

**Code problems:**

- Function behavior changes with package updates
- Edge cases cause silent failures
- Copy-paste errors accumulate

**Analysis problems:**

- Results change when re-run
- Different random seeds produce different answers
- Collaborators cannot reproduce findings

## Two Goals of Testing

Data analysis testing serves two primary goals:

1. **Computational Reproducibility**: Another researcher can use your code and
   data to independently obtain identical results. Tests verify that the
   computational pipeline works.

2. **Result Correctness**: The results generated by the code are accurate and
   meaningful. Tests validate expected properties of outputs.

As Wilson et al. (2017) note, "An important aspect of this is to validate the
code using software development practices that prevent errors and software
testing methods that can help detect them when they occur."

# Types of Tests for Data Analysis

## Testing Taxonomy

| Test Type | What It Checks | When to Use |
|-----------|---------------|-------------|
| Unit tests | Individual functions | Helper functions, utilities |
| Data validation | Data quality | After loading, after transforms |
| Integration | Full pipeline | Before finalizing analysis |
| Reproducibility | Same results | Random processes, cross-platform |

## Unit Tests

Unit tests verify that individual functions behave correctly in isolation.
Test with known inputs and expected outputs:

```{r}
#| eval: false
test_that("outlier detection function works", {
  test_data <- c(1, 2, 3, 100, 4, 5)
  outliers <- detect_outliers(test_data, method = "iqr")

  expect_equal(outliers, 100)
  expect_length(outliers, 1)
})
```

## Data Validation Tests

Data validation tests ensure data meets quality requirements before analysis
proceeds:

```{r}
#| eval: false
test_that("data meets quality standards", {
  data <- read.csv("analysis/data/raw_data/penguins.csv")

  # Check structure
  expect_equal(ncol(data), 8)
  expect_true(all(c("species", "body_mass_g") %in% names(data)))

  # Check ranges
  expect_true(all(data$body_mass_g > 0, na.rm = TRUE))
  expect_true(all(data$body_mass_g < 10000, na.rm = TRUE))
})
```

## Integration Tests

Integration tests verify that components work together through the complete
workflow:

```{r}
#| eval: false
test_that("analysis pipeline runs successfully", {
  expect_no_error({
    raw_data <- load_raw_data()
    clean_data <- clean_data(raw_data)
    model <- fit_model(clean_data)
    results <- generate_results(model)
  })

  expect_s3_class(results, "data.frame")
  expect_true(nrow(results) > 0)
})
```

## Reproducibility Tests

Reproducibility tests confirm that results are deterministic when using the
same random seed:

```{r}
#| eval: false
test_that("bootstrap analysis is reproducible", {
  set.seed(42)
  results1 <- bootstrap_analysis(data, n_boots = 1000)

  set.seed(42)
  results2 <- bootstrap_analysis(data, n_boots = 1000)

  expect_equal(results1$estimate, results2$estimate)
  expect_equal(results1$ci_lower, results2$ci_lower)
})
```

# The testthat Framework

## testthat Basics

The testthat package provides the foundation for testing in R. Tests follow the
Arrange-Act-Assert pattern:

```{r}
#| eval: false
library(testthat)

test_that("description of what we're testing", {
  # Arrange: set up test data
  x <- c(1, 2, 3, 4, 5)

  # Act: run the code
  result <- mean(x)

  # Assert: check expectations
  expect_equal(result, 3)
})
```

## Common Expectations

testthat provides a rich vocabulary for expressing test expectations:

```{r}
#| eval: false
# Equality
expect_equal(result, expected)          # Allows tolerance
expect_identical(result, expected)      # Exact match

# Logical
expect_true(condition)
expect_false(condition)

# Types and classes
expect_type(x, "double")
expect_s3_class(model, "lm")

# Errors and warnings
expect_error(bad_function())
expect_warning(risky_function())
expect_no_error(safe_function())
```

## Test File Organization

Organize tests following R package conventions:

```
tests/
├── testthat.R              # Test runner
└── testthat/
    ├── helper-test-data.R  # Shared test utilities
    ├── test-data-loading.R
    ├── test-data-cleaning.R
    ├── test-models.R
    └── test-visualization.R
```

Test files must start with `test-` to be discovered by the test runner.

## Helper Functions

Create reusable test utilities in `helper-*.R` files. These are automatically
loaded before tests run:

```{r}
#| eval: false
# helper-test-data.R
create_test_penguins <- function(n = 50) {
  data.frame(
    species = sample(c("Adelie", "Chinstrap", "Gentoo"), n, replace = TRUE),
    bill_length_mm = rnorm(n, mean = 44, sd = 5),
    body_mass_g = rnorm(n, mean = 4200, sd = 800)
  )
}

expect_valid_model <- function(model) {
  expect_s3_class(model, "lm")
  expect_true(length(coef(model)) > 0)
  expect_true(!any(is.na(coef(model))))
}
```

## Running Tests

```{r}
#| eval: false
# Run all tests
devtools::test()

# Run specific test file
testthat::test_file("tests/testthat/test-data-cleaning.R")

# Run tests matching a pattern
testthat::test_dir("tests/testthat", filter = "model")
```

# Data Validation with assertr

## Pipeline-Friendly Assertions

The assertr package integrates data validation into tidyverse pipelines.
Assertions fail loudly when violated:

```{r}
#| eval: false
library(assertr)

penguins |>
  verify(nrow(.) > 300) |>
  verify(ncol(.) == 8) |>
  assert(within_bounds(0, 10000), body_mass_g) |>
  assert(in_set("Adelie", "Chinstrap", "Gentoo"), species) |>
  insist(within_n_sds(3), bill_length_mm)
```

## assertr Functions

The package provides several assertion functions:

- `verify()`: Check that a logical condition is TRUE
- `assert()`: Check that a predicate holds for a column
- `insist()`: Check that a predicate holds using row-wise computation

## Practical Example

```{r}
penguins_validated <- penguins |>
  verify(nrow(.) > 300) |>
  assert(not_na, species, island) |>
  assert(within_bounds(2000, 7000), body_mass_g)

cat("Validation passed:", nrow(penguins_validated), "rows\n")
```

# Custom Validation Rules

## Domain-Specific Validation

Create validation functions tailored to your data domain:

```{r}
#| eval: false
validate_penguin_data <- function(data) {
  test_that("penguin data validation", {
    # Required columns exist
    required_cols <- c("species", "island", "bill_length_mm",
                       "bill_depth_mm", "flipper_length_mm",
                       "body_mass_g", "sex", "year")
    expect_true(all(required_cols %in% names(data)))

    # Species are valid
    valid_species <- c("Adelie", "Chinstrap", "Gentoo")
    expect_true(all(data$species %in% valid_species))

    # Measurements are positive
    expect_true(all(data$bill_length_mm > 0, na.rm = TRUE))
    expect_true(all(data$body_mass_g > 0, na.rm = TRUE))

    # Years are reasonable
    expect_true(all(data$year >= 2007 & data$year <= 2009))
  })
}
```

## Testing Data Transformations

Test that data cleaning functions produce expected results:

```{r}
#| eval: false
test_that("clean_penguins removes missing values correctly", {
  # Create test data with known missing pattern
  test_data <- data.frame(
    species = c("Adelie", "Gentoo", "Chinstrap"),
    bill_length_mm = c(39.1, NA, 46.5),
    body_mass_g = c(3750, 4500, NA)
  )

  # Apply cleaning function
  cleaned <- clean_penguins(test_data)

  # Verify results
  expect_equal(nrow(cleaned), 1)
  expect_equal(cleaned$species, "Adelie")
  expect_false(any(is.na(cleaned)))
})
```

# Reproducibility Testing

## Seed Management

Random number seeds are essential for reproducible analysis. Always document
and test seed usage:

```{r}
#| eval: false
test_that("cross-validation is reproducible", {
  data <- create_test_penguins(100)

  set.seed(123)
  cv1 <- perform_cv(data, folds = 5)

  set.seed(123)
  cv2 <- perform_cv(data, folds = 5)

  expect_equal(cv1$fold_assignments, cv2$fold_assignments)
  expect_equal(cv1$rmse, cv2$rmse)
})
```

## Testing Against Known Results

Store expected values from verified runs and test against them:

```{r}
#| eval: false
test_that("regression coefficients match expected values", {
  data(penguins, package = "palmerpenguins")
  clean_data <- na.omit(penguins)

  model <- lm(body_mass_g ~ flipper_length_mm, data = clean_data)
  coefs <- coef(model)

  # Test against pre-computed values
  expect_equal(coefs["(Intercept)"], -5780.83, tolerance = 0.1)
  expect_equal(coefs["flipper_length_mm"], 49.69, tolerance = 0.01)
})
```

## Package Version Testing

Track package versions to identify when results might change:

```{r}
#| eval: false
test_that("analysis uses expected package versions", {
  expect_true(packageVersion("dplyr") >= "1.0.0")
  expect_true(packageVersion("ggplot2") >= "3.4.0")

  # Check renv lockfile is synchronized
  if (requireNamespace("renv", quietly = TRUE)) {
    status <- renv::status()
    expect_true(status$synchronized)
  }
})
```

# Testing Analysis Scripts

## Script Execution Tests

Verify that analysis scripts run without error:

```{r}
#| eval: false
test_that("analysis scripts run without error", {
  scripts <- c(
    "scripts/01_load_data.R",
    "scripts/02_clean_data.R",
    "scripts/03_fit_models.R",
    "scripts/04_create_figures.R"
  )

  for (script in scripts) {
    expect_true(file.exists(script))
    expect_no_error(
      source(script, local = new.env()),
      info = paste("Failed:", script)
    )
  }
})
```

## Output Validation

Test that scripts produce expected output files:

```{r}
#| eval: false
test_that("scripts produce expected outputs", {
  source("scripts/02_clean_data.R", local = new.env())

  expect_true(file.exists("analysis/data/derived_data/penguins_clean.rds"))

  clean_data <- readRDS("analysis/data/derived_data/penguins_clean.rds")
  expect_true(nrow(clean_data) > 300)
  expect_false(any(is.na(clean_data)))
})
```

## Testing Report Rendering

Verify that R Markdown documents render successfully:

```{r}
#| eval: false
test_that("report renders successfully", {
  report_path <- "analysis/paper/paper.Rmd"

  expect_true(file.exists(report_path))

  skip_if_not_installed("tinytex")

  output_dir <- tempdir()
  expect_no_error({
    rmarkdown::render(report_path,
                     output_dir = output_dir,
                     quiet = TRUE)
  })
})
```

# Continuous Integration

## GitHub Actions for R

Create `.github/workflows/test-analysis.yml`:

```yaml
name: Test Analysis
on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    - uses: r-lib/actions/setup-r@v2
    - uses: r-lib/actions/setup-r-dependencies@v2

    - name: Run tests
      run: |
        testthat::test_dir('tests/testthat')
      shell: Rscript {0}
```

## Automated Quality Checks

Add data validation and reproducibility checks to CI:

```yaml
    - name: Data validation
      run: |
        source('tests/validate_data.R')
      shell: Rscript {0}

    - name: Reproducibility check
      run: |
        source('tests/reproducibility_check.R')
      shell: Rscript {0}
```

Tests run automatically on every push, catching problems before they reach
collaborators.

# Best Practices

## Testing Philosophy

1. **Test early, not just at the end.** Write tests during exploration, not
   after the analysis is complete.

2. **Test behavior, not implementation.** Focus on what the function should
   do, not how it does it.

3. **Use meaningful test names.** Write `"model returns valid coefficients"`
   rather than `"test model"`.

4. **Test edge cases.** Include empty data, missing values, and extreme values.

## Common Pitfalls

**Avoid:**

- Testing implementation details
- Overly specific tests that break with minor changes
- Ignoring random elements
- Skipping integration tests

**Instead:**

- Test observable behavior
- Allow appropriate flexibility in tests
- Always set and document seeds
- Test full workflows

## Testing Checklist for Projects

- [ ] Data loading tests (file exists, correct dimensions)
- [ ] Data validation tests (ranges, types, missing patterns)
- [ ] Function unit tests (known inputs/outputs)
- [ ] Model tests (coefficients, diagnostics)
- [ ] Reproducibility tests (seeds, package versions)
- [ ] Integration tests (full pipeline runs)
- [ ] Output tests (files created, correct format)

# Complete Example

## Penguins Regression Testing

Here is a complete example testing a regression analysis:

```{r}
#| eval: false
# test-regression.R

test_that("body mass model has expected properties", {
  data(penguins, package = "palmerpenguins")
  clean_data <- na.omit(penguins)

  model <- lm(body_mass_g ~ flipper_length_mm + species,
              data = clean_data)

  # Model structure
  expect_s3_class(model, "lm")
  expect_equal(length(coef(model)), 4)  # Intercept + 3 predictors

  # Model quality
  r_squared <- summary(model)$r.squared
  expect_true(r_squared > 0.8)  # Should explain >80% variance

  # Residuals are reasonable
  expect_true(abs(mean(resid(model))) < 1e-10)  # ~0
})

test_that("model is reproducible", {
  data(penguins, package = "palmerpenguins")
  clean_data <- na.omit(penguins)

  model1 <- lm(body_mass_g ~ flipper_length_mm + species, data = clean_data)
  model2 <- lm(body_mass_g ~ flipper_length_mm + species, data = clean_data)

  expect_equal(coef(model1), coef(model2))
  expect_equal(summary(model1)$r.squared, summary(model2)$r.squared)
})
```

# Conclusion

Testing data analysis workflows requires adapting traditional software testing
practices to the unique challenges of data science. The combination of unit
tests, data validation, integration tests, and reproducibility tests provides
comprehensive coverage for research code.

**Key takeaways:**

- Testing ensures both computational reproducibility and result correctness
- testthat provides the foundation; assertr adds pipeline-friendly validation
- Test during analysis development, not just at the end
- Continuous integration automates testing on every code change

Rigorous testing transforms analysis code from fragile scripts into reliable
research infrastructure that others can trust and build upon.

# References and Acknowledgments

## Academic Literature

- Wilson, G., Bryan, J., Cranston, K., Kitzes, J., Nederbragt, L., & Teal,
  T.K. (2017). Good enough practices in scientific computing. *PLOS
  Computational Biology*, 13(6), e1005510.
- Wickham, H. (2011). testthat: Get started with testing. *The R Journal*,
  3(1), 5-10.
- Poldrack, R.A. (2019). Statistical thinking for the 21st century.
  https://statsthinking21.github.io/
- van der Loo, M. & de Jonge, E. (2021). Data validation infrastructure for
  R. *Journal of Statistical Software*, 97(10), 1-31.

## Package Documentation

- [testthat package](https://testthat.r-lib.org/)
- [assertr package](https://docs.ropensci.org/assertr/)
- [validate package](https://github.com/data-cleaning/validate)

## Acknowledgments

This post draws substantially from the "Testing Data Analysis Workflows"
vignette in the zzcollab package, developed by the ZZCOLLAB Development Team.
The testing philosophy and three-phase approach described here follow the
reproducible research workflow principles articulated by Poldrack (2019).

# Reproducibility Information

```{r}
#| echo: false
sessionInfo()
```
